[{"content":"這篇文章主要記錄透過 NFS Server 在 K3s cluster 新增 Storage Class的方法\n之前讀過 Shawn Ho 大大的在GKE上使用ReadWrite Many的Disk，其中提到:\n通過部署一套NFS Pod，該Pod先使用ReadWrite Once，再把這個NFS Pod當作是File Server提供ReadWrite Many的使用\n突然意識到 multipass 產生的 Ubuntu VM ，不就是現成的 Filesystem ! 只要在 Ubuntu 上安裝了 NFS server ， 並在其他 VM 上安裝 NFS client ， 那應該就能新增使用 NFS 的 Storage Class 了！ 查了一些資料後發現可行，於是就手動實做看看。\nNFS Server 一樣先用 multipass 啟動一台 VM 作為 NFS Server 使用\nmultipass launch -n nfs-server -c 1 -m 2G -d 10G 22.04 登入 VM nfs-server 後，安裝 nfs-server\n$ multipass shell nfs-server ... # install nfs-kernel-server ubuntu@nfs-server:~$ sudo apt-get install -y nfs-kernel-server ... # confirm nfs-kernel-server status ubuntu@nfs-server:~$ sudo service nfs-kernel-server status ● nfs-server.service - NFS server and services Loaded: loaded (/lib/systemd/system/nfs-server.service; enabled; vendor preset: enabled) Active: active (exited) since Fri 2023-10-27 00:24:21 CST; 1min 7s ago Main PID: 2296 (code=exited, status=0/SUCCESS) CPU: 4ms Oct 27 00:24:19 nfs-server systemd[1]: Starting NFS server and services... Oct 27 00:24:19 nfs-server exportfs[2295]: exportfs: can\u0026#39;t open /etc/exports for reading Oct 27 00:24:21 nfs-server systemd[1]: Finished NFS server and services. nfs-server 安裝成功， nfs-kernel-server 服務啟動，但提示仍要設定 /etc/exports\n# create a folder for storage nfs client files ubuntu@nfs-server:~$ mkdir -p /home/ubuntu/storage # edit /etc/exports ubuntu@nfs-server:~$ sudo vim /etc/exports # add config of file /etc/exports and save it /home/ubuntu/storage 192.168.64.0/24(rw,sync,no_subtree_check,root_squash,insecure) # export /etc/exports configuration ubuntu@nfs-server:~$ sudo exportfs -a # restart service ubuntu@nfs-server:~$ sudo service nfs-kernel-server restart # confirm nfs-kernel-server status ubuntu@nfs-server:~$ sudo service nfs-kernel-server status ● nfs-server.service - NFS server and services Loaded: loaded (/lib/systemd/system/nfs-server.service; enabled; vendor preset: enabled) Active: active (exited) since Fri 2023-10-27 00:43:39 CST; 29s ago Process: 2802 ExecStartPre=/usr/sbin/exportfs -r (code=exited, status=0/SUCCESS) Process: 2803 ExecStart=/usr/sbin/rpc.nfsd (code=exited, status=0/SUCCESS) Main PID: 2803 (code=exited, status=0/SUCCESS) CPU: 4ms Oct 27 00:43:39 nfs-server systemd[1]: Starting NFS server and services... Oct 27 00:43:39 nfs-server systemd[1]: Finished NFS server and services. 沒有提示需要設定 /etc/exports ， 表示上述設定 nfs-server 的動作完成\n/etc/exports 的設定格式如下\n{share_folder_path} {allowed ip range | any host}(options) {share_folder_path} : NFS server 分享的資料夾位置，用以存放 NFS client 的檔案位置 {allowed ip range | any host} : 允許可存取 {share_folder_path} 的 IP 來源，可用 CIDR 或 * options 有以下選項 : 下表轉載於Akiicat 學習筆記 - 在 Ubuntu 上架設 NFS server，感謝作者採用創用 CC 姓名標示-相同方式分享 4.0 國際 授權條款授權.，讓我可以直接分享 選項是選擇性填寫的，有很多參數可以選 - ro：read only - rw：read and write - async：此選項允許 NFS Server 違反 NFS protocol，允許檔案尚未存回磁碟之前回覆請求。這個選項可以提高性能，但是有可能會讓 server 崩潰，可能會需要重新啟動 server. 或檔案遺失。 - sync：只會儲存檔案會磁碟之後才會回覆請求。 - no_subtree_check：禁用子樹檢查，會有些微的不安全，但在某些情況下可以提高可靠性。 - secure：請求的 port 必須小於 1024，這個選項是預設的。 - insecure：請求的 port 不一定要小於 1024。 User ID Mapping 參數： - root_squash：將 uid 0 (root) 的使用者映射到 nobody (uid 65534) 匿名使用者，這個選項是預設的。 - no_root_squash：關掉 root squash 的選項，這個選項可以使用 root 身份來控制 NFS Server 的檔案。 - all_squash：所有登入 NFS 的使用者身份都會被壓縮成為 nobody。 更多參數的選項可以參考 exports(5) - Linux man page\nnfs-subdir-external-provisioner 我用 helm 安裝 nfs-subdir-external-provisioner，它會自動設定並使用上面安裝好的 NFS server ，同時產生一個 storage class ， 之後建立 PVC 時採用這個 storage class ， 就會自動產生對應的 PV ，我覺得超級方便。\n$ helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ $ helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=192.168.64.4 \\ --set nfs.path=/home/ubuntu/storage NAME: nfs-subdir-external-provisioner LAST DEPLOYED: Fri Oct 27 00:58:25 2023 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None 這時檢查一下 cluster 內的 resource\n$ kubectl get sc,pod NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE storageclass.storage.k8s.io/local-path (default) rancher.io/local-path Delete WaitForFirstConsumer false 26h storageclass.storage.k8s.io/nfs-client cluster.local/nfs-subdir-external-provisioner Delete Immediate true 2m7s NAME READY STATUS RESTARTS AGE pod/nfs-subdir-external-provisioner-7fdb777787-nc6rw 0/1 ContainerCreating 0 2m7s 可以看到 nfs-subdir-external-provisioner 已經建立起一個 名稱為 nfs-client 的 storage class\n但 nfs-subdir-external-provisioner 的 pod 似乎卡在 ContainerCreating，檢查 pod 狀態\n$ kubectl describe pod nfs-subdir-external-provisioner-7fdb777787-nc6rw ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4m56s default-scheduler Successfully assigned default/nfs-subdir-external-provisioner-7fdb777787-nc6rw to worker Warning FailedMount 47s (x10 over 4m57s) kubelet MountVolume.SetUp failed for volume \u0026#34;nfs-subdir-external-provisioner-root\u0026#34; : mount failed: exit status 32 Mounting command: mount Mounting arguments: -t nfs 192.168.64.4:/home/ubuntu/storage /var/lib/kubelet/pods/a84c53a7-8356-4443-b40d-8c7b4d9d3767/volumes/kubernetes.io~nfs/nfs-subdir-external-provisioner-root Output: mount: /var/lib/kubelet/pods/a84c53a7-8356-4443-b40d-8c7b4d9d3767/volumes/kubernetes.io~nfs/nfs-subdir-external-provisioner-root: bad option; for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.\u0026lt;type\u0026gt; helper program. Warning FailedMount 40s (x2 over 2m54s) kubelet Unable to attach or mount volumes: unmounted volumes=[nfs-subdir-external-provisioner-root], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition 提示 pod 在 mount /home/ubuntu/storage 時出現了錯誤\nOutput: mount: /var/lib/kubelet/pods/a84c53a7-8356-4443-b40d-8c7b4d9d3767/volumes/kubernetes.io~nfs/nfs-subdir-external-provisioner-root: bad option; for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.\u0026lt;type\u0026gt; helper program. 這是因為 pod 掛載的 node 沒有安裝對應的 nfs-client 套件導致，檢查 pod 目前掛載在哪個 node\n$ kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE nfs-subdir-external-provisioner-7fdb777787-nc6rw 0/1 ContainerCreating 0 7m31s \u0026lt;none\u0026gt; worker 登入到 node worker 安裝 nfs-common 套件\n# login to node worker $ multipass shell worker ... # install nfs-common ubuntu@worker:~$ sudo apt-get install -y nfs-common # confirm nfs-common service status ubuntu@worker:~$ sudo service nfs-common status ○ nfs-common.service Loaded: masked (Reason: Unit nfs-common.service is masked.) Active: inactive (dead) 驚了！剛安裝好的 nfs-common 竟然是 inactive ，檢查 nfs-common.service 檔案\nubuntu@worker:~$ file /lib/systemd/system/nfs-common.service /lib/systemd/system/nfs-common.service: symbolic link to /dev/null 不知道什麼原因造成 link 到 /dev/null ， 嘗試刪除檔案後重新加載服務\nubuntu@worker:~$ sudo rm -f /lib/systemd/system/nfs-common.service ubuntu@worker:~$ sudo systemctl status nfs-common ○ nfs-common.service - LSB: NFS support files common to client and server Loaded: loaded (/etc/init.d/nfs-common; generated) Active: inactive (dead) Docs: man:systemd-sysv-generator(8) nfs-common 服務已載入，接下來重啟服務即可\nubuntu@worker:~$ sudo service nfs-common restart ubuntu@worker:~$ sudo service nfs-common status ● nfs-common.service - LSB: NFS support files common to client and server Loaded: loaded (/etc/init.d/nfs-common; generated) Active: active (running) since Fri 2023-10-27 01:14:12 CST; 8s ago Docs: man:systemd-sysv-generator(8) Process: 9992 ExecStart=/etc/init.d/nfs-common start (code=exited, status=0/SUCCESS) Tasks: 2 (limit: 2275) Memory: 2.4M CPU: 38ms CGroup: /system.slice/nfs-common.service ├─10000 /sbin/rpc.statd └─10012 /usr/sbin/rpc.idmapd Oct 27 01:14:12 worker systemd[1]: Starting LSB: NFS support files common to client and server... Oct 27 01:14:12 worker nfs-common[9992]: * Starting NFS common utilities Oct 27 01:14:12 worker rpc.statd[10000]: Version 2.6.1 starting Oct 27 01:14:12 worker sm-notify[10001]: Version 2.6.1 starting Oct 27 01:14:12 worker sm-notify[10001]: Already notifying clients; Exiting! Oct 27 01:14:12 worker rpc.statd[10000]: Failed to read /var/lib/nfs/state: Success Oct 27 01:14:12 worker rpc.statd[10000]: Initializing NSM state Oct 27 01:14:12 worker rpc.idmapd[10012]: Setting log level to 0 Oct 27 01:14:12 worker nfs-common[9992]: ...done. Oct 27 01:14:12 worker systemd[1]: Started LSB: NFS support files common to client and server. 再次檢查 nfs-subdir-external-provisioner pod 的狀態， 可以看到已經是 running 了\n$ kubectl get pod NAME READY STATUS RESTARTS AGE nfs-subdir-external-provisioner-7fdb777787-nc6rw 1/1 Running 0 16m 測試 storage class 透過部署一個 nginx 服務，並將 nginx log 掛載到 pv 上，測試 storage class 是否能正常使用\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: nginx-log spec: storageClassName: nfs-client accessModes: - ReadWriteMany resources: requests: storage: 100Mi --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 volumeMounts: - name: nginx-log mountPath: /var/log/nginx volumes: - name: nginx-log persistentVolumeClaim: claimName: nginx-log --- apiVersion: v1 kind: Service metadata: name: nginx spec: type: ClusterIP ports: - port: 80 targetPort: 80 selector: app: nginx 儲存成 install-nginx.yaml 檔之後，部署到 k3s cluster\n# deploy install-nginx.yaml file $ kubectl apply -f install-nginx.yaml persistentvolumeclaim/nginx-log created deployment.apps/nginx created service/nginx created # verify pvc,pv,pod,svc $ kubectl get pvc,pv,pod,svc persistentvolumeclaim/nginx-log Bound pvc-f4a17b71-d79d-44fa-9085-ad2d537dae32 100Mi RWX nfs-client 5m26s NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-f4a17b71-d79d-44fa-9085-ad2d537dae32 100Mi RWX Delete Bound default/nginx-log nfs-client 5m26s NAME READY STATUS RESTARTS AGE pod/nfs-subdir-external-provisioner-7fdb777787-nc6rw 1/1 Running 0 66m pod/nginx-6dd49b6ccb-v6ggj 1/1 Running 0 5m26s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 27h service/nginx ClusterIP 10.43.195.255 \u0026lt;none\u0026gt; 80/TCP 4m3s 可以看到\nnginx pod 已經部署完成並進入到 Running 的狀態 自動產生了一個 Persistent Volume 讓 PVC 使用，並進入可用的 Bound 狀態 開啟兩個 Terminal ， 執行 kubectl port-forwarding 的指令，並用 curl 打一個 request 到 nginx pod\n$ kubectl port-forward service/nginx 8080:80 Forwarding from 127.0.0.1:8080 -\u0026gt; 80 Forwarding from [::1]:8080 -\u0026gt; 80 Handling connection for 8080 Handling connection for 8080 # another terminal $ curl http://localhost:8080 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; 最後登入到 VM nfs-server 內，查看 NFS server 分享的資料夾根目錄\n$ multipass shell nfs-server ... ubuntu@nfs-server:~$ ls storage/ default-nginx-log-pvc-f4a17b71-d79d-44fa-9085-ad2d537dae32 ubuntu@nfs-server:~$ ls storage/default-nginx-log-pvc-f4a17b71-d79d-44fa-9085-ad2d537dae32 access.log error.log ubuntu@nfs-server:~$ cat storage/default-nginx-log-pvc-f4a17b71-d79d-44fa-9085-ad2d537dae32/access.log 127.0.0.1 - - [27/Oct/2023:13:17:54 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.84.0\u0026#34; \u0026#34;-\u0026#34; 127.0.0.1 - - [27/Oct/2023:13:17:56 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.84.0\u0026#34; \u0026#34;-\u0026#34; 可以看到 storage/ 下建立了一個資料夾 default-nginx-log-pvc-f4a17b71-d79d-44fa-9085-ad2d537dae32 提供給剛才建立的 Persistent Volume 使用 ， 並且 Nginx 也將 access log 內容成功寫入到檔案中。\n","permalink":"https://blog.zhengweiliu.com/posts/devops/k3s-nfs-server/","summary":"之前讀過 Shawn Ho 大大的在GKE上使用ReadWrite Many的Disk ， 突然意識到 multipass 產生的 Ubuntu VM ，不就是現成的 Filesystem ! 只要在 Ubuntu 上安裝了 NFS server ， 並在其他 VM 上安裝 NFS client ， 那應該就能新增使用 NFS 的 Storage Class 了！查了一些資料後發現可行，於是就手動實做看看。","title":"透過 NFS Server 在 K3s cluster 新增 RWO Storage Class"},{"content":"這篇文章主要記錄在 Mac M2 上安裝設定 K3s 的方法\nK3s是輕量化的 Kubernetes，由於先前我都是使用Docker Desktop Kubernetes，因為 Docker Desktop Kubernetes 是 single-node Kubernetes or Docker Swarm cluster，在local 部署 Pod 時也無法實際測試 affinity 功能，因此就想玩玩看 K3s 。(通過 CKA 考試後也就漸漸忘了怎麼從零開始使手動建置一個 Kubernetes Cluster \u0026hellip; )\nmultipass 為了實現在 local 架設一個 multi nodes 組成的 cluster ， 首先是需要在 local 建立 VMs ， 這邊我選用 multipass，可以簡單並且快速的建立 Ubuntu VM ，並且編輯設定 VM 規格的方式也是簡單直覺的指令。\n在 Mac M2 上可以透過 brew 安裝 multipass\nbrew install multipass 在這個例子中，我用 multipass 啟用兩台 VM ， 分別是作為 control plane 的 server 以及 data plane 的 worker\n先建立 node server\nmultipass launch -n server -c 1 -m 2G -d 5G 22.04 -n 設定 node name -c 設定 node vCPU -m 設定 node memory -d 設定 node 22.04 設定 ubuntu image 的版本 再來建立 node worker\nmultipass launch -n worker -c 1 -m 2G -d 5G 22.04 透過 multipass list 指令，就能看到目前已經建立出來的 VMs\n$ multipass list Name State IPv4 Image server Running 192.168.64.2 Ubuntu 22.04 LTS worker Running 192.168.64.3 Ubuntu 22.04 LTS K3s 接著分別在兩台 VM 安裝 K3s ， 首先透過 multipass 指令登入 node server ， 並安裝 K3s\n$ multipass shell server ... ubuntu@server:~$ curl -sfL https://get.k3s.io | sh - [INFO] Finding release for channel stable [INFO] Using v1.27.6+k3s1 as release [INFO] Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.27.6+k3s1/sha256sum-arm64.txt [INFO] Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.27.6+k3s1/k3s-arm64 [INFO] Verifying binary download [INFO] Installing k3s to /usr/local/bin/k3s [INFO] Skipping installation of SELinux RPM [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s [INFO] Creating killall script /usr/local/bin/k3s-killall.sh [INFO] Creating uninstall script /usr/local/bin/k3s-uninstall.sh [INFO] env: Creating environment file /etc/systemd/system/k3s.service.env [INFO] systemd: Creating service file /etc/systemd/system/k3s.service [INFO] systemd: Enabling k3s unit Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service → /etc/systemd/system/k3s.service. [INFO] systemd: Starting k3s 安裝完成後，先打印出 node server 上的 kubeconfig 設定 ， 以便之後可以在 local 直接透過 kubectl 訪問 k3s cluster\n$ cat /etc/rancher/k3s/k3s.yaml apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS... server: https://192.168.64.2:6443 name: default contexts: - context: cluster: default user: default name: default current-context: default kind: Config preferences: {} users: - name: default user: client-certificate-data: LS0tLS... client-key-data: LS0tLS... 把這邊的 clusters 和 users 內容貼回 local 端 ~/kube/config 內，新增對應的 contexts ，並記得將上面 clusters.cluster.server 的 IP 換成 node server 的 IP 即可。 以我的例子，server IP 是 192.168.64.2。 同時，也建議把上面 cluster user context 的 name 更換成容易識別的名稱，比如 k3s。設定完就可以登出 node server。\n這時先在 local 用 kubectl config 查看剛才設定好的ㄚ k3s cluster ， 並將 kube context 切換到 k3s cluster 。\n# list kube contexts $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE k3s k3s k3s # set kube context $ kubectl config set-context k3s Context \u0026#34;k3s\u0026#34; modified. # confirm context switched kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * k3s k3s k3s 到這一步，可以先驗證是否可以正常訪問 k3s cluster\n# verify k3s cluster-info $ kubectl cluster-info Kubernetes control plane is running at https://192.168.64.2:6443 CoreDNS is running at https://192.168.64.2:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://192.168.64.2:6443/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy # verify k3s cluster nodes $ kubectl get nodes NAME STATUS ROLES AGE VERSION server Ready control-plane,master 18m v1.27.6+k3s1 此時 k3s cluster 應該只有一台 node server ， 接著要將 multipass vm worker 註冊成 k3s cluster 的 node ， 為此需要 node server (control-plane) 的 IP 與 token ， 讓 worker 可以通過 node 的註冊驗證\n# get node server ip and token $ multipass exec server sudo cat /var/lib/rancher/k3s/server/node-token K10ad55509e50dcc23a7e06d5a8e115668d97a31fa0b2d225462d8ee82aa89827e8::server:d91bbb9e9a3bae55e412a12ccc6ec307 $ multipass info server | grep IP IPv4: 192.168.64.2 登入 node worker ， 安裝 k3s 時帶入 control-plane 的 token 和 IP\n$ multipass shell worker ... ubuntu@worker:~$ curl -sfL https://get.k3s.io | K3S_URL=https://192.168.64.2:6443 K3S_TOKEN=\u0026#34;K10ad55509e50dcc23a7e06d5a8e115668d97a31fa0b2d225462d8ee82aa89827e8::server:d91bbb9e9a3bae55e412a12ccc6ec307\u0026#34; sh - [INFO] Finding release for channel stable [INFO] Using v1.27.6+k3s1 as release [INFO] Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.27.6+k3s1/sha256sum-arm64.txt [INFO] Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.27.6+k3s1/k3s-arm64 [INFO] Verifying binary download [INFO] Installing k3s to /usr/local/bin/k3s [INFO] Skipping installation of SELinux RPM [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s [INFO] Creating killall script /usr/local/bin/k3s-killall.sh [INFO] Creating uninstall script /usr/local/bin/k3s-agent-uninstall.sh [INFO] env: Creating environment file /etc/systemd/system/k3s-agent.service.env [INFO] systemd: Creating service file /etc/systemd/system/k3s-agent.service [INFO] systemd: Enabling k3s-agent unit Created symlink /etc/systemd/system/multi-user.target.wants/k3s-agent.service → /etc/systemd/system/k3s-agent.service. [INFO] systemd: Starting k3s-agent 登出 node worker ， 在 local 端執行 kubectl 指令查看 k3s cluster nodes\n$ kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME server Ready control-plane,master 26m v1.27.6+k3s1 192.168.64.2 \u0026lt;none\u0026gt; Ubuntu 22.04.3 LTS 5.15.0-86-generic containerd://1.7.6-k3s1.27 worker Ready \u0026lt;none\u0026gt; 28s v1.27.6+k3s1 192.168.64.3 \u0026lt;none\u0026gt; Ubuntu 22.04.3 LTS 5.15.0-86-generic containerd://1.7.6-k3s1.27 此時 node worker 也被註冊到 k3s cluster 上，且兩台 node 的 OS-IMAGE 都是先前在 multipass 建立 VM 時使用的 Ubuntu 22.04 版本，container runtime 則是 k3s 提供的 1.27 版本。\n","permalink":"https://blog.zhengweiliu.com/posts/devops/mac-m2-k3s/","summary":"K3s是輕量化的 Kubernetes，由於先前我都是使用Docker Desktop Kubernetes，因為 Docker Desktop Kubernetes 是 single-node Kubernetes or Docker Swarm cluster，在local 部署 Pod 時也無法實際測試 affinity 功能，因此就想玩玩看 K3s 。","title":"Mac M2 安裝 K3s"},{"content":"利用 Abstract Factory Pattern - Design Patterns 學習設計模式 Abstract Factory Pattern，並利用 Python 撰寫 sample\n摘要 簡而言之， Factory Method Pattern 描述的是定義製作者(Creator)和產品(Product)間的關係 ， Abstract Factory Pattern 則是一種將多個製作者(Creator) 群組化的關係；\nAbstract Factory Pattern 是基於 Factory Method Pattern 建構而成的一種設計模式，因此需要先理解 Factory Method Pattern 的核心精神與設計方式\nDesign Patterns - Factory Method Pattern Date: 2023-03-08 \u0026nbsp; Categories: #Design Patterns\u0026nbsp; Factory Method Pattern 簡明扼要的說，就是定義製作者(Creator)和產品(Product)間的關係。我們並不需要在意具體是哪個製作者生產產品，也不需要在意製作者用何種方式生產特定產品，因為我們關注的部分為，是否可拿到特定產品。 ...... 在影片中提供的案例是應用程式的 UI 設計 :\n當一組應用程式的 UI 控制項固定後，每個控制項功能背後要達成的目的是固定的，但依據作業系統的不同，如: Windows、Mac OS 、 Linux，控制項功能需要有不同的實作方式，如: 呼叫不同的 driver 或 kernel APIs，以達成目的，而 Factory Method Pattern 恰好適合解決這個問題；\n因此，透過宣告 Abstract Factory Pattern，將每個控制項功能當作一種產品(Product)，透過不同的製作者(Creator)來產生控制項，並將適用於同一種作業系統的製作者(Creator)整合成一個群組，當 需要渲染出符合當前作業系統的UI 時，只要採納對應的群組生成控制項功能就能達成目的，這也表達了 Abstract Factory Pattern 的設計理念\n舉例 假設有一速食快餐店要推出漢堡系列的套餐，套餐的組合中有主餐漢堡、副餐炸薯條和一杯飲料；其中炸薯條和飲料可以點選不同的尺寸選擇，並且預設為中份薯條和大杯飲料。\n在這個場景中，使用 Abstract Factory Pattern 來實現套餐組合的過程。\n產品 (Product) 整個套餐是透過不同的產品組合而成，假設當前已知的產品如下表\n產品類型 產品名稱 單價 單位 肉 (Meat) 牛肉 (Beef) 30.0 片 肉 (Meat) 豬肉 (Pork) 25.0 片 肉 (Meat) 魚肉 (Fish) 20.0 片 飲料 (Drink) 可樂 (Coke) 依據尺寸 杯 飲料 (Drink) 蘇打 (Soda) 依據尺寸 杯 飲料 (Drink) 綠茶 (GreenTea) 依據尺寸 杯 薯條 (Fries) 薯條 (Fries) 依據尺寸 份 漢堡 (Burger) 漢堡 (Burger) 依據肉量 + 20.0 份 從產品表中可以整理出下列資訊 :\n漢堡 : 漢堡由麵包和肉類組成。依據加入的肉類與肉片量，有不同的售價。 飲料 : 使用不同尺寸的杯子裝飲料。不同尺寸有不同售價。 炸薯條 : 薯條有不同尺寸份量。不同尺寸有不同售價。 進一步對飲料尺寸、薯條尺寸，與漢堡組成的方式做不同定義\n飲料尺寸\n利用枚舉 (enumeration) 定義飲料尺寸與售價，並建立飲料尺寸的資料結構來儲存尺寸和售價的資訊。\n飲料尺寸 售價 特大(venti) 10.0 大(grande) 7.5 中(tall) 5.0 小大(short) 2.5 薯條尺寸\n利用枚舉 (enumeration) 定義薯條尺寸與售價，並建立薯條尺寸的資料結構來儲存尺寸和售價的資訊。\n薯條尺寸 售價 特大(supersize) 10.0 大(large) 7.5 中(medium) 5.0 漢堡組成\n將漢堡組成定義成一種產品，指定漢堡建構子需要傳入肉類(meat)和肉片數量(piece) 以產生不同類型的漢堡。\n製作者 (Creator) 透過枚舉建立尺寸資訊的資料結構，當製作者需要生產不同尺寸的飲料/薯條時，只需要將資料結構傳遞給飲料/薯條的建構子，便能簡化產生具體尺寸的飲料/薯條的產品。\n另外，將漢堡組成定義成產品的好處，在於製作者要生產漢堡時，也只需要傳遞對應的肉類和肉片數量，便可以生產不同具體類型的漢堡，如：牛肉漢堡、豬肉漢堡或魚肉漢堡。\n為了滿足套餐組合的需求，分別需要定義漢堡製作者(BurgerCreator)、飲料製作者(DrinkCreator) 和薯條製作者(FriedCreator)。\n同時，為了讓套餐組合時可直接選用，還需要分別建立不同的 漢堡/飲料和尺寸/薯條尺寸 的製作者，並讓其繼承該項產品類型的製作者。\n製作者類型 製作者名稱 建構子需求 漢堡製作者 牛肉漢堡製作者 肉類: 牛肉, 肉片量: 1 片 (預設) 漢堡製作者 豬肉漢堡製作者 肉類: 豬肉, 肉片量: 1 片 (預設) 漢堡製作者 魚肉漢堡製作者 肉類: 魚肉, 肉片量: 1 片 (預設) 飲料製作者 可樂製作者 飲料尺寸: [ 特大、大、中、小 ] 飲料製作者 蘇打製作者 飲料尺寸: [ 特大、大、中、小 ] 飲料製作者 綠茶製作者 飲料尺寸: [ 特大、大、中、小 ] 薯條製作者 薯條製作者 薯條尺寸: [ 特大、大、中 ] 套餐組合 (combo) 套餐組合直接從已定義好的各類產品製作者進行挑選，以組成漢堡、飲料和薯條的套餐內容；透過選擇不同的產品製作者，便能夠快速地建立起不同的套餐內容\n套餐名稱 套餐內容 牛肉漢堡套餐 [牛肉漢堡、特大杯可樂、特大份薯條] 豬肉漢堡套餐 [豬肉漢堡、大杯可樂、中份薯條] 魚肉漢堡套餐 [魚肉漢堡、大杯綠茶、中份薯條] Demo 用 Python 撰寫 Abstract Factory Pattern 的示例。\nSource code\n示例 : 顯示套餐內容 不同的具體套餐組合都繼承了套餐(Combo) 類別，若想顯示不同具體的套餐的內容時，便可調用同一組程式碼來達成目的\norder information\n# 傳入 Combo 類，即具體的套餐 def order_information(combo: Combo): # 獲取套餐內的漢堡 burger = combo.get_burger() # 獲取套餐內的飲料 drink = combo.get_drink() # 獲取套餐內的薯條 fries = combo.get_fries() # 整合套餐內容的產品資訊，並計算整份套餐的售價 description = { \u0026#39;burger\u0026#39;: burger.get_description(), \u0026#39;drink\u0026#39;: drink.get_description(), \u0026#39;fries\u0026#39;: fries.get_description(), \u0026#39;total_price\u0026#39;: burger.price + drink.price + fries.price } # 打印套餐類型的售價 print(f\u0026#39;Combo: {combo.__class__.__name__}\u0026#39;) print(description) print(\u0026#39;-\u0026#39; * 30) 主程式\nif __name__ == \u0026#39;__main__\u0026#39;: # 生產豬肉漢堡套餐 pork_combo = PorkBurgerCombo() # 打印套餐內容 order_information(pork_combo) # 生產魚肉漢堡套餐 fish_combo = FishBurgerCombo() # 打印套餐內容 order_information(fish_combo) # 生產牛肉漢堡套餐 hamburger_combo = SupersizeHamburgerCombo() # 打印套餐內容 order_information(hamburger_combo) 執行結果 小結\n在 Factory Method Pattern 和本篇 Abstract Factory Pattern 中，我沒有使用工廠這一翻譯措辭的原因:\n我認為工廠的定義是可變的，依據不同的場景會對工廠一詞有不同的定義； 但工廠的目的是不變的，即生產產品，並且不需要關注由誰生產產品。 因此，我才使用了 產品、製作者、製作者群組 等詞來表達我認知的 Factory Method Pattern 和 Abstract Factory Pattern。\n在本文的示例中所採用的編成架構，將各項具體單品(產品)、具體生產者，以及套餐組合進行分層，並且讓每一層只專注自己的變動，如 :\n產品層只需要關注單品的參數，如名稱、售價，且單品參數的改變並不影響生產者層和套餐組合層的處理方式，以此為產品層異動保留了極大的彈性\n生產者層同樣只考慮該如何生產出具體的產品，圍繞著建構子傳遞的肉類或尺寸的既有資訊進行拓展，而不是去改變肉類或尺寸的資訊\n套餐組合層也僅考慮選用哪些具體生產者，來定義套餐組合的內容\n未來無論是要增加新的套餐組合或是新的產品，不會(或非常小)對於現有的程式碼與軟體架構造成改動。\n我覺得這是一個，很好的闡述了鬆散耦合軟體框架的範例\n","permalink":"https://blog.zhengweiliu.com/posts/design-pattern/abstract-factory-pattern/","summary":"簡而言之， Factory Method Pattern 描述的是定義製作者(Creator)和產品(Product)間的關係 ， Abstract Factory Pattern 則是一種將多個製作者(Creator) 群組化的關係；Abstract Factory Pattern是基於Factory Method Pattern建構而成的一種設計模式，因此需要先理解 Factory Method Pattern 的核心精神與設計方式。","title":"Design Patterns - Abstract Factory Pattern"},{"content":"利用 Factory Method Pattern - Design Patterns 學習設計模式 Factory Method Pattern，並利用 Python 撰寫 sample code.\n摘要 Factory Method Pattern 簡明扼要的說，就是定義 製作者(Creator) 和 產品(Product) 間的關係\n定義製作者可生產的產品，需要產品的時候才讓製作者生產出產品 不同的製作者可以有不同的製作方式，但統一使用生產 (produce) 來明確要求進行產品產出的行為 依據上述兩點可以得知 :\n為產品定義一個基礎類別，並將同類型的 具體產品(Concrete Product) 繼承該基礎類別 為製作者定義一個基礎類別，並將產出同類型產品的 具體製作者(Concrete Creator) 繼承該基礎類別 不同類型的具體產品，應有不同的基礎產品類別 產出不同類型產品的具體製作者，應有不同的基礎製作者類類別 當需要某一特定產品時，委託可生產該產品的製作者進行生產並交付；\n我們並不需要在意具體是哪個製作者生產產品， 也不需要在意製作者用何種方式生產特定產品， 因為我們關注的部分為，是否可拿到特定產品。 註:在 《Design Pattern》書中有給出明確的定義和說明。我覺得太過繞口，所以用自己的想法來表達\n舉例 假設有一個工業物流的場景，這個場景中將原礦定義為一級產品，並包含了二級產品與三級產品，每一層級的產品都由具體的工廠提供。考量日後存在統計庫存的需求，對於庫存與生產管理的要求如下 :\n需要對原料產品規範對應生產的具體工廠 工廠需檢視當前原料庫存量，以判斷是否可生產產品 若一個工廠具備生產多種具體產品的能力，則假設庫存原料原料充足 若一個工廠具備生產多種具體產品的能力，則該工廠每次生產一樣隨機產品 產品 假設已知的所有產品如下表所示\n產品名稱 層級 需求原料 [品名, 數量] 產出數量 鐵礦石 一級產品 1 銅礦石 一級產品 1 鐵板 二級產品 [鐵礦石, 1] 1 銅線 二級產品 [銅礦石, 1] 3 電路板 三級產品 [鐵板, 1], [銅線, 2] 1 一級產品作為二級產品的原料、二級產品作為三級產品的原料，可得知\n產品可能作為下一層級的生產原料 產品需要標註原料資訊、產出資訊，以及產品名稱 為此，將需求原料定義成一個資料組結構作為生產過程的依據，這會帶來以下好處\n當製作者在生產過程需要參考需求原料時，可從當前已知的資料組結構中，提取需要的參數資料，而不需要考慮整體資料組結構包含什麼 保留需求原料資訊的擴充彈性，即便是刪除資料組結構中既定的參數，也只需要修改使用了刪除參數的生產過程，而不需要檢視所有的製作者 將需求原料資訊打包成一個整體，並傳遞給接收者使用，這是 依賴注入(Dependency Injection) ， 一種簡單有效的降低軟體耦合的方法。\n製作者 在定義具體的製作者時，需要考量的是該製作者可以提供什麼產品； 製作者可以具備生產同類型中多種具體產品的能力，但每次只能生產出一個特定的產品。\n為此，先定義出一個符合要求的製作者 DefaultCreator 的生產過程\ndef produce(self) -\u0026gt; Union[Product, None]: # 取得產品資訊 product_description = self.product.get_description() # 取得產品的原料需求資訊 require_materials: list[Material] = product_description.materials material_to_product = {} # 檢核庫存 for m in require_materials: # 判斷目前庫存中是否具備需求的原料 if not self.check_material(m.require.name): # 若庫存中不具備需求原料，則無法生產產品 return None # 計算該原料最多可製作的產品數量 material_to_product[m.require.name] = self.materials[m.require.name] // m.amount # 從每項原料可製作的產品數量中取最小值。 若最小值不大於 0 ， 則表示無法生產產品 product = self.product if min(material_to_product.values()) \u0026gt; 0 else None return product 若有不同於 DefaultCreator 的生產方式，則每種生產方式都可以定義一個對應的製作者。 假設希望使用隨機挑選產品來進行生產，定義一個 RandomCreator 的生產方式如下\ndef produce(self) -\u0026gt; Union[Product, None]: # 定義隨機選取的產品清單 product_list = [ IronOre, CopperOre, IronPlate, CopperCable, CircuitBoard ] # 使用隨機方法圖選產品，並直接提供該產品 self.product = product_list[random.randrange(len(product_list))]() return self.product Demo 用 Python 撰寫 Factory Method Pattern 的示例。\nSource code\n示例 : 指定生產具體產品 這個示例指定了鐵板作為 DefaultCreator 生產的具體產品，同時要為檢視庫存與投料提供對應的方法\n# 指定鐵板為 DefaultCreator 生產的具體產品 factory = DefaultCreator(product=IronPlate()) # 檢視當前庫存 factory.list_materials() print(f\u0026#39;DefaultCreator produce : {factory.produce()}\u0026#39;) # 投料 factory.update_material(material=Material(require=IronOre(), amount=10)) # 再次檢視當前庫存 factory.list_materials() # 生產具體產品 print(f\u0026#39;DefaultCreator produce after update materials : {factory.produce()}\u0026#39;) 執行結果 示例 : 隨機生產具體產品 這個示例中， RandomCreator 會從所有產品中，隨機挑選一種產品進行生產\n# RandomCreator 隨機挑選產品進行生產 random_factory = RandomCreator() # 第一次隨機生產 print(f\u0026#39;RandomCreator produce : {random_factory.produce()}\u0026#39;) # 第二次隨機生產 print(f\u0026#39;RandomCreator produce : {random_factory.produce()}\u0026#39;) # 第三次隨機生產 print(f\u0026#39;RandomCreator produce : {random_factory.produce()}\u0026#39;) 執行結果 示例 : 隨機生產一級產品 因一級產品和二級產品會作為下一層級的原料，這邊也選擇用隨機挑選生產產品，並用一級產品製造者 OreCreator 進行演示。和\n與 RandomCreator 不同的部分，則是在 生產(produce) 方法提供不同的產品列表。\n# OreCreator 生產方法 def produce(self) -\u0026gt; Union[Product, None]: # 僅選擇一級產品作為隨機挑選的產品清單 ore_list = [IronOre, CopperOre] self.product = ore_list[random.randrange(len(ore_list))]() return self.product 執行結果 ","permalink":"https://blog.zhengweiliu.com/posts/design-pattern/factory-method-pattern/","summary":"Factory Method Pattern 簡明扼要的說，就是定義製作者(Creator)和產品(Product)間的關係。我們並不需要在意具體是哪個製作者生產產品，也不需要在意製作者用何種方式生產特定產品，因為我們關注的部分為，是否可拿到特定產品。","title":"Design Patterns - Factory Method Pattern"},{"content":"利用 Decorator Pattern - Design Patterns 學習設計模式 Decorator Pattern，並利用 Python 撰寫 sample code.\n摘要 Decorator Pattern 透過修改已定義的行為，以擴展或變更其功能，而不需透過繼承和覆寫。\n使用組合 (composition) 替代繼承 (inherit)，可動態地添加或移除行為，且不需要在繼承關係中堆疊子類別。\nDecorator模式更具靈活性和可維護性，因此被廣泛地應用於軟體開發領域中。\n繼承 (inherit) 架構面對的問題 假設當前的議題是 : 要在不同的渠道，如: Google Ads、 Meta Ads 或 Instagram，投放廣告 ， 需要建立特定的資料結構來處理廣告文案與投放渠道，並採用繼承的方式來建置資料模型，資料模型間的關係初步設計可能會如下圖所示\n建立一個 AdContent 並定義廣告內容的行為，為文字廣告和圖片廣告分別建立 TextAd 和 ImageAd 並讓兩者繼承 AdContent；\n同時，每個投放渠道對於廣告文案的限制可能有所不同，利用繼承建立不同渠道的類別，如: GoogleTextAd 、 LineTextAd、MetaImageAd\u0026hellip;等等，透過覆寫來設定不同渠道的限制。\n基於上述設計的思考邏輯: 若後續迭代需求要求為 ImageAd 增加一個 split 的屬性，使得圖片廣告可用分割區塊的方式放入更多圖片，對已存在的 GoogleImageAd 、 MetaImageAd 和 InstagramImageAd 實現相關屬性的操作方式。\n顯而易見，AdContent 理應為 content 定一個泛用性較高的資料模型。然而，若當前的資料模型無法滿足新需求，則資料模型的調整也會影響整個繼承鏈上的類別。\n根本的問題在於採用繼承鏈的架構，當新需求只能適用於特定或部份渠道，卻要對繼承鏈上所有的類別進行調整；當繼承鏈過於龐大或負責的時候，會導致開發迭代版本需要更多的時間成本。\n用 Composition 替代 Inherit Decorator Pattern 則透過包裹 (wrapper) 方式來達成組合目的。\n因為 AdContent 提供對文案的操作行為，而投放渠道或是廣告類型 ，如:文字或圖片，對文案而言都是附加描述，因此可以將 AdContent 定義為最基礎的元件 (component) ；\n附加描述可以對元件的行為 (behavior) 或執行結果 (result)，做進一步的拓展、擴充或修改，因此將附加描述定義為 decorator 。 因 decorator 是獨立的，因此也可以被其他的 decorator 包覆。\n若 decorator 修改執行 behavior 的參數，或是修改了 behavior 的執行結果，則會依據 decorator 的包覆順序做出對應的改變 :\n若有多個 decorator 對同一個 behavior 的傳入參數進行修改，則傳入參數的改變會從 outer decorator 影響 inner decorator ，最終影響 component 執性 behavior 時的傳入參數\n若有多個 decorator 對同一個 behavior 的執行結果進行修改，則從 component 回傳的執行結果開始，由 inner decorator 的修改影響 outer decorator 的修改，直至最外層的 outer decorator 回覆執行結果。\n設計方法 依據先前的小節說明整理出幾點資訊\n需要定義出 元件 (component) 和 附加描述 (Decorator) 執行 behavior 是由外層 decorate 到內層 component ， behavior 的 signature 需統一，可判斷附加描述是元件的一種 (is-a) 附加描述可以疊加、 outer decorator 需要呼叫 inner decorator ，可判斷附加描述應具備 inner decorator 或 component (has-a) 再依據上述三點描述，以廣告文案投放至不同渠道的議題為例，用 Decorator Pattern 方式設計類別之間的關係，如下圖所示\n令 Decorator 繼承 Component 類，並在 Decorator 的建構方法傳入 component 實體 ( object | instance ) ， 使 decorator 具備包裹 (wrapper) 其他 decorator 和 component 的能力。\nDemo 用 Python 撰寫 Decorator Pattern 的示例。\nSource code\n以下分別提供了 4 種示例\n示例 1 Single decorator wrapped component\nprint(f\u0026#39;Example 1 - text ad decorator output:\u0026#39;) print(TextAdsDecorator(text_ad).get_content()) print(f\u0026#39;Example 1 - image ad decorator output:\u0026#39;) print(ImageAdsDecorator(image_ad).get_content()) 示例 2 Apply multiple decorators\nprint(f\u0026#39;Example 2 - Google Ads with text ad decorator output:\u0026#39;) print(AdsChannelDecorator(TextAdsDecorator(text_ad), channel=\u0026#34;Google Ads\u0026#34;).get_ad_channel()) print(f\u0026#39;Example 2 - Meta Ads with text ad decorator output:\u0026#39;) print(AdsChannelDecorator(TextAdsDecorator(text_ad), channel=\u0026#34;Meta Ads\u0026#34;).get_ad_channel()) 示例 3 Apply multiple decorators with different order\nprint(f\u0026#39;Example 3 - image ads decorator output:\u0026#39;) print(AdsChannelDecorator(ImageAdsDecorator(image_ad), channel=\u0026#34;Youtube Ads\u0026#34;).get_description()) print(f\u0026#39;Example 3 - exchange decorators output:\u0026#39;) print(ImageAdsDecorator(AdsChannelDecorator(image_ad, channel=\u0026#34;Instagram Ads\u0026#34;)).get_description()) 示例 4 Inherit decorator class to setting different behavior\nprint(f\u0026#39;Example 4 - inherit image ads decorator output:\u0026#39;) google_image_ads = GoogleImageAdDecorator(AdsChannelDecorator(image_ad, channel=\u0026#39;Google Ads\u0026#39;)) # assume google image ads size limitation with: 320 \u0026lt;= width \u0026lt;= 1920, 480 \u0026lt;= height \u0026lt;= 1080 google_image_ads.width = 2560.0 google_image_ads.height = 1440.0 print(f\u0026#39;Over Google Image Ads size limitation\u0026#39;) print(google_image_ads.get_description()) ","permalink":"https://blog.zhengweiliu.com/posts/design-pattern/decorator-pattern/","summary":"Decorator Pattern 透過修改已定義的行為，以擴展或變更其功能，而不需透過繼承和覆寫。 使用組合 (composition) 替代繼承 (inherit)，可動態地添加或移除行為，且不需要在繼承關係中堆疊子類別。 Decorator模式更具靈活性和可維護性，因此被廣泛地應用於軟體開發領域中。","title":"Design Patterns - Decorator Pattern"},{"content":"利用 Observer Pattern - Design Patterns 學習設計模式 Observer Pattern，並利用 Python 撰寫 sample code.\n摘要 當有 A 、 B 兩類實體，其中實體 B 類實體為了某些原因，需要得知 A 類的某些狀態是否有改變； 若在 Server-Client 的架構中，假設 Server 為 A 類、Client 為 B 類 ，最簡單的想法是讓 Client 定期去問 Server 狀態是否已經改變，這樣的方式稱為輪詢(Polling) 然而，輪詢有些顯而易見的缺點：\n大多時候輪詢的答案都是狀態未改變，屬於無效的輪詢 若同時有多個 Client 實體對 Server 進行輪詢，會降低 Server 的處理效能 讓 Server 主動**推送(Push)**狀態變更的信號給 Client，可以有效的改善上述的缺點 因此在 Observer Pattern 中，將會明確定義出兩種角色 :\nIObservable : 被觀察者，如上述的 Server (A類) IObserver : 觀察者，如上述的 Client (B類) Pattern [0..*] 表示 IObservable 和 IObserver 的關係為 1-to-many， 即 1 個 IObservable 可以同時註冊多個 IObserver 。 IObservable 如上圖所述，在 IObservable 介面定義 register(註冊) 、 unregister(註銷) 和 notify(通知) 三種基礎方法。\nregister : 註冊 IObserver 。 使得 notify() 方法可以通知到已註冊的 Observer。 unregister : 註銷 IObserver 。 已註銷的 IObserver 不會收到 notify 方法的通知 notify : 促使 IObservable 通知目前已註冊且未註銷的 IObserver ， 使得符合條件的 IObserver 可以執行 update() IObserver IObserver 中定義僅定義了 update() 方法，問題是 IObserver 需要和誰取得更新的狀態或訊息 ？\n在 IObservable 的描述中，已知 IObservable has-a (or has-many) IObserver ， 因此 IObservable 執行 notify() 方法時會有具體的對象可以呼叫， 但 IObserver 卻沒有定義一個可以 update() 的對象。\n為此，仍需實做輪詢中的一個重點步驟 : 詢問(Ask) ，同時為了避免 Circular Imports的情形，詢問(Ask) 的實作會放在繼承了 IObserver 的子類建構子中，使得 IObserver 的子類能夠獲取具體向哪一個 IObservable 取得更新的狀態與訊息；\n實際上 詢問(Ask) 是一個雙向的溝通，即：有問有答，因此在繼承了 IObservable 的子類中，也會定義對應的方法讓 IObserver 的子類可以獲取更新的狀態和訊息。\nDemo 整併上述提及 詢問(Ask) 的實作 ，並定義出 IObservable 和 IObserver 的子類，如下圖所示 用 Python 撰寫 Observer Pattern 的示例。\nSource code\n# initial publisher and subscribers topic = Publisher() sub_1 = Subscriber(publisher=topic) sub_2 = Subscriber(publisher=topic) sub_3 = Subscriber(publisher=topic) # register subscribers to publisher topic.register(subscriber=sub_1) topic.register(subscriber=sub_2) topic.register(subscriber=sub_3) # set message for publisher topic.message = \u0026#39;Demo Observer Pattern\u0026#39; print(f\u0026#34;{\u0026#39;-\u0026#39;*20}\\n\u0026#34;) # change message to verify notify and update topic.message = \u0026#39;Change message 2nd times\u0026#39; print(f\u0026#34;{\u0026#39;-\u0026#39; * 20}\\n\u0026#34;) # unregister sub_1 topic.unregister(subscriber=sub_2) # change message to verify notify and update topic.message = f\u0026#39;Unregister sub 2 (id: {sub_2.id})\u0026#39; 執行結果 撰文的當下是 2023 年，而 YT 影片大約是 2017 年的產物，以 2023 年應用發展來看，我覺得 Observer Pattern 最常見的一種應用(或者說演化)，就是 PubSub Pattern；\n因此，我也直接以 Simple Pub/Sub 的概念來實做這個 Demo。\n","permalink":"https://blog.zhengweiliu.com/posts/design-pattern/observer-pattern/","summary":"在 Observer Pattern 中，將會明確定義出兩種角色 : 1. IObservable : 被觀察者，如上述的 Server (A類), 2. IObserver : 觀察者，如上述的 Client (B類) 。讓 Server 主動推送(Push)狀態變更的信號給 Client，可以有效的改善輪詢帶來的缺點。","title":"Design Patterns - Observer Pattern"},{"content":"利用 Strategy Pattern - Design Patterns 學習設計模式 Strategy Pattern，並利用 Python 撰寫 sample code.\n摘要 相對於繼承(inherit)， Strategy Pattern 則是組合優於繼承(composition over inheritance) 的精神。\n假設有一個薪水計算器要給兩個不同的客戶使用 : 速食業客戶以每小時時薪和工時來核算薪水，外送業客戶以每單獎金和總外送單數來核算薪水。\n薪水計算器需要提供給不同業者不同核算薪水的方法， Strategy Pattern 則提供了一種方式，使得不同業者可以使用同一個計算器，並選擇不同的核算方式，來獲取薪水計算的結果。\n繼承(inherit) 繼承的作法可以讓 子類 (sub classes) 具有 父類 (parent classes) 定義過的屬性(attributes) 和 方法(method) ，因此可以重複利用(re-used) 父類中已經撰寫過的片段程式碼。\n當子類需更改父類定義的屬性或方法時，需要透過 覆寫(overwriting) 的方式在子類重新定義。\n下圖為繼承的示例 Wild Duck 、 City Duck 和 Cloud Duck 都是繼承 Duck 的子類，因此也能解釋這些子類是一個 (is-a) Duck 類 ，而 Office Duck 繼承了 City Duck ，也能夠解釋 Office Duck is-a City Duck 。\nCity Duck 能夠被解釋為 is-a Duck 類，這個解釋的關係也被 Office Duck 繼承了，因此也能夠說 Office Duck is-a Duck 類。\n當子類覆寫父類的屬性或者方法後，繼承子類的子類 (以下簡稱孫子類) 繼承的是覆寫後的結果。然而，若孫子類希望使用的是父類的定義，這時是否又需要在對孫子類進行覆寫呢？\n舉個例子： City Duck 覆寫了 Duck 的 fly() 方法，但 Office Duck 卻希望使用 Duck 的 fly() 方法； 在遵從繼承原則的情況下， Office Duck 需要在對自己的 fly() 方法進行一次覆寫。\n在上述的例子中，當 fly() 的方法透過覆寫來進行變更，不可避免的會出現 ˋ相同的片段程式碼 出現在不同的地方，因此程式碼片段的重複使用性會大大降低，這也容易造成後續維護的困難，或是在需求擴增時對於方法的拓坦變得不那麼彈性。\n組合(composition) 與繼承不同之處在於，組合(composition) 利用 介面(interfaces) 拓展可執行的方法 : 透過條件判斷(或預先提供)的方式選擇實際執行片段程式碼，但在介面的定義中僅提供唯一的署名(signature)。\n實際執行片段程式碼的撰寫方式有很多，以下說明我採取的方式：\n使用 Abstract Class 定義父類，並透過繼承父類的子類，來實作不同的執行片段程式碼 在上圖示例中，將 Duck 的 fly() 與 eat() 方法分別拓展成兩個介面， IFlyBehavior 與 IEatBehavior ，並分別定義不同的子類，如: SimpleEating CannotEating 繼承 IEatBehavior， SimpleFlying CannotFlying 繼承 IFlyBehavior ， 以撰寫不同情況的下需要執行的片段程式碼。\n將 FlyBehavior 和 EatBehavior 設定為 Duck 類的屬性，並在 Duck 類的 建構子(Constructor) ，要求將 FlyBehavior 和 EatBehavior 作為必要傳入參數。 這使得 Duck 類會有一個(has-a) FlyBehavior 和 有一個(has-a) EatBehavior。\n當 Duck 類建構子要求傳入的 Behavior 參數，使得 Duck 需要執行的方法變為可選擇 ，因此也能夠將原先建構的 繼承鏈關係 解耦。\n在編寫軟體時，只需要將符合場景需求的對應能力，以組合的方式輸入給 Duck 類建構子，變能預期軟體執行符合期望的行為。\nDemo 用 Python 撰寫 Strategy Pattern 的示例。 Salary Calculator 要求傳入 Paycheck Factor 作為屬性，並提供 salary_calculating() 方法來計算薪資。\nSource code\nfrom SalaryDef import SalaryCalculator from PaycheckFactor import SimplePaycheck, TicketPaycheck if __name__ == \u0026#39;__main__\u0026#39;: salary_calculator_for_employee_1 = SalaryCalculator(SimplePaycheck(employee_id=1002003)) salary_calculator_for_employee_2 = SalaryCalculator(TicketPaycheck(employee_id=3002001)) print(f\u0026#39;employee_1 salary of this month : {salary_calculator_for_employee_1.salary_calculating()}\u0026#39;) print(f\u0026#39;employee_2 salary of this month : {salary_calculator_for_employee_2.salary_calculating()}\u0026#39;) ","permalink":"https://blog.zhengweiliu.com/posts/design-pattern/strategy-pattern/","summary":"相對於繼承(inherit)， Strategy Pattern 則是組合優於繼承(composition over inheritance)的精神。假設有一個薪水計算器要給兩個不同的客戶使用 : 速食業客戶以每小時時薪和工時來核算薪水，外送業客戶以每單獎金和總外送單數來核算薪水。薪水計算器需要提供給不同業者不同核算薪水的方法， Strategy Pattern 則提供了一種方式，使得不同業者可以使用同一個計算器，並選擇不同的核算方式，來獲取薪水計算的結果。","title":"Design Patterns - Strategy Pattern"},{"content":"這篇文章主要記錄借助 ChatGPT 來修改履歷的過程\n由於正在尋找新的工作機會，但覺得自己實在是不擅長撰寫履歷，便想嘗試讓 ChatGPT 對於已寫好的初版履歷提供修改建議； 但轉念一想，何不乾脆請 ChatGPT 直接輸出修改好的內容，於是便有了這次嘗試。\nChatGPT ChatGPT 是由 OpenAI release 的一套對談式 AI Model (撰文當下已被 Microsoft 收購)；由於對談溝通的特性，目前也有許多利用 ChatGPT 進行二次開發的服務與產品，多數是在各行業領域以擔任助手的角色，提供使用者諸多便利性。\n在實際使用 ChatGPT 的過程，我感覺到 ChatGPT 應該是具備短期記憶的特性，而短期記憶又能夠讓我們建立起和 ChatGPT 的共識，令 ChatGPT 能夠明白當前問題的背景環境與假設條件，促使 ChatGPT 盡可能的回答出符合環境設定的回答。\n與 ChatGPT 建構共識 我覺得在需要溝通以合作解決問題的情境下，我必須先知道 ChatGPT 是否已具備了某些基礎能力，以夠應付接下來合作過程中我所提出的需求； 對此，我對 ChatGPT 進行了以下測試\n是否具備上傳檔案到 Google Driver 的能力 是否具備訪問 Public Files URL 的能力 檔案上傳測試 我在自己的 Google Driver 建立一個共用文件夾，並設定共用權限讓知道連結者可進行編輯，再對 ChatGPT 提出問題 ChatGPT 目前尚未具備上傳檔案的能力\n訪問 Public Files URL 測試 我在前一個測試中建立的公用資料夾中上傳一個簡單的純文字檔案，檔案內容為 Hello from ChatGPT! ，並對 ChatGPT 提問 ChatGPT 的輸出並非我想要的，於是繼續和 ChatGPT 溝通 ChatGPT 正確訪問了檔案連結並讀出內容；\n同時， ChatGPT 告訴我他透過執行了 Python script 來下載檔案以及輸出內容\n讀取履歷 我事先準備好一份撰寫完成的履歷，並將履歷設定為可被公開訪問，並請 ChatGPT 讀取履歷後產生一份適合我的自傳； 同時，也告訴 ChatGPT 目前我想尋找的職缺優先順序為何 在 ChatGPT 產生的自傳中，有 80% 描述符合我的過往經歷。\n追加條件以修改履歷 我進一步的追加一個條件\n假設每位 HR 只有 30秒的時間來查看一份履歷 並請 ChatGPT 依據條件，提出修改履歷的建議，以利增加履歷的獨特性與曝光度\n簡略列出 ChatGPT 提供吸引 HR 目光的建議\n1. 突顯關鍵字 2. 結構簡潔 3. 重點突出 4. 特色突顯 5. 具體數據 6. 簡明扼要 以及對履歷的修改建議\n1. 強調成果 2. 更好的排版 3. 添加關鍵字 4. 突出自己的技能 5. 更新簡歷內容 6. 自我介紹 調整自我介紹和自傳 依據 ChatGPT 的建議，我分別提供簡單的自我介紹、自傳以及工作經歷，並請 ChatGPT 協助修改，以便更加符合 HR 查看履歷時著重的重點\n自我介紹 自傳 工作經歷 我覺得在 ChatGPT 修改過後，讀起來確實通暢許多。\n結語 透過 ChatGPT 輔助修改履歷的過程，我覺得在和 ChatGPT 溝通的過程其實挺順暢的，簡單扼要並清楚的告訴 ChatGPT 當前遭遇的問題，以及相關的背景設定與假設條件，通常都能獲得不錯的結果。\n即使第一次輸出的結果並不符合我的期待，在明確告知 ChatGPT 後也能得到較為滿意的結果。\n而唯一需要注意的一點，是 ChatGPT 告訴我他會執行下載檔案的行為，以便讀取檔案內容；未來若有相似需要 ChatGPT 的協助，則應該更加注意是否已避開提供機敏性的資料。\n","permalink":"https://blog.zhengweiliu.com/posts/chatgpt/resume/","summary":"\u003cp\u003e透過 ChatGPT 輔助修改履歷的過程，我覺得在和 ChatGPT 溝通的過程其實挺順暢的，簡單扼要並清楚的告訴 ChatGPT 當前遭遇的問題，以及相關的背景設定與假設條件，通常都能獲得不錯的結果。\u003c/p\u003e\n\u003cp\u003e即使第一次輸出的結果並不符合我的期待，在明確告知 ChatGPT 後也能得到較為滿意的結果。\u003c/p\u003e\n\u003cp\u003e而唯一需要注意的一點，是 ChatGPT 告訴我他會執行下載檔案的行為，以便讀取檔案內容；未來若有相似需要 ChatGPT 的協助，則應該更加注意是否已避開提供機敏性的資料。\u003c/p\u003e\n","title":"借助 ChatGPT 修改履歷"},{"content":"這篇文章整理了個人使用 PaperMod theme 的設定\nWhat is Hugo-PaperMod ? Hugo PaperMod is a theme based on hugo-paper. The goal of this project is to add more features and customization to the og theme.\nHugo-PaperMod 使用 yml/yaml 格式提供所有示例，作者認為 yaml 的格式會比 toml 容易閱讀。\n安裝方式 Hugo 官方建議採用 submodule 的方式來安裝 hugo themes； 使用 submodule 來安裝 Hugo-PaperMod theme\ngit submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod git submodule update --init --recursive # needed when you reclone your repo (submodules may not get cloned automatically) 若要更新 Hugo-PaperMod theme :\ngit submodule update --remote --merge Hugo-PaperMod 也提供基本的 config.yml 和 page.md\nFeatures Features 設置參考來源 架站║ Hugo部落格與PaperMod主題\n建立 Archive 利用指令在 content 下建立 archive.md\nhugo new content/archive.md 編輯 archive.md 內容\n--- title: \u0026#34;Archive\u0026#34; layout: \u0026#34;archives\u0026#34; url: \u0026#34;/archives/\u0026#34; summary: archives --- 在 config.yml 的 menu 區塊設置 archive 按鈕\nmenu: main: - identifier: archives name: 📚 Archives url: /archives/ weight: 10 建立 Search Page 在 config.yml 加入以下設定\noutputs: home: - HTML - RSS - JSON # is necessary 利用指令在 content 下建立 search.md\nhugo new content/search.md 編輯 search.md 內容\n--- title: \u0026#34;Search\u0026#34; # in any language you want layout: \u0026#34;search\u0026#34; # is necessary url: \u0026#34;/search/\u0026#34; # description: \u0026#34;Description for Search\u0026#34; summary: \u0026#34;search\u0026#34; placeholder: \u0026#34;placeholder text in search input box\u0026#34; --- 對於不希望被 search page 搜尋到的文章，可以在文章開頭的 archtype 加入以下設定\nsearchHidden: true Setup Post Keywords 在文章開頭的 archtype 加入以下設定\nkeywords: [\u0026#34;keyword 1\u0026#34;, \u0026#34;keyword 2\u0026#34;, ...] Customizing Fusejs Options PaperMod 選用 Fusejs 作為 search 元件，參考Fusejs 參數項來自定義 search page 功能，如\nparams: fuseOpts: isCaseSensitive: false shouldSort: true location: 0 distance: 1000 threshold: 0.4 minMatchCharLength: 0 keys: [\u0026#34;title\u0026#34;, \u0026#34;permalink\u0026#34;, \u0026#34;summary\u0026#34;, \u0026#34;content\u0026#34;] Code Block 設定 max-height 在 assets/css/common/post-single.css 添加下面內容\n.post-content pre code { max-height: 30em; } Shortcode Shortcode 可以看作是「一小塊 HTML 程式片段」，與 Hugo Template 不同的是，前者通常運用在「插入特定用途」、「重複使用」的片段語法到 markdown 內容中，而後者則是作為 markdown content 的外殼載體、或是佈局規劃等，用以構成我們最後呈現的視圖頁面 (View)。 — 來源:iT邦\n更詳細的 shortcode 設定範例可以參考 Day 22. Hugo Shortcode 介紹 和 Hugo博客自定义shortcodes\n以下僅整理我目前有使用的\nPDP Shortcode 在 layouts/shortcodes/ 目錄下建立一個新文件 pdf.html，貼上下列內容\n\u0026lt;!DOCTYPE HTML\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; #googleslides_shortcodes { padding-bottom: 66%; position: relative; display: block; width: 100%; border-bottom: 5px solid; } #googleslides_shortcodes iframe { position: absolute; top: 0; left: 0 } \u0026lt;/style\u0026gt; \u0026lt;title\u0026gt;\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;googleslides_shortcodes\u0026#34;\u0026gt; \u0026lt;iframe id=\u0026#34;googleSlideIframe\u0026#34; width=\u0026#34;100%\u0026#34; height=\u0026#34;100%\u0026#34; src=\u0026#34;{{ .Get \u0026#34;src\u0026#34; }}\u0026#34; frameborder=\u0026#34;0\u0026#34; allowfullscreen=\u0026#34;\u0026#34; \u0026gt; \u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 使用方式\n{a{\u0026lt;pdf src=\u0026#34;pdf網址 | 站內 pdf 位址\u0026#34;\u0026gt;}} # 使用的時候把字母 a 去掉；這邊加入 a 是防止被識別生效 # 把 pdf file 放在 static/{目錄名}/ 下，可以直接使用 /{目錄名}/{檔案名}.pdf 指定使用站內 pdf \u003c!DOCTYPE HTML\u003e Youtube Shortcode 在 layouts/shortcodes/ 目錄下建立一個新文件 youtube.html，貼上下列內容\n\u0026lt;!DOCTYPE HTML\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; .youtube_shortcodes { position: relative; width: 100%; height: 0; padding-bottom: 66%; margin: auto; overflow: hidden; text-align: center; } .youtube_shortcodes iframe { position: absolute; width: 100%; height: 100%; left: 0; top: 0; } \u0026lt;/style\u0026gt; \u0026lt;title\u0026gt;\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;youtube_shortcodes\u0026#34;\u0026gt; \u0026lt;iframe class=\u0026#34;youtube-player\u0026#34; type=\u0026#34;text/html\u0026#34; width=\u0026#34;640\u0026#34; height=\u0026#34;385\u0026#34; src=\u0026#34;https://www.youtube.com/embed/{{ index .Params 0 }}?autoplay=0\u0026#34; style=\u0026#34; position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\u0026#34; allowfullscreen frameborder=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 使用方式\n{a{\u0026lt;youtube _XbJhL7WsE8\u0026gt;}} # 使用的時候把字母 a 去掉；這邊加入 a 是防止被識別生效 # _XbJhL7WsE8 是 Youtube 分享鏈結中的最後一段識別碼 \u003c!DOCTYPE HTML\u003e Blog 站內文章 在 layouts/shortcodes/ 目錄下建立一個新文件 innerlink.html，貼上下列內容\n\u0026lt;div style=\u0026#34;height: 200px;margin: 1em auto;position: relative; box-shadow: 0 2px 4px rgb(0 0 0 / 25%), 0 0 2px rgb(0 0 0 / 25%); border-radius: 15px;padding: 23px;max-width: 780px;background: var(--entry);\u0026#34;\u0026gt; {{ $url := .Get \u0026#34;src\u0026#34; }} {{ with .Site.GetPage $url }} \u0026lt;div style=\u0026#34;font-size: 22px; font-weight: 600\u0026#34;\u0026gt; \u0026lt;a target=\u0026#34;_blank\u0026#34; href=\u0026#34;{{ .Permalink }}\u0026#34; style=\u0026#34;box-shadow: none\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;span style=\u0026#34;font-size: 14px; color: #999\u0026#34;\u0026gt; Date: {{ .Date.Format ( default \u0026#34;2006-01-02\u0026#34;) }} {{ if .Params.categories }}\u0026amp;nbsp; Categories: {{ range .Params.categories }} #{{ . }}\u0026amp;nbsp; {{ end }} \u0026lt;/span\u0026gt; \u0026lt;div style=\u0026#34;font-size: 14px; line-height: 1.825;max-height: 75px; overflow: hidden;margin-top: 5px;\u0026#34;\u0026gt; {{ .Summary | plainify}} ...... \u0026lt;/div\u0026gt; {{ end }} {{ end }} \u0026lt;/div\u0026gt; 使用方式\n{a{\u0026lt;innerlink src=\u0026#34;posts/hugo/installation.md\u0026#34;\u0026gt;}} # 使用的時候把字母 a 去掉；這邊加入 a 是防止被識別生效 # 直接指定 content/posts/ 下的文章路徑，結尾要加上 .md 檔名 # 卡片獲取的是文章的 summary 內容，默認長度是 70 個中文字 Hugo 安裝與部署到 GitHub Pages ( Mac M2 ) Date: 2023-01-05 \u0026nbsp; Categories: #Hugo\u0026nbsp; 這篇文章主要提供在 Mac M2 上安裝 Hugo 、執行一個 quick start 的示範站點，並自動部署到 GitHub Pages 的過程 ...... Hugo Front Matter 參數說明 Content Summaries\nHugo generates summaries of your content. With the use of the .Summary page variable, Hugo generates summaries of content to use as a short version in summary views.\n將目錄 (ToC) 改到側邊 參考 Hugo博客目录放在侧边 | PaperMod主题\n文章内容仅限于PaperMod主题，对于其他主题仅供参考\n修改 ToC 首先找到 layouts/partials/toc.html ， 更換檔案內容如下\n{{- $headers := findRE \u0026#34;\u0026lt;h[1-6].*?\u0026gt;(.|\\n])+?\u0026lt;/h[1-6]\u0026gt;\u0026#34; .Content -}} {{- $has_headers := ge (len $headers) 1 -}} {{- if $has_headers -}} \u0026lt;aside id=\u0026#34;toc-container\u0026#34; class=\u0026#34;toc-container wide\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;toc\u0026#34;\u0026gt; \u0026lt;details {{if (.Param \u0026#34;TocOpen\u0026#34;) }} open{{ end }}\u0026gt; \u0026lt;summary accesskey=\u0026#34;c\u0026#34; title=\u0026#34;(Alt + C)\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;details\u0026#34;\u0026gt;{{- i18n \u0026#34;toc\u0026#34; | default \u0026#34;Table of Contents\u0026#34; }}\u0026lt;/span\u0026gt; \u0026lt;/summary\u0026gt; \u0026lt;div class=\u0026#34;inner\u0026#34;\u0026gt; {{- $largest := 6 -}} {{- range $headers -}} {{- $headerLevel := index (findRE \u0026#34;[1-6]\u0026#34; . 1) 0 -}} {{- $headerLevel := len (seq $headerLevel) -}} {{- if lt $headerLevel $largest -}} {{- $largest = $headerLevel -}} {{- end -}} {{- end -}} {{- $firstHeaderLevel := len (seq (index (findRE \u0026#34;[1-6]\u0026#34; (index $headers 0) 1) 0)) -}} {{- $.Scratch.Set \u0026#34;bareul\u0026#34; slice -}} \u0026lt;ul\u0026gt; {{- range seq (sub $firstHeaderLevel $largest) -}} \u0026lt;ul\u0026gt; {{- $.Scratch.Add \u0026#34;bareul\u0026#34; (sub (add $largest .) 1) -}} {{- end -}} {{- range $i, $header := $headers -}} {{- $headerLevel := index (findRE \u0026#34;[1-6]\u0026#34; . 1) 0 -}} {{- $headerLevel := len (seq $headerLevel) -}} {{/* get id=\u0026#34;xyz\u0026#34; */}} {{- $id := index (findRE \u0026#34;(id=\\\u0026#34;(.*?)\\\u0026#34;)\u0026#34; $header 9) 0 }} {{- /* strip id=\u0026#34;\u0026#34; to leave xyz, no way to get regex capturing groups in hugo */ -}} {{- $cleanedID := replace (replace $id \u0026#34;id=\\\u0026#34;\u0026#34; \u0026#34;\u0026#34;) \u0026#34;\\\u0026#34;\u0026#34; \u0026#34;\u0026#34; }} {{- $header := replaceRE \u0026#34;\u0026lt;h[1-6].*?\u0026gt;((.|\\n])+?)\u0026lt;/h[1-6]\u0026gt;\u0026#34; \u0026#34;$1\u0026#34; $header -}} {{- if ne $i 0 -}} {{- $prevHeaderLevel := index (findRE \u0026#34;[1-6]\u0026#34; (index $headers (sub $i 1)) 1) 0 -}} {{- $prevHeaderLevel := len (seq $prevHeaderLevel) -}} {{- if gt $headerLevel $prevHeaderLevel -}} {{- range seq $prevHeaderLevel (sub $headerLevel 1) -}} \u0026lt;ul\u0026gt; {{/* the first should not be recorded */}} {{- if ne $prevHeaderLevel . -}} {{- $.Scratch.Add \u0026#34;bareul\u0026#34; . -}} {{- end -}} {{- end -}} {{- else -}} \u0026lt;/li\u0026gt; {{- if lt $headerLevel $prevHeaderLevel -}} {{- range seq (sub $prevHeaderLevel 1) -1 $headerLevel -}} {{- if in ($.Scratch.Get \u0026#34;bareul\u0026#34;) . -}} \u0026lt;/ul\u0026gt; {{/* manually do pop item */}} {{- $tmp := $.Scratch.Get \u0026#34;bareul\u0026#34; -}} {{- $.Scratch.Delete \u0026#34;bareul\u0026#34; -}} {{- $.Scratch.Set \u0026#34;bareul\u0026#34; slice}} {{- range seq (sub (len $tmp) 1) -}} {{- $.Scratch.Add \u0026#34;bareul\u0026#34; (index $tmp (sub . 1)) -}} {{- end -}} {{- else -}} \u0026lt;/ul\u0026gt; \u0026lt;/li\u0026gt; {{- end -}} {{- end -}} {{- end -}} {{- end }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;#{{- $cleanedID -}}\u0026#34; aria-label=\u0026#34;{{- $header | plainify -}}\u0026#34;\u0026gt;{{- $header | safeHTML -}}\u0026lt;/a\u0026gt; {{- else }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;#{{- $cleanedID -}}\u0026#34; aria-label=\u0026#34;{{- $header | plainify -}}\u0026#34;\u0026gt;{{- $header | safeHTML -}}\u0026lt;/a\u0026gt; {{- end -}} {{- end -}} \u0026lt;!-- {{- $firstHeaderLevel := len (seq (index (findRE \u0026#34;[1-6]\u0026#34; (index $headers 0) 1) 0)) -}} --\u0026gt; {{- $firstHeaderLevel := $largest }} {{- $lastHeaderLevel := len (seq (index (findRE \u0026#34;[1-6]\u0026#34; (index $headers (sub (len $headers) 1)) 1) 0)) }} \u0026lt;/li\u0026gt; {{- range seq (sub $lastHeaderLevel $firstHeaderLevel) -}} {{- if in ($.Scratch.Get \u0026#34;bareul\u0026#34;) (add . $firstHeaderLevel) }} \u0026lt;/ul\u0026gt; {{- else }} \u0026lt;/ul\u0026gt; \u0026lt;/li\u0026gt; {{- end -}} {{- end }} \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/details\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/aside\u0026gt; \u0026lt;script\u0026gt; let activeElement; let elements; window.addEventListener(\u0026#39;DOMContentLoaded\u0026#39;, function (event) { checkTocPosition(); elements = document.querySelectorAll(\u0026#39;h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]\u0026#39;); // Make the first header active activeElement = elements[0]; const id = encodeURI(activeElement.getAttribute(\u0026#39;id\u0026#39;)).toLowerCase(); document.querySelector(`.inner ul li a[href=\u0026#34;#${id}\u0026#34;]`).classList.add(\u0026#39;active\u0026#39;); }, false); window.addEventListener(\u0026#39;resize\u0026#39;, function(event) { checkTocPosition(); }, false); window.addEventListener(\u0026#39;scroll\u0026#39;, () =\u0026gt; { // Check if there is an object in the top half of the screen or keep the last item active activeElement = Array.from(elements).find((element) =\u0026gt; { if ((getOffsetTop(element) - window.pageYOffset) \u0026gt; 0 \u0026amp;\u0026amp; (getOffsetTop(element) - window.pageYOffset) \u0026lt; window.innerHeight/2) { return element; } }) || activeElement elements.forEach(element =\u0026gt; { const id = encodeURI(element.getAttribute(\u0026#39;id\u0026#39;)).toLowerCase(); if (element === activeElement){ document.querySelector(`.inner ul li a[href=\u0026#34;#${id}\u0026#34;]`).classList.add(\u0026#39;active\u0026#39;); } else { document.querySelector(`.inner ul li a[href=\u0026#34;#${id}\u0026#34;]`).classList.remove(\u0026#39;active\u0026#39;); } }) }, false); const main = parseInt(getComputedStyle(document.body).getPropertyValue(\u0026#39;--article-width\u0026#39;), 10); const toc = parseInt(getComputedStyle(document.body).getPropertyValue(\u0026#39;--toc-width\u0026#39;), 10); const gap = parseInt(getComputedStyle(document.body).getPropertyValue(\u0026#39;--gap\u0026#39;), 10); function checkTocPosition() { const width = document.body.scrollWidth; if (width - main - (toc * 2) - (gap * 4) \u0026gt; 0) { document.getElementById(\u0026#34;toc-container\u0026#34;).classList.add(\u0026#34;wide\u0026#34;); } else { document.getElementById(\u0026#34;toc-container\u0026#34;).classList.remove(\u0026#34;wide\u0026#34;); } } function getOffsetTop(element) { if (!element.getClientRects().length) { return 0; } let rect = element.getBoundingClientRect(); let win = element.ownerDocument.defaultView; return rect.top + win.pageYOffset; } \u0026lt;/script\u0026gt; {{- end }} 然後確認 layouts/_default/single.html 是否有引入使用 toc.html 。 這邊預設是有引入的，作者寫出來是防止有人自定義文件名稱，導致設定失敗\n{{- if (.Param \u0026#34;ShowToc\u0026#34;) }} {{- partial \u0026#34;toc.html\u0026#34; . }} {{- end }} 修改 CSS 找到 css/extended/blank.css ， 更換檔案內容如下\n:root { --nav-width: 1380px; --article-width: 650px; --toc-width: 300px; } .toc { margin: 0 2px 40px 2px; border: 1px solid var(--border); background: var(--entry); border-radius: var(--radius); padding: 0.4em; } .toc-container.wide { position: absolute; height: 100%; border-right: 1px solid var(--border); left: calc((var(--toc-width) + var(--gap)) * -1); top: calc(var(--gap) * 2); width: var(--toc-width); } .wide .toc { position: sticky; top: var(--gap); border: unset; background: unset; border-radius: unset; width: 100%; margin: 0 2px 40px 2px; } .toc details summary { cursor: zoom-in; margin-inline-start: 20px; padding: 12px 0; } .toc details[open] summary { font-weight: 500; } .toc-container.wide .toc .inner { margin: 0; } .active { font-size: 110%; font-weight: 600; } .toc ul { list-style-type: circle; } .toc .inner { margin: 0 0 0 20px; padding: 0px 15px 15px 20px; font-size: 16px; /*目录显示高度*/ max-height: 83vh; overflow-y: auto; } .toc .inner::-webkit-scrollbar-thumb { /*滚动条*/ background: var(--border); border: 7px solid var(--theme); border-radius: var(--radius); } .toc li ul { margin-inline-start: calc(var(--gap) * 0.5); list-style-type: none; } .toc li { list-style: none; font-size: 0.95rem; padding-bottom: 5px; } .toc li a:hover { color: var(--secondary); } ","permalink":"https://blog.zhengweiliu.com/posts/hugo/papermod/","summary":"這篇文章整理了個人使用 PaperMod theme 的設定","title":"Hugo-PaperMod theme 設定"},{"content":"這篇文章主要提供 GitHub Pages 設定 custom domain 的過程\nGitHub Pages 提供了可以設定 custom domain 的方法，因此想要透過 custom domain 設定，將 GitHub Pages 的 URL 更改為自己的 domain。\n申請 Domain 有很多 provider 提供註冊 domain 的服務，如 Google Domain、Go Daddy 等等。\n我自己是在 GoDaddy 上購買 Domain ， 因此這篇文章記錄的是在 Go Daddy 上的設定方式。\nGitHub Pages Custom Domain 根據 GitHub Pages 的文件説明，設定 subdomain 的方式相對簡單明白\nGitHub Pages Repository \u0026gt; Settings \u0026gt; Pages (左側選單) \u0026gt; Custom Domain \u0026gt; 輸入 subdomain \u0026gt; Save\n這時會看到 Custom Domain 下方出現正在驗證 domain 的字樣，等 DNS Record 設定好之後，幾分鐘內就會出現驗證通過的綠色字樣。\nGoDaddy DNS Record 設定 在 GoDaddy 的 DNS 紀錄頁面，新增一筆 C Name Record，給予想要的 subdomain 名稱，如: www 、 blog ，內容值填入 GitHub Pages URL ，如 : zhweiliu.github.io. ，TTL 時間可以留預設值，之後儲存以新增紀錄即可\nEnable GitHub Pages Enforce Https 在 terminal 中使用指令確認 DNS Record 的設置是否讓 GitHub Pages Custom Domain 生效\n$ dig blog.zhengweiliu.com 若設置已經生效，你會看到如下圖所示的 Answer Section 將 blog.zhengweiliu.com 置換為你設定 subdomain\n假設你設定 CNAME 得名稱為 www ， 你購買的 domain name 是 example.com ，那你需要 dig www.example.com\n回到先前 GitHub Pages Custom Domain 設定的頁面，刷新網頁後將 Enforce HTTPS 打勾，以確保 http:// 開頭的請求都能夠被強制轉為 https://\n","permalink":"https://blog.zhengweiliu.com/posts/hugo/custom_domain/","summary":"這篇文章主要提供 GitHub Pages 設定 custom domain 的過程","title":"GitHub Pages 設定 Custom Domain ( Go Daddy )"},{"content":"這篇文章主要提供在 Mac M2 上安裝 Hugo 、執行一個 quick start 的示範站點，並自動部署到 GitHub Pages 的過程\nWhat is Hugo ? Hugo 是一個通用的網站框架。從技術上講，Hugo 是一個靜態站點生成器。與根據每個訪問者請求動態構建頁面的系統不同，Hugo 在您創建或更新內容時構建頁面。\n靜態站點生成器 網站生成器的目的是將內容呈現為 HTML 文件。大多數是“動態站點生成器”。這意味著 HTTP 服務器——即，將文件發送到瀏覽器以供查看的程序——運行生成器以在每次最終用戶請求頁面時創建一個新的 HTML 文件。\n隨著時間的推移，動態站點生成器被編程為緩存它們的 HTML 文件，以防止在向最終用戶交付頁面時出現不必要的延遲。緩存頁面是網頁的靜態版本。\nHugo 使緩存更進一步，所有 HTML 文件都呈現在您的計算機上。在將文件複製到託管 HTTP 服務器的計算機之前，您可以在本地查看這些文件。由於 HTML 文件不是動態生成的，我們說 Hugo 是一個靜態站點生成器。\nMac M2 安裝 安裝前需求 Hugo 經常會伴隨著 Git 和 Go 的功能來進行部署和使用其他的 modules feature，因此需要先檢查 Git 和 Go 是否有安裝\nGit installation Mac M2 預設已安裝 Git\n透過指令確認 git 是否已安裝\ngit --version 如需要重新安裝 Git ，可利用 brew 指令進行安裝\nbrew install git Go installation 同樣利用 brew install 指令安裝 Go 即可\nbrew install go 安裝完成後，利用指令檢查安裝版本，以確認安裝成功\nbrew install Hugo 利用 brew install 指令，直接安裝 Hugo extended edition 版本即可\nbrew install hugo 安裝完成後，一樣利用指令檢查安裝版本，以確認安裝成功 Quick Start 以 Hogo 官方的 Quick Start 文件為例\n建立 Hugo 站點 # 利用 hugo 指令建立新的靜態站點 # 並在當前目錄下建立名為 \u0026#34;quickstart\u0026#34; 的資料夾 $ hugo new site quickstart # 切換到 quickstart $ cd quickstart # 利用 git 指令將 quickstart 資料夾變成一個 repository # 並受 git 管理 $ git init # 以 submodule 方式為添加可用 hugo 主題 # 官方採用 Ananke 主題作為示範 $ git submodule add https://github.com/theNewDynamic/gohugo-theme-ananke themes/ananke # 將主題設置給 hugo # hugo 通常以 config.toml 作為主要設定檔案 # 除了 toml 格式標準，也可以使用 json 或 yaml 格式 $ echo \u0026#34;theme = \u0026#39;ananke\u0026#39;\u0026#34; \u0026gt;\u0026gt; config.toml # 在本機電腦上執行 hugo server 服務 # -D 表示可以在本機預覽 draft: true 的文章 $ hugo server -D 執行 hugo server -D 後，可以在 terminal 看到本機電腦提供的測試 URL : http://localhost:1313/ ，在瀏覽器上輸入這段 URL 便能夠看到本機點腦上的 Hugo 站點與文章 新增一篇文章 在 terminal 中，進入了 qucikstart 的 Hugo 站點資料夾後，利用 Hugo 指令建立一篇新的文章\n$ hugo new posts/hello_world.md hugo new 指令可以幫助我們快速的建立 Hugo 站點內的所以新資源 如果不熟悉的話，也可以直接在圖像介面中，在 content 資料夾下新增一個 posts 資料夾，並且在 posts 資料夾在新增一個 hello_world.md 檔案，也能達成相同的效果\n文章內容 打開 hello_world.md 檔案，預設應該會看到 Hugo 幫我們建立的 Front Matter 資訊\ntitle: \u0026#34;My First Post\u0026#34; date: 2022-11-20T09:03:20-08:00 draft: true title : 文章標題\ndate : 文章建立時間\ndraft : 是否為草稿。 true 表示為草稿，Hugo 不會將草稿發佈到正式的站點環境，草稿文章僅在 hugo server -D 時可見。\n*.md 檔案使用的是 markdown 語法 ， 可以在 Front Matter 下直接使用 markdown 語法開始編寫文章\n--- title: \u0026#34;My First Post\u0026#34; date: 2022-11-20T09:03:20-08:00 draft: true --- ## Introduction This is **bold** text, and this is *emphasized* text. Visit the [Hugo](https://gohugo.io) website! 文章內容有異動並且儲存檔案後，在 hugo server -D 啟動本機電腦 Hugo 站點的情況下，可以直接在 http://localhost:1313/ 中看到變動後的文章內容。\n部署到 GitHub Pages 目前有滿多 provider 支援部署 Hugo 站點的服務，如 AWS Amplify, CloudCannon, Cloudflare Pages, GitHub Pages, GitLab Pages, and Netlify.\n因個人習慣，所以本文採用部署到 GitHub Pages\n建立 GitHub Pages 的 Repository 登入自己的 GitHub 帳號後，建立一個 repository 給 GitHub Pages 使用\nRepository Name 嚴格規定要使用 {github 帳號}.github.io\n先找到剛才建立好的 GitHub Pages Repository 提供的 HTTP URL\n開啟 Terminal 並在 Hugo 站點下執行 git 指令\n$ git remote add origin {GitHub Pages Repository URL} 透過指令檢查 git remote 是否設定完成\n$ git remote -v GitHub Personal Access Token 如果第一次在 Mac 上設定 Git 連線資訊，有可能會提示你要輸入 GitHub 的帳號密碼。 但 GitHub 已經取消使用密碼登入的方式，目前僅能透過 personal access token 的方式來通過驗證。 (即在輸入密碼時，改以提供 person access token 而非密碼)\nHugo 在部署站點時會需要使用 CI/CD 的功能，因此 GitHub Access Token 的 Scope 中需要把 workflow 一併勾選以授權\n設定 GitHub Action 透過設定 GitHub Action ， 在每次將文章推送到 GitHub Repository 後，自動執行部署站點的動作\n首先，在 Terminal 透過指令建立 .github/workflows/gh-pages.yml 檔案\n# 記得先將 Terminal 切換到 Hugo 站點的資料夾下，比如 quickstart # 建立資料夾 .github/workflows/ # -p 指令可以直接將路徑中缺少的資料夾一併建立 $ mkdir -p .github/workflows/ # touch 指令建立一個空檔案，命名為 gh-pages.yml ， # 因為輸入了相對路徑 .github/workflows ， # gh-pages.yml 檔案會被放在 .github/workflows 資料夾下 $ touch .github/workflows/gh-pages.yml 將下列的內容複製貼上到 .github/workflows/gh-pages.yml 檔案中\nname: GitHub Pages on: push: branches: - main # Set a branch name to trigger deployment pull_request: jobs: deploy: runs-on: ubuntu-22.04 permissions: contents: write concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - uses: actions/checkout@v3 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 # If you\u0026#39;re changing the branch from main, # also change the `main` in `refs/heads/main` # below accordingly. if: ${{ github.ref == \u0026#39;refs/heads/main\u0026#39; }} with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public 修改 Hugo 設定檔 baseURL 設定 Hugo config.toml (或 config.yml | config.json) ， 更改 baseURL 為 https://{GitHub帳號}.github.io\n同步 Hugo 站點到 GitHub 透過 Terminal 使用 git 指令\n# 將 GitHub Pages Repository 的資訊同步到本機電腦上的 Hugo 站點資料夾中 $ git fetch # 將目前 Hugo 站點的所有檔案納入 git 管理 $ git add --all # 將本次納入 git 管理的異動檔案，打包成一個版本 $ git commit -m \u0026#39;first commit\u0026#39; # 將打包的版本同步到 GitHub Pages Repository $ git push -u origin main 檢查 GitHub Pages 部署狀況 在 GitHub Pages 中點選 Actions 頁簽，可以查看所有 workflow 的運型狀況。 由於前面設定了 .github/workflows/gh-pages.yml 檔案，每次將異動的檔案同步到 GitHub Pages Repository 後， GitHub 便會自動執行 .github/workflows/gh-pages.yml 檔案設定的工作流內容，並將 Hugo 站點的文章部署到 GitHub Pages 修改 GitHub Pages Repository 的分支 在 GitHub Pages Repository 的 Settings 頁簽中修改 Branch ，這個動作是讓 GitHub Pages 可以知道應該從哪一個 Branch 抓取需要部署的 Hugo 站點資料\nGitHub Pages Repository \u0026gt; Settings \u0026gt; Pages (左側選單) \u0026gt; Build and deployment \u0026gt; Branch \u0026gt; 點擊下拉選單從 main 更改為 gh-pages.\n","permalink":"https://blog.zhengweiliu.com/posts/hugo/installation/","summary":"這篇文章主要提供在 Mac M2 上安裝 Hugo 、執行一個 quick start 的示範站點，並自動部署到 GitHub Pages 的過程","title":"Hugo 安裝與部署到 GitHub Pages ( Mac M2 ) "},{"content":"Description Five silent philosophers sit at a round table with bowls of spaghetti. Forks are placed between each pair of adjacent philosophers.\nEach philosopher must alternately think and eat. However, a philosopher can only eat spaghetti when they have both left and right forks. Each fork can be held by only one philosopher and so a philosopher can use the fork only if it is not being used by another philosopher. After an individual philosopher finishes eating, they need to put down both forks so that the forks become available to others. A philosopher can take the fork on their right or the one on their left as they become available, but cannot start eating before getting both forks.\nEating is not limited by the remaining amounts of spaghetti or stomach space; an infinite supply and an infinite demand are assumed.\nDesign a discipline of behaviour (a concurrent algorithm) such that no philosopher will starve; i.e., each can forever continue to alternate between eating and thinking, assuming that no philosopher can know when others may want to eat or think.\nThe problem statement and the image above are taken from wikipedia.org\nThe philosophers’ ids are numbered from 0 to 4 in a clockwise order. Implement the function void wantsToEat(philosopher, pickLeftFork, pickRightFork, eat, putLeftFork, putRightFork) where:\nphilosopher is the id of the philosopher who wants to eat. pickLeftFork and pickRightFork are functions you can call to pick the corresponding forks of that philosopher. eat is a function you can call to let the philosopher eat once he has picked both forks. putLeftFork and putRightFork are functions you can call to put down the corresponding forks of that philosopher. The philosophers are assumed to be thinking as long as they are not asking to eat (the function is not being called with their number). Five threads, each representing a philosopher, will simultaneously use one object of your class to simulate the process. The function may be called for the same philosopher more than once, even before the last call ends.\nIdea The Dining philosophers problem.\nIn computer science, the dining philosophers problem is an example problem often used in concurrent algorithm design to illustrate synchronization issues and techniques for resolving them.\nFocus on the forks instead of philosophers, because forks are necessary resources if philosopher would like to eat food.\nI used a list to put 5 lock, each lock indicates a fork, let philosopher id + 1 as their left-hand, philosopher id as their right-hand.\nTake the pickup left-hand’s fork first because it’s have a higher number , and put down right fork first because it a higher number fork for right side philosopher.\nSolution from threading import Condition class DiningPhilosophers: def __init__(self) -\u0026gt; None: self._forks = [Condition()] * 5 # call the functions directly to execute, for example, eat() def wantsToEat(self, philosopher: int, pickLeftFork: \u0026#39;Callable[[], None]\u0026#39;, pickRightFork: \u0026#39;Callable[[], None]\u0026#39;, eat: \u0026#39;Callable[[], None]\u0026#39;, putLeftFork: \u0026#39;Callable[[], None]\u0026#39;, putRightFork: \u0026#39;Callable[[], None]\u0026#39;) -\u0026gt; None: with self._forks[(philosopher+1)%5], self._forks[philosopher]: pickLeftFork() pickRightFork() eat() putRightFork() putLeftFork() ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/concurrency/the-dining-philosophers/","summary":"Description Five silent philosophers sit at a round table with bowls of spaghetti. Forks are placed between each pair of adjacent philosophers.\nEach philosopher must alternately think and eat. However, a philosopher can only eat spaghetti when they have both left and right forks. Each fork can be held by only one philosopher and so a philosopher can use the fork only if it is not being used by another philosopher.","title":"[leetcode][Python][Concurrency][Medium] 1226. The Dining Philosophers"},{"content":"Description You have the four functions:\nprintFizz that prints the word \u0026quot;fizz\u0026quot; to the console, printBuzz that prints the word \u0026quot;buzz\u0026quot; to the console, printFizzBuzz that prints the word \u0026quot;fizzbuzz\u0026quot; to the console, and printNumber that prints a given integer to the console. You are given an instance of the class FizzBuzz that has four functions: fizz, buzz, fizzbuzz and number. The same instance of FizzBuzz will be passed to four different threads:\nThread A: calls fizz() that should output the word \u0026quot;fizz\u0026quot;. Thread B: calls buzz() that should output the word \u0026quot;buzz\u0026quot;. Thread C: calls fizzbuzz() that should output the word \u0026quot;fizzbuzz\u0026quot;. Thread D: calls number() that should only output the integers. Modify the given class to output the series [1, 2, \u0026quot;fizz\u0026quot;, 4, \u0026quot;buzz\u0026quot;, ...] where the ith token (1-indexed) of the series is:\n\u0026quot;fizzbuzz\u0026quot; if i is divisible by 3 and 5, \u0026quot;fizz\u0026quot; if i is divisible by 3 and not 5, \u0026quot;buzz\u0026quot; if i is divisible by 5 and not 3, or i if i is not divisible by 3 or 5. Implement the FizzBuzz class:\nFizzBuzz(int n) Initializes the object with the number n that represents the length of the sequence that should be printed. void fizz(printFizz) Calls printFizz to output \u0026quot;fizz\u0026quot;. void buzz(printBuzz) Calls printBuzz to output \u0026quot;buzz\u0026quot;. void fizzbuzz(printFizzBuzz) Calls printFizzBuzz to output \u0026quot;fizzbuzz\u0026quot;. void number(printNumber) Calls printnumber to output the numbers. Idea For example\nInput: n = 15 Output: [1,2,\u0026#34;fizz\u0026#34;,4,\u0026#34;buzz\u0026#34;,\u0026#34;fizz\u0026#34;,7,8,\u0026#34;fizz\u0026#34;,\u0026#34;buzz\u0026#34;,11,\u0026#34;fizz\u0026#34;,13,14,\u0026#34;fizzbuzz\u0026#34;] Basically, I guess it could be used Condition or Lock to solve this question, but its could be bring about not easily to read for the solution.\nAfter study the discussion with other contributors, I agree to use Python threading.Semaphore to solve this question.\nThe Semaphore introduce on official documentation as :\nA semaphore manages an atomic counter representing the number of release() calls minus the number of acquire() calls, plus an initial value. The acquire() method blocks if necessary until it can return without making the counter negative. If not given, value defaults to 1. We can create Semaphore objects for fizz, buzz, fizzbuzz and numbers. And use the for-loops setup their runtimes with fit conditions to n .\nThe semaphore initial values are 0 for fizz, buzz, fizzbuzz, but setup the semaphore initial value 1 for numbers, because we know the serial start with a number, 1 to n , and all conditions of fizz, buzz, fizzbuzz requires divisible by number ,at least 3 , it will help the function number to print numbers without blocking.\nSolution from threading import Semaphore class FizzBuzz: def __init__(self, n: int): self.n = n self._lock_fz = Semaphore(0) self._lock_bz = Semaphore(0) self._lock_fzbz = Semaphore(0) self._lock_num = Semaphore(1) # printFizz() outputs \u0026#34;fizz\u0026#34; def fizz(self, printFizz: \u0026#39;Callable[[], None]\u0026#39;) -\u0026gt; None: for i in range(self.n//3-self.n//15): self._lock_fz.acquire() printFizz() self._lock_num.release() # printBuzz() outputs \u0026#34;buzz\u0026#34; def buzz(self, printBuzz: \u0026#39;Callable[[], None]\u0026#39;) -\u0026gt; None: for i in range(self.n//5-self.n//15): self._lock_bz.acquire() printBuzz() self._lock_num.release() # printFizzBuzz() outputs \u0026#34;fizzbuzz\u0026#34; def fizzbuzz(self, printFizzBuzz: \u0026#39;Callable[[], None]\u0026#39;) -\u0026gt; None: for _ in range(self.n//15): self._lock_fzbz.acquire() printFizzBuzz() self._lock_num.release() # printNumber(x) outputs \u0026#34;x\u0026#34;, where x is an integer. def number(self, printNumber: \u0026#39;Callable[[int], None]\u0026#39;) -\u0026gt; None: for i in range(1, self.n+1): self._lock_num.acquire() if i%3==0 and i%5==0: self._lock_fzbz.release() elif i%3==0: self._lock_fz.release() elif i%5==0: self._lock_bz.release() else: printNumber(i) self._lock_num.release() ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/concurrency/fizz-buzz-multithreaded/","summary":"Description You have the four functions:\nprintFizz that prints the word \u0026quot;fizz\u0026quot; to the console, printBuzz that prints the word \u0026quot;buzz\u0026quot; to the console, printFizzBuzz that prints the word \u0026quot;fizzbuzz\u0026quot; to the console, and printNumber that prints a given integer to the console. You are given an instance of the class FizzBuzz that has four functions: fizz, buzz, fizzbuzz and number. The same instance of FizzBuzz will be passed to four different threads:","title":"[leetcode][Python][Concurrency][Medium] 1195. Fizz Buzz Multithreaded"},{"content":"Description Suppose you are given the following code:\nclass FooBar { public void foo() { for (int i = 0; i \u0026lt; n; i++) { print(\u0026#34;foo\u0026#34;); } } public void bar() { for (int i = 0; i \u0026lt; n; i++) { print(\u0026#34;bar\u0026#34;); } } } The same instance of FooBar will be passed to two different threads:\nthread A will call foo(), while thread B will call bar(). Modify the given program to output \u0026quot;foobar\u0026quot; n times.\nIdea An example for output\nInput: n = 2 Output: \u0026#34;foobarfoobar\u0026#34; Explanation: \u0026#34;foobar\u0026#34; is being output 2 times. Using a flag to switch printFoo() and printBar() when acquire the lock.\nSolution from threading import Condition class FooBar: def __init__(self, n): self.n = n self._lock = Condition() self._is_printed_foo = False def foo(self, printFoo: \u0026#39;Callable[[], None]\u0026#39;) -\u0026gt; None: for i in range(self.n): with self._lock: while self._is_printed_foo: self._lock.wait() # printFoo() outputs \u0026#34;foo\u0026#34;. Do not change or remove this line. printFoo() self._is_printed_foo = True self._lock.notify_all() def bar(self, printBar: \u0026#39;Callable[[], None]\u0026#39;) -\u0026gt; None: for i in range(self.n): with self._lock: while not self._is_printed_foo: self._lock.wait() # printBar() outputs \u0026#34;bar\u0026#34;. Do not change or remove this line. printBar() self._is_printed_foo = False self._lock.notify_all() ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/concurrency/print-foobar-alternately/","summary":"Description Suppose you are given the following code:\nclass FooBar { public void foo() { for (int i = 0; i \u0026lt; n; i++) { print(\u0026#34;foo\u0026#34;); } } public void bar() { for (int i = 0; i \u0026lt; n; i++) { print(\u0026#34;bar\u0026#34;); } } } The same instance of FooBar will be passed to two different threads:\nthread A will call foo(), while thread B will call bar(). Modify the given program to output \u0026quot;foobar\u0026quot; n times.","title":"[leetcode][Python][Concurrency][Medium] 1115. Print FooBar Alternately"},{"content":"asyncio is a library to write concurrent code using the async/await syntax. ---- from Python3.11.1 documentation This article is write down the note with my study of python asyncio package.\nHow does asyncio work ? The main process, which is start run by IDE or command line, have a main thread to execute submit a coroutine to asyncio event loops by asyncio.create_task() or asyncio.run() , the keyword async will packet methods as coroutine. The event loops will monitoring all of the submit task, and choice a not finished, current can going on coroutine to execute its task until it finish, or change status to wait when meet await . When event loops meet await , you should be notify (or notify all) task(s) which status is waiting for blocking, and check the blocking condition is still exist or not. Repeating above step 2 and step 3 until no more coroutines in asyncio event loops (i.e. all of the coroutine will be finish or canceled). async \u0026amp; await Using async to create a coroutine method Using await to call another coroutine _B_ in coroutine _A_.\n_A_ will into wait status until _B_ execute finish and notify. Using asyncio.run() to submit a coroutine Create Task \u0026amp; Submit Coroutine The asyncio.create_task() will submit a coroutine into Task Queue When await _task_1_ be execute, main thread will keep waiting until _task_1_ finish/await, and so on _task_2_ It have concurrency effect like as multi threading( or multi processing ) Timeout \u0026amp; Cancel Using Task.done() to determine a task is finish or not yet. Using Task.cancel() to cancel a task which is executing. Using asyncio.wait_for(task, timeout=wait_duration) for automate cancel a task if execute timeout Sometimes, we want the task keep going on their work until finish, and I just would like to know the task will happened timeout or not. For example: Counting the times of timeout to calculate performance\nGather multi task Using asyncio.gather(task1, task2, …, taskN) Add parameter return_exceptions = True capture exception result ","permalink":"https://blog.zhengweiliu.com/posts/normal/python3-asyncio/","summary":"asyncio is a library to write concurrent code using the async/await syntax.\nHow does \u003cem\u003easyncio\u003c/em\u003e work ?\nasync \u0026amp; await,\u003cbr\u003e\nCreate Task \u0026amp; Submit Coroutine,\nTimeout \u0026amp; Cancel,\nGather multi task\u0026hellip;","title":"Python3 - asyncio"},{"content":"Description Implement a thread-safe bounded blocking queue that has the following methods:\nBoundedBlockingQueue(int capacity) The constructor initializes the queue with a maximum capacity. void enqueue(int element) Adds an element to the front of the queue. If the queue is full, the calling thread is blocked until the queue is no longer full. int dequeue() Returns the element at the rear of the queue and removes it. If the queue is empty, the calling thread is blocked until the queue is no longer empty. int size() Returns the number of elements currently in the queue. Please do not use built-in implementations of bounded blocking queue as this will not be accepted in an interview.\nIdea Your implementation will be tested using multiple threads at the same time. Each thread will either be a producer thread that only makes calls to the enqueue method or a consumer thread that only makes calls to the dequeue method. The size method will be called after every test case.\nInput: 3 4 [\u0026#34;BoundedBlockingQueue\u0026#34;,\u0026#34;enqueue\u0026#34;,\u0026#34;enqueue\u0026#34;,\u0026#34;enqueue\u0026#34;,\u0026#34;dequeue\u0026#34;,\u0026#34;dequeue\u0026#34;,\u0026#34;dequeue\u0026#34;,\u0026#34;enqueue\u0026#34;] [[3],[1],[0],[2],[],[],[],[3]] Output: [1,0,2,1] Explanation: Number of producer threads = 3 Number of consumer threads = 4 BoundedBlockingQueue queue = new BoundedBlockingQueue(3); // initialize the queue with capacity = 3. queue.enqueue(1); // Producer thread P1 enqueues 1 to the queue. queue.enqueue(0); // Producer thread P2 enqueues 0 to the queue. queue.enqueue(2); // Producer thread P3 enqueues 2 to the queue. queue.dequeue(); // Consumer thread C1 calls dequeue. queue.dequeue(); // Consumer thread C2 calls dequeue. queue.dequeue(); // Consumer thread C3 calls dequeue. queue.enqueue(3); // One of the producer threads enqueues 3 to the queue. queue.size(); // 1 element remaining in the queue. Since the number of threads for producer/consumer is greater than 1, we do not know how the threads will be scheduled in the operating system, even though the input seems to imply the ordering. Therefore, any of the output [1,0,2] or [1,2,0] or [0,1,2] or [0,2,1] or [2,0,1] or [2,1,0] will be accepted. I guess the blocking means a task cannot going on its work when some condition cannot fit.\nIn this question, I need to design a bounded blocking queue, the queue have a capacity that means :\nCannot enqueue if queue have no remain space for element Can not dequeue when no more element in the queue The block happened when meet above situation.\nWhile the thread acquire lock, thread must be detect currently status of queue :\nWaiting for next time notify if queue have not remaining space for enqueue Waiting for next time notify if queue have not any element for dequeue Solution from threading import Condition class BoundedBlockingQueue(object): def __init__(self, capacity: int): self.__capacity = capacity self.__lock = Condition() self.__queue = list() def enqueue(self, element: int) -\u0026gt; None: with self.__lock: while self.size() == self.__capacity: self.__lock.wait() self.__queue.insert(0, element) self.__lock.notify_all() def dequeue(self) -\u0026gt; int: ret = None with self.__lock: while self.size() == 0: self.__lock.wait() ret = self.__queue.pop() self.__lock.notify_all() return ret def size(self) -\u0026gt; int: return len(self.__queue) ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/concurrency/design-bounded-blocking-queue/","summary":"Description Implement a thread-safe bounded blocking queue that has the following methods:\nBoundedBlockingQueue(int capacity) The constructor initializes the queue with a maximum capacity. void enqueue(int element) Adds an element to the front of the queue. If the queue is full, the calling thread is blocked until the queue is no longer full. int dequeue() Returns the element at the rear of the queue and removes it. If the queue is empty, the calling thread is blocked until the queue is no longer empty.","title":"[leetcode][Python][Concurrency][Medium] 1188. Design Bounded Blocking Queue"},{"content":"Description Given a URL startUrl and an interface HtmlParser, implement a Multi-threaded web crawler to crawl all links that are under the same hostname as startUrl.\nReturn all URLs obtained by your web crawler in any order.\nYour crawler should:\nStart from the page: startUrl Call HtmlParser.getUrls(url) to get all URLs from a webpage of a given URL. Do not crawl the same link twice. Explore only the links that are under the same hostname as startUrl. As shown in the example URL above, the hostname is example.org. For simplicity\u0026rsquo;s sake, you may assume all URLs use HTTP protocol without any port specified. For example, the URLs http://leetcode.com/problems and http://leetcode.com/contest are under the same hostname, while URLs http://example.org/test and http://example.com/abc are not under the same hostname.\nThe HtmlParser interface is defined as such:\ninterface HtmlParser { // Return a list of all urls from a webpage of given url. // This is a blocking call, that means it will do HTTP request and return when this request is finished. public List\u0026lt;String\u0026gt; getUrls(String url); } Note that getUrls(String url) simulates performing an HTTP request. You can treat it as a blocking function call that waits for an HTTP request to finish. It is guaranteed that getUrls(String url) will return the URLs within 15ms. Single-threaded solutions will exceed the time limit so, can your multi-threaded web crawler do better?\nIdea Below are two examples explaining the functionality of the problem. For custom testing purposes, you’ll have three variables urls, edges and startUrl. Notice that you will only have access to startUrl in your code, while urls and edges are not directly accessible to you in code.\nInput: urls = [ \u0026#34;http://news.yahoo.com\u0026#34;, \u0026#34;http://news.yahoo.com/news\u0026#34;, \u0026#34;http://news.yahoo.com/news/topics/\u0026#34;, \u0026#34;http://news.google.com\u0026#34;, \u0026#34;http://news.yahoo.com/us\u0026#34; ] edges = [[2,0],[2,1],[3,2],[3,1],[0,4]] startUrl = \u0026#34;http://news.yahoo.com/news/topics/\u0026#34; Output: [ \u0026#34;http://news.yahoo.com\u0026#34;, \u0026#34;http://news.yahoo.com/news\u0026#34;, \u0026#34;http://news.yahoo.com/news/topics/\u0026#34;, \u0026#34;http://news.yahoo.com/us\u0026#34; ] Be multi threading(or multi processing), Python recommend use ThreadPoolExecutor (or ProcessPoolExecutor)to protect the threads (or processes) in a safe state when it executing. And this question maybe execute under a virtual environment on leetcode platform, so I guess take the ThreadPoolExecutor is a better choice.\nSo, I write 2 methods of the class Solution , one for extract hostname from url name get_hostname(), another one for filter url which is not visited name visit_url().\nThen, using the ThreadPoolExecutor to submit task visit_url for each url which is in the queue, and call future.result() to execute each visit_url with url.\nFinally, shutdown the ThreadPoolExecutor to release resources and return a list for visit url result.\nSolution # \u0026#34;\u0026#34;\u0026#34; # This is HtmlParser\u0026#39;s API interface. # You should not implement it, or speculate about its implementation # \u0026#34;\u0026#34;\u0026#34; #class HtmlParser(object): # def getUrls(self, url): # \u0026#34;\u0026#34;\u0026#34; # :type url: str # :rtype List[str] # \u0026#34;\u0026#34;\u0026#34; from concurrent.futures import ThreadPoolExecutor from threading import Condition class Solution: def __init__(self) -\u0026gt; None: self._queue = list() self._lock = Condition() self._visited = set() def get_hostname(self, url: str): hostname = \u0026#39;.\u0026#39;.join(url.split(\u0026#39;/\u0026#39;)[2].split(\u0026#39;.\u0026#39;)[1:]) return hostname def visit_url(self, url: str): next_urls: List[str] = self._parser.getUrls(url) with self._lock: for next_url in next_urls: if next_url not in self._visited and self.current_hostname == self.get_hostname(next_url) : self._visited.add(next_url) self._queue.insert(0,next_url) def crawl(self, startUrl: str, htmlParser: \u0026#39;HtmlParser\u0026#39;) -\u0026gt; List[str]: self._queue.insert(0,startUrl) self._visited.add(startUrl) self.current_hostname = self.get_hostname(startUrl) self._parser = htmlParser executor = ThreadPoolExecutor() while self._queue: urls = [self._queue.pop(), ] while self._queue: urls.append(self._queue.pop()) excecutor_list = [executor.submit(self.visit_url, (url)) for url in urls] for future in excecutor_list: future.result() executor.shutdown() return list(self._visited) ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/concurrency/web-crawler-multithreaded/","summary":"Description Given a URL startUrl and an interface HtmlParser, implement a Multi-threaded web crawler to crawl all links that are under the same hostname as startUrl.\nReturn all URLs obtained by your web crawler in any order.\nYour crawler should:\nStart from the page: startUrl Call HtmlParser.getUrls(url) to get all URLs from a webpage of a given URL. Do not crawl the same link twice. Explore only the links that are under the same hostname as startUrl.","title":"[leetcode][Python][Concurrency][Medium] 1242. Web Crawler Multithreaded"},{"content":"Description There is an intersection of two roads. First road is road A where cars travel from North to South in direction 1 and from South to North in direction 2. Second road is road B where cars travel from West to East in direction 3 and from East to West in direction 4.\nThere is a traffic light located on each road before the intersection. A traffic light can either be green or red.\nGreen means cars can cross the intersection in both directions of the road. Red means cars in both directions cannot cross the intersection and must wait until the light turns green. The traffic lights cannot be green on both roads at the same time. That means when the light is green on road A, it is red on road B and when the light is green on road B, it is red on road A.\nInitially, the traffic light is green on road A and red on road B. When the light is green on one road, all cars can cross the intersection in both directions until the light becomes green on the other road. No two cars traveling on different roads should cross at the same time.\nDesign a deadlock-free traffic light controlled system at this intersection.\nImplement the function void carArrived(carId, roadId, direction, turnGreen, crossCar) where:\ncarId is the id of the car that arrived. roadId is the id of the road that the car travels on. direction is the direction of the car. turnGreen is a function you can call to turn the traffic light to green on the current road. crossCar is a function you can call to let the current car cross the intersection. Idea Your answer is considered correct if it avoids cars deadlock in the intersection. Turning the light green on a road when it was already green is considered a wrong answer.\nInput: cars = [1,2,3,4,5], directions = [2,4,3,3,1], arrivalTimes = [10,20,30,40,40] Output: [ \u0026#34;Car 1 Has Passed Road A In Direction 2\u0026#34;, // Traffic light on road A is green, car 1 can cross the intersection. \u0026#34;Traffic Light On Road B Is Green\u0026#34;, // Car 2 requests green light for road B. \u0026#34;Car 2 Has Passed Road B In Direction 4\u0026#34;, // Car 2 crosses as the light is green on road B now. \u0026#34;Car 3 Has Passed Road B In Direction 3\u0026#34;, // Car 3 crosses as the light is green on road B now. \u0026#34;Traffic Light On Road A Is Green\u0026#34;, // Car 5 requests green light for road A. \u0026#34;Car 5 Has Passed Road A In Direction 1\u0026#34;, // Car 5 crosses as the light is green on road A now. \u0026#34;Traffic Light On Road B Is Green\u0026#34;, // Car 4 requests green light for road B. Car 4 blocked until car 5 crosses and then traffic light is green on road B. \u0026#34;Car 4 Has Passed Road B In Direction 3\u0026#34; // Car 4 crosses as the light is green on road B now. ] Explanation: This is a dead-lock free scenario. Note that the scenario when car 4 crosses before turning light into green on road A and allowing car 5 to pass is also correct and Accepted scenario. Fulfill requirements :\nHere an important thing that which road can change the traffic control light, but also cars can pass through fast if current light is GREEN with the road their own.\nSo, change traffic control light to Green if the lock status is release, or wait it until release by notify.\nSolution from threading import Condition class TrafficLight: def __init__(self): self.__lock = Condition() self.__light = 1 def carArrived( self, carId: int, # ID of the car roadId: int, # ID of the road the car travels on. Can be 1 (road A) or 2 (road B) direction: int, # Direction of the car turnGreen: \u0026#39;Callable[[], None]\u0026#39;, # Use turnGreen() to turn light to green on current road crossCar: \u0026#39;Callable[[], None]\u0026#39; # Use crossCar() to make car cross the intersection ) -\u0026gt; None: with self.__lock: if self.__light != roadId: turnGreen() self.__light = roadId crossCar() ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/concurrency/traffic-light-controlled-intersection/","summary":"Description There is an intersection of two roads. First road is road A where cars travel from North to South in direction 1 and from South to North in direction 2. Second road is road B where cars travel from West to East in direction 3 and from East to West in direction 4.\nThere is a traffic light located on each road before the intersection. A traffic light can either be green or red.","title":"[leetcode][Python][Concurrency][Easy] 1279. Traffic Light Controlled Intersection"},{"content":"Description Table: Matches\n+-------------+------+ | Column Name | Type | +-------------+------+ | player_id | int | | match_day | date | | result | enum | +-------------+------+ (player_id, match_day) is the primary key for this table. Each row of this table contains the ID of a player, the day of the match they played, and the result of that match. The result column is an ENUM type of (\u0026#39;Win\u0026#39;, \u0026#39;Draw\u0026#39;, \u0026#39;Lose\u0026#39;). The winning streak of a player is the number of consecutive wins uninterrupted by draws or losses.\nWrite an SQL query to count the longest winning streak for each player.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Matches (player_id int, match_day date, result ENUM(\u0026#39;Win\u0026#39;, \u0026#39;Draw\u0026#39;, \u0026#39;Lose\u0026#39;)) Truncate table Matches insert into Matches (player_id, match_day, result) values (\u0026#39;1\u0026#39;, \u0026#39;2022-01-17\u0026#39;, \u0026#39;Win\u0026#39;) insert into Matches (player_id, match_day, result) values (\u0026#39;1\u0026#39;, \u0026#39;2022-01-18\u0026#39;, \u0026#39;Win\u0026#39;) insert into Matches (player_id, match_day, result) values (\u0026#39;1\u0026#39;, \u0026#39;2022-01-25\u0026#39;, \u0026#39;Win\u0026#39;) insert into Matches (player_id, match_day, result) values (\u0026#39;1\u0026#39;, \u0026#39;2022-01-31\u0026#39;, \u0026#39;Draw\u0026#39;) insert into Matches (player_id, match_day, result) values (\u0026#39;1\u0026#39;, \u0026#39;2022-02-08\u0026#39;, \u0026#39;Win\u0026#39;) insert into Matches (player_id, match_day, result) values (\u0026#39;2\u0026#39;, \u0026#39;2022-02-06\u0026#39;, \u0026#39;Lose\u0026#39;) insert into Matches (player_id, match_day, result) values (\u0026#39;2\u0026#39;, \u0026#39;2022-02-08\u0026#39;, \u0026#39;Lose\u0026#39;) insert into Matches (player_id, match_day, result) values (\u0026#39;3\u0026#39;, \u0026#39;2022-03-30\u0026#39;, \u0026#39;Win\u0026#39;) Idea The query result format is in the following example.\n+-----------+----------------+ | player_id | longest_streak | +-----------+----------------+ | 1 | 3 | | 2 | 0 | | 3 | 1 | +-----------+----------------+ Fulfill requirements :\nI guess that I have to extract each game result which is not a winner, and convert _a split timeline_ for each player to help calculating _longest winning streak_.\nSo, I marked each game and sorted by match_day of each player, that will help to find losing matches, then I can use it as a split timeline to calculate with win of continuous games.\nSolution with game_result_rn as ( select row_number() over(partition by player_id order by match_day) as rn, player_id, result from Matches ), player_lose_game_rn as ( select player_id, rn, ifnull(lead(rn, 1) over(partition by player_id order by rn) ,rn) as next_rn from ( select player_id, 0 as rn from game_result_rn group by player_id union select player_id, rn from game_result_rn where result \u0026lt;\u0026gt; \u0026#39;Win\u0026#39; union select player_id, max(rn)+1 as rn from game_result_rn group by player_id ) a ), count_player_win as ( select distinct a.player_id, count(result) over(partition by a.player_id, b.next_rn) as longest_streak from game_result_rn a left join player_lose_game_rn b on b.player_id=a.player_id where a.rn between b.rn and b.next_rn and a.result = \u0026#39;Win\u0026#39; ) select a.player_id, ifnull(max(b.longest_streak), 0) as longest_streak from ( select distinct player_id from Matches ) a left join count_player_win b using(player_id) group by a.player_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/longest-winning-streak/","summary":"Description Table: Matches\n+-------------+------+ | Column Name | Type | +-------------+------+ | player_id | int | | match_day | date | | result | enum | +-------------+------+ (player_id, match_day) is the primary key for this table. Each row of this table contains the ID of a player, the day of the match they played, and the result of that match. The result column is an ENUM type of (\u0026#39;Win\u0026#39;, \u0026#39;Draw\u0026#39;, \u0026#39;Lose\u0026#39;).","title":"[leetcode][Database][Hard] 2173. Longest Winning Streak"},{"content":"Description Table: Orders\n+--------------+------+ | Column Name | Type | +--------------+------+ | order_id | int | | customer_id | int | | order_date | date | | price | int | +--------------+------+ order_id is the primary key for this table. Each row contains the id of an order, the id of customer that ordered it, the date of the order, and its price. Write an SQL query to report the IDs of the customers with the total purchases strictly increasing yearly.\nThe total purchases of a customer in one year is the sum of the prices of their orders in that year. If for some year the customer did not make any order, we consider the total purchases 0. The first year to consider for each customer is the year of their first order. The last year to consider for each customer is the year of their last order. Return the result table in any order.\nSQL Schema\nCreate table If Not Exists Orders (order_id int, customer_id int, order_date date, price int) Truncate table Orders insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2019-07-01\u0026#39;, \u0026#39;1100\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2019-11-01\u0026#39;, \u0026#39;1200\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;3\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2020-05-26\u0026#39;, \u0026#39;3000\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;4\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2021-08-31\u0026#39;, \u0026#39;3100\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;5\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2022-12-07\u0026#39;, \u0026#39;4700\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;6\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;2015-01-01\u0026#39;, \u0026#39;700\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;7\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;2017-11-07\u0026#39;, \u0026#39;1000\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;8\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2017-01-01\u0026#39;, \u0026#39;900\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;9\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2018-11-07\u0026#39;, \u0026#39;900\u0026#39;) Idea The query result format is in the following example.\n+-------------+ | customer_id | +-------------+ | 1 | +-------------+ Fulfill requirements :\nI need to find the range between the first order year to last order year of each customer, and set price as 0 if the year have not order record(s) of customer.\nThen, I can calculate strictly increasing purchases with each year of each customer via window function lead(column, offset) over().\nMarking the calculate result 1 if the total purchase price of next year larger than current year, otherwise, marking 0 . Name marking result as mark_increase_purchases .\nFinally, to compare the counting result of records and total mark_increase_purchases of each customer, I can get the result for customers are strictly increasing purchases or not. Due to next order after last order of each customer is not exist, so the records counting need to minus 1 .\nSolution with recursive cte_customer_order_year as ( select customer_id, year(min(order_date)) as first_order_year, year(max(order_date)) as last_order_year from Orders group by customer_id ), cte_customer_zero_order as ( select customer_id, first_order_year, last_order_year from cte_customer_order_year union select customer_id, first_order_year+1, last_order_year from cte_customer_zero_order where first_order_year \u0026lt; last_order_year ), cte_orders as ( select distinct customer_id, year(order_date) as order_year, sum(price) over(partition by customer_id, year(order_date) order by year(order_date)) as price from ( select customer_id, makedate(first_order_year, 1) as order_date, 0 as price from cte_customer_zero_order union select customer_id, order_date, price from Orders ) union_orders ) select a.customer_id from ( select customer_id, order_year, if( lead(price, 1) over(partition by customer_id order by order_year)-price \u0026gt; 0, 1, 0 ) as mark_increase_purchases from cte_orders ) a group by a.customer_id having sum(a.mark_increase_purchases) = (count(a.customer_id)-1) ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/customers-with-strictly-increasing-purchases/","summary":"Description Table: Orders\n+--------------+------+ | Column Name | Type | +--------------+------+ | order_id | int | | customer_id | int | | order_date | date | | price | int | +--------------+------+ order_id is the primary key for this table. Each row contains the id of an order, the id of customer that ordered it, the date of the order, and its price. Write an SQL query to report the IDs of the customers with the total purchases strictly increasing yearly.","title":"[leetcode][Database][Hard] 2474. Customers With Strictly Increasing Purchases"},{"content":"Description Table: Products\n+-------------+------+ | Column Name | Type | +-------------+------+ | product_id | int | | price | int | +-------------+------+ product_id is the primary key for this table. Each row in this table shows the ID of a product and the price of one unit. Table: Purchases\n+-------------+------+ | Column Name | Type | +-------------+------+ | invoice_id | int | | product_id | int | | quantity | int | +-------------+------+ (invoice_id, product_id) is the primary key for this table. Each row in this table shows the quantity ordered from one product in an invoice. Write an SQL query to show the details of the invoice with the highest price. If two or more invoices have the same price, return the details of the one with the smallest invoice_id.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Products (product_id int, price int) Create table If Not Exists Purchases (invoice_id int, product_id int, quantity int) Truncate table Products insert into Products (product_id, price) values (\u0026#39;1\u0026#39;, \u0026#39;100\u0026#39;) insert into Products (product_id, price) values (\u0026#39;2\u0026#39;, \u0026#39;200\u0026#39;) Truncate table Purchases insert into Purchases (invoice_id, product_id, quantity) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;) insert into Purchases (invoice_id, product_id, quantity) values (\u0026#39;3\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;) insert into Purchases (invoice_id, product_id, quantity) values (\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;) insert into Purchases (invoice_id, product_id, quantity) values (\u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;4\u0026#39;) insert into Purchases (invoice_id, product_id, quantity) values (\u0026#39;4\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;10\u0026#39;) Idea The query result format is shown in the following example.\n+------------+----------+-------+ | product_id | quantity | price | +------------+----------+-------+ | 2 | 3 | 600 | | 1 | 4 | 400 | +------------+----------+-------+ Fulfill requirements :\nI used the function sum() over() to calculate totally price of each invoice, and raking the above calaulate result by sorted with invoide_id ascending, and totally pricing descending, and name ranking result as rn.\nThen, finding the record which rn equals to 1 for query result.\nSolution with cte as ( select invoice_id, product_id, quantity, price, s_price, row_number() over(order by s_price desc, invoice_id) as rn from ( select a.invoice_id, a.product_id, a.quantity, ifnull(a.quantity * b.price, 0) as price, sum(ifnull(a.quantity * b.price, 0)) over(partition by a.invoice_id) as s_price from Purchases a left join Products b using(product_id) ) detail ) select product_id, quantity, price from cte where invoice_id = (select invoice_id from cte where rn = 1) ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/generate-the-invoice/","summary":"Description Table: Products\n+-------------+------+ | Column Name | Type | +-------------+------+ | product_id | int | | price | int | +-------------+------+ product_id is the primary key for this table. Each row in this table shows the ID of a product and the price of one unit. Table: Purchases\n+-------------+------+ | Column Name | Type | +-------------+------+ | invoice_id | int | | product_id | int | | quantity | int | +-------------+------+ (invoice_id, product_id) is the primary key for this table.","title":"[leetcode][Database][Hard] 2362. Generate the Invoice"},{"content":"Description Table: Keywords\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | topic_id | int | | word | varchar | +-------------+---------+ (topic_id, word) is the primary key for this table. Each row of this table contains the id of a topic and a word that is used to express this topic. There may be more than one word to express the same topic and one word may be used to express multiple topics. Table: Posts\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | post_id | int | | content | varchar | +-------------+---------+ post_id is the primary key for this table. Each row of this table contains the ID of a post and its content. Content will consist only of English letters and spaces. Leetcode has collected some posts from its social media website and is interested in finding the topics of each post. Each topic can be expressed by one or more keywords. If a keyword of a certain topic exists in the content of a post (case insensitive) then the post has this topic.\nWrite an SQL query to find the topics of each post according to the following rules:\nIf the post does not have keywords from any topic, its topic should be \u0026quot;Ambiguous!\u0026quot;. If the post has at least one keyword of any topic, its topic should be a string of the IDs of its topics sorted in ascending order and separated by commas ','. The string should not contain duplicate IDs. Return the result table in any order.\nSQL Schema\nCreate table If Not Exists Keywords (topic_id int, word varchar(25)) Create table If Not Exists Posts (post_id int, content varchar(100)) Truncate table Keywords insert into Keywords (topic_id, word) values (\u0026#39;1\u0026#39;, \u0026#39;handball\u0026#39;) insert into Keywords (topic_id, word) values (\u0026#39;1\u0026#39;, \u0026#39;football\u0026#39;) insert into Keywords (topic_id, word) values (\u0026#39;3\u0026#39;, \u0026#39;WAR\u0026#39;) insert into Keywords (topic_id, word) values (\u0026#39;2\u0026#39;, \u0026#39;Vaccine\u0026#39;) Truncate table Posts insert into Posts (post_id, content) values (\u0026#39;1\u0026#39;, \u0026#39;We call it soccer They call it football hahaha\u0026#39;) insert into Posts (post_id, content) values (\u0026#39;2\u0026#39;, \u0026#39;Americans prefer basketball while Europeans love handball and football\u0026#39;) insert into Posts (post_id, content) values (\u0026#39;3\u0026#39;, \u0026#39;stop the war and play handball\u0026#39;) insert into Posts (post_id, content) values (\u0026#39;4\u0026#39;, \u0026#39;warning I planted some flowers this morning and then got vaccinated\u0026#39;) Idea The query result format is in the following example.\n+---------+------------+ | post_id | topic | +---------+------------+ | 1 | 1 | | 2 | 1 | | 3 | 1,3 | | 4 | Ambiguous! | +---------+------------+ Fulfill requirements :\nI guess that will maybe use the functions SUBSTRING or SUBSTRING_INDE when most people, include me, seem this question at first. But I would like to solve this question via function INSTR .\nAs we can find the definition for INSTR in MySQL official documentation.\nReturns the position of the first occurrence of substring substr in string str.\nThis is the same as the two-argument form of LOCATE(),\nexcept that the order of the arguments is reversed.\nmysql \u0026gt; SELECT INSTR(\u0026#39;foobarbar\u0026#39;, \u0026#39;bar\u0026#39;); -\u0026gt; 4 mysql \u0026gt; SELECT INSTR(\u0026#39;xbar\u0026#39;, \u0026#39;foobar\u0026#39;); -\u0026gt; 0 So, in the first step, an easy way for adding two space characters as prefix and suffix for a post content, it’s help to find the first/last words are keyword or not in a post content via function INSTR to use {SPACE_SYMBOL}{keyword}{SPACE_SYMBOL} as a condition.\nINSTR will return a position(or an index) of the content if a keyword in this content, but also return 0 if a keyword not in this content.\nFinally, using the table Posts as a main table in query, and using left join to associate the result from INSTR to map if topic(s) is/are in a post content. Also, replacing the topic to Ambiguous! if here haven’t a topic in.\nSolution with cte_post as ( select post_id, concat(\u0026#39; \u0026#39;, content, \u0026#39; \u0026#39;) as content from Posts -- The easy way for INSTR() to find keyword ), cte_match_topics as ( select a.post_id, group_concat(distinct b.topic_id separator \u0026#39;,\u0026#39;) as topic from cte_post a, Keywords b where INSTR(a.content, concat(\u0026#39; \u0026#39;, b.word, \u0026#39; \u0026#39;)) \u0026gt; 0 -- find keyword position group by a.post_id ) select distinct(a.post_id), ifnull(b.topic, \u0026#39;Ambiguous!\u0026#39;) as topic from Posts a left join cte_match_topics b using(post_id) order by 1 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/finding-the-topic-of-each-post/","summary":"Description Table: Keywords\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | topic_id | int | | word | varchar | +-------------+---------+ (topic_id, word) is the primary key for this table. Each row of this table contains the id of a topic and a word that is used to express this topic. There may be more than one word to express the same topic and one word may be used to express multiple topics.","title":"[leetcode][Database][Hard] 2199. Finding the Topic of Each Post"},{"content":"Description Table: Candidates\n+-------------+------+ | Column Name | Type | +-------------+------+ | employee_id | int | | experience | enum | | salary | int | +-------------+------+ employee_id is the primary key column for this table. experience is an enum with one of the values (\u0026#39;Senior\u0026#39;, \u0026#39;Junior\u0026#39;). Each row of this table indicates the id of a candidate, their monthly salary, and their experience. The salary of each candidate is guaranteed to be unique. A company wants to hire new employees. The budget of the company for the salaries is $70000. The company\u0026rsquo;s criteria for hiring are:\nKeep hiring the senior with the smallest salary until you cannot hire any more seniors. Use the remaining budget to hire the junior with the smallest salary. Keep hiring the junior with the smallest salary until you cannot hire any more juniors. Write an SQL query to find the ids of seniors and juniors hired under the mentioned criteria.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Candidates (employee_id int, experience ENUM(\u0026#39;Senior\u0026#39;, \u0026#39;Junior\u0026#39;), salary int) Truncate table Candidates insert into Candidates (employee_id, experience, salary) values (\u0026#39;1\u0026#39;, \u0026#39;Junior\u0026#39;, \u0026#39;10000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;9\u0026#39;, \u0026#39;Junior\u0026#39;, \u0026#39;15000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;2\u0026#39;, \u0026#39;Senior\u0026#39;, \u0026#39;20000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;11\u0026#39;, \u0026#39;Senior\u0026#39;, \u0026#39;16000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;13\u0026#39;, \u0026#39;Senior\u0026#39;, \u0026#39;50000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;4\u0026#39;, \u0026#39;Junior\u0026#39;, \u0026#39;40000\u0026#39;) Idea The query result format is in the following example.\n+-------------+ | employee_id | +-------------+ | 11 | | 2 | | 1 | | 9 | +-------------+ Fulfill requirements :\nThe thinking process like as [leetcode][Database][Hard] 2004. The Number of Seniors and Juniors to Join the Company.\nSo we can calculating the cumulative salary, sorted the salary of each employee and partition by different experience at first.\nNext, finding the max cumulative salary which less than budget $70,000 and take employee_id which are fitting the above condition. Also, finding the junior employees by remaining budget from hired senior employees.\nSolution with salary_cumulative as ( select employee_id, experience, salary, sum(salary) over(partition by experience order by salary) as cumulative_salary from Candidates ), senior_hiring as ( select -1 as employee_id, 0 as cumulative_salary union select employee_id, cumulative_salary from salary_cumulative where experience = \u0026#39;Senior\u0026#39; and 70000 - cumulative_salary \u0026gt;=0 ), junior_hiring as ( select employee_id from salary_cumulative join ( select 70000-max(cumulative_salary) as remaining from senior_hiring ) senior where experience = \u0026#39;Junior\u0026#39; and remaining-cumulative_salary\u0026gt;=0 ) select employee_id from senior_hiring where employee_id \u0026gt; 0 union select employee_id from junior_hiring ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/the-number-of-seniors-and-juniors-to-join-the-company-ii/","summary":"Description Table: Candidates\n+-------------+------+ | Column Name | Type | +-------------+------+ | employee_id | int | | experience | enum | | salary | int | +-------------+------+ employee_id is the primary key column for this table. experience is an enum with one of the values (\u0026#39;Senior\u0026#39;, \u0026#39;Junior\u0026#39;). Each row of this table indicates the id of a candidate, their monthly salary, and their experience. The salary of each candidate is guaranteed to be unique.","title":"[leetcode][Database][Hard] 2010. The Number of Seniors and Juniors to Join the Company II"},{"content":"Description Table: Terms\n+-------------+------+ | Column Name | Type | +-------------+------+ | power | int | | factor | int | +-------------+------+ power is the primary key column for this table. Each row of this table contains information about one term of the equation. power is an integer in the range [0, 100]. factor is an integer in the range [-100, 100] and cannot be zero. You have a very powerful program that can solve any equation of one variable in the world. The equation passed to the program must be formatted as follows:\nThe left-hand side (LHS) should contain all the terms. The right-hand side (RHS) should be zero. Each term of the LHS should follow the format \u0026quot;\u0026lt;sign\u0026gt;\u0026lt;fact\u0026gt;X^\u0026lt;pow\u0026gt;\u0026quot; where:\n\u0026lt;sign\u0026gt; is either \u0026quot;+\u0026quot; or \u0026quot;-\u0026quot;.\n\u0026lt;fact\u0026gt; is the absolute value of the factor.\n\u0026lt;pow\u0026gt; is the value of the power. If the power is 1, do not add \u0026quot;^\u0026lt;pow\u0026gt;\u0026quot;.\nFor example, if power = 1 and factor = 3, the term will be \u0026quot;+3X\u0026quot;. If the power is 0, add neither \u0026quot;X\u0026quot; nor \u0026quot;^\u0026lt;pow\u0026gt;\u0026quot;.\nFor example, if power = 0 and factor = -3, the term will be \u0026quot;-3\u0026quot;. The powers in the LHS should be sorted in descending order. Write an SQL query to build the equation.\nSQL Schema\nCreate table If Not Exists Terms (power int, factor int) Truncate table Terms insert into Terms (power, factor) values (\u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;) insert into Terms (power, factor) values (\u0026#39;1\u0026#39;, \u0026#39;-4\u0026#39;) insert into Terms (power, factor) values (\u0026#39;0\u0026#39;, \u0026#39;2\u0026#39;) Idea The query result format is in the following example.\n+-----------------+ | equation | +-----------------+ | -4X^4+1X^2-1X=0 | +-----------------+ Fulfill requirements :\nFor generating the equation by records of table Terms , it can be reorganize to a few parts of a term {factor sign}{absolute factor value}{power of X sign}{power value} For example :\nTerm will be present -4X when the record.factor=-4 , record.power=1 Term will be present +2x^4 when the record.factor=2 , record.power=4 Term will be present +1 when the record.factor=1 , record.power=0 Finally, using the function group_concat() combainating all of the terms from reorganize.\nSolution with builder as ( select 0 as LHS, -1 as rk, \u0026#39;=0\u0026#39; as e union select 0 as LHS, row_number() over(order by power) rk, concat( if(factor \u0026gt;0, \u0026#39;+\u0026#39;, \u0026#39;-\u0026#39;), -- factor sign abs(factor), -- remove factor sign of the value if(power=0, \u0026#39;\u0026#39;, if(power=1, \u0026#39;X\u0026#39;, \u0026#39;X^\u0026#39;) ), -- power of X if(power\u0026lt;2, \u0026#39;\u0026#39;, power) -- show power text when power large than 2 ) e from Terms ) select group_concat( e order by rk desc separator \u0026#39;\u0026#39; ) as equation from builder group by LHS ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/build-the-equation/","summary":"Description Table: Terms\n+-------------+------+ | Column Name | Type | +-------------+------+ | power | int | | factor | int | +-------------+------+ power is the primary key column for this table. Each row of this table contains information about one term of the equation. power is an integer in the range [0, 100]. factor is an integer in the range [-100, 100] and cannot be zero. You have a very powerful program that can solve any equation of one variable in the world.","title":"[leetcode][Database][Hard] 2118. Build the Equation"},{"content":"Description Table: Products\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | product_id | int | | store_name1 | int | | store_name2 | int | | : | int | | : | int | | : | int | | store_namen | int | +-------------+---------+ product_id is the primary key for this table. Each row in this table indicates the product\u0026#39;s price in n different stores. If the product is not available in a store, the price will be null in that store\u0026#39;s column. The names of the stores may change from one testcase to another. There will be at least 1 store and at most 30 stores. Important note: This problem targets those who have a good experience with SQL. If you are a beginner, we recommend that you skip it for now.\nImplement the procedure UnpivotProducts to reorganize the Products table so that each row has the id of one product, the name of a store where it is sold, and its price in that store. If a product is not available in a store, do not include a row with that product_id and store combination in the result table. There should be three columns: product_id, store, and price.\nThe procedure should return the table after reorganizing it.\nReturn the result table in any order.\nSQL Schema\nTruncate table Products insert into Products (product_id, LC_Store, Nozama, Shop, Souq) values (\u0026#39;1\u0026#39;, \u0026#39;100\u0026#39;, \u0026#39;None\u0026#39;, \u0026#39;110\u0026#39;, \u0026#39;None\u0026#39;) insert into Products (product_id, LC_Store, Nozama, Shop, Souq) values (\u0026#39;2\u0026#39;, \u0026#39;None\u0026#39;, \u0026#39;200\u0026#39;, \u0026#39;None\u0026#39;, \u0026#39;190\u0026#39;) insert into Products (product_id, LC_Store, Nozama, Shop, Souq) values (\u0026#39;3\u0026#39;, \u0026#39;None\u0026#39;, \u0026#39;None\u0026#39;, \u0026#39;1000\u0026#39;, \u0026#39;1900\u0026#39;) Idea The query result format is in the following example.\n+------------+----------+-------+ | product_id | store | price | +------------+----------+-------+ | 1 | LC_Store | 100 | | 1 | Shop | 110 | | 2 | Nozama | 200 | | 2 | Souq | 190 | | 3 | Shop | 1000 | | 3 | Souq | 1900 | +------------+----------+-------+ Refer bofeng07\u0026rsquo;s MySQL solution, it a grate idea!\nGetting the column names of table Products from information_schema.columns, using group_concat to combian each column values of each row of table Products by union .\nSolution CREATE PROCEDURE UnpivotProducts() BEGIN set session group_concat_max_len = 1000000; set @macro = null; select group_concat( concat( \u0026#39;select product_id, \u0026#34;\u0026#39;, column_name, \u0026#39;\u0026#34; as store, \u0026#39;, column_name, \u0026#39; as price \u0026#39;, \u0026#39;from Products \u0026#39;, \u0026#39;where \u0026#39;, column_name, \u0026#39; is not null\u0026#39; ) separator \u0026#39; union \u0026#39; ) into @macro from information_schema.columns where table_schema=\u0026#39;test\u0026#39; and table_name=\u0026#39;Products\u0026#39; and column_name != \u0026#39;product_id\u0026#39;; prepare sql_query from @macro; execute sql_query; deallocate prepare sql_query; END ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/dynamic-unpivoting-of-a-table/","summary":"Description Table: Products\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | product_id | int | | store_name1 | int | | store_name2 | int | | : | int | | : | int | | : | int | | store_namen | int | +-------------+---------+ product_id is the primary key for this table. Each row in this table indicates the product\u0026#39;s price in n different stores. If the product is not available in a store, the price will be null in that store\u0026#39;s column.","title":"[leetcode][Database][Hard]2253. Dynamic Unpivoting of a Table"},{"content":"Description Table: Listens\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | song_id | int | | day | date | +-------------+---------+ There is no primary key for this table. It may contain duplicates. Each row of this table indicates that the user user_id listened to the song song_id on the day day. Table: Friendship\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user1_id | int | | user2_id | int | +---------------+---------+ (user1_id, user2_id) is the primary key for this table. Each row of this table indicates that the users user1_id and user2_id are friends. Note that user1_id \u0026lt; user2_id. Write an SQL query to report the similar friends of Leetcodify users. A user x and user y are similar friends if:\nUsers x and y are friends, and Users x and y listened to the same three or more different songs on the same day. Return the result table in any order. Note that you must return the similar pairs of friends the same way they were represented in the input (i.e., always user1_id \u0026lt; user2_id).\nSQL Schema\nCreate table If Not Exists Listens (user_id int, song_id int, day date) Create table If Not Exists Friendship (user1_id int, user2_id int) Truncate table Listens insert into Listens (user_id, song_id, day) values (\u0026#39;1\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;1\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;1\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;2\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;2\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;2\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;3\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;3\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;3\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;4\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;4\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;4\u0026#39;, \u0026#39;13\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;5\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-16\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;5\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-16\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;5\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-16\u0026#39;) Truncate table Friendship insert into Friendship (user1_id, user2_id) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;2\u0026#39;, \u0026#39;5\u0026#39;) Idea The query result format is in the following example.\n+----------+----------+ | user1_id | user2_id | +----------+----------+ | 1 | 2 | +----------+----------+ Fulfill requirements :\nThe thinking process like as [leetcode][Database][Hard] 1917. Leetcodify Friends Recommendations, the different is this question asking to find similar friend (i.e. user x and user y have a pair record in Friendship).\nSo, we can do that with the same logic to find user x and user y are listened to the same three or more different songs on the same day or not.\nListing each user and their friends cte_user_friends . To avoid the records that user listen the same song in a day, using the distinct to remove duplicates records. Finally, counting the song_id via group by day, user_id, recommended_id, and filtering the counting song_id value large than or equals to 3 after group by statement.\nSolution with cte_user_friends as ( select user1_id as user1_id, user2_id as user2_id from Friendship union select user2_id as user1_id, user1_id as user2_id from Friendship ), cte_listen_distinct as ( select distinct user_id, song_id , day from Listens ), cte_similar_friend as ( select a.user_id as user1_id, b.user_id as user2_id from cte_listen_distinct a # user1 left join cte_listen_distinct b on b.song_id=a.song_id and a.day=b.day # user2 left join cte_user_friends c on c.user1_id = a.user_id and c.user2_id = b.user_id where a.user_id \u0026lt;\u0026gt; b.user_id and user1_id is not null group by a.day, a.user_id, b.user_id having count(a.song_id) \u0026gt;=3 ) select distinct b.user1_id, b.user2_id from Friendship a join cte_similar_friend b on b.user1_id = a.user1_id and b.user2_id = a.user2_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/leetcodify-similar-friends/","summary":"Description Table: Listens\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | song_id | int | | day | date | +-------------+---------+ There is no primary key for this table. It may contain duplicates. Each row of this table indicates that the user user_id listened to the song song_id on the day day. Table: Friendship\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user1_id | int | | user2_id | int | +---------------+---------+ (user1_id, user2_id) is the primary key for this table.","title":"[leetcode][Database][Hard] 1919. Leetcodify Similar Friends"},{"content":"Description Table: Listens\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | song_id | int | | day | date | +-------------+---------+ There is no primary key for this table. It may contain duplicates. Each row of this table indicates that the user user_id listened to the song song_id on the day day. Table: Friendship\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user1_id | int | | user2_id | int | +---------------+---------+ (user1_id, user2_id) is the primary key for this table. Each row of this table indicates that the users user1_id and user2_id are friends. Note that user1_id \u0026lt; user2_id. Write an SQL query to recommend friends to Leetcodify users. We recommend user x to user y if:\nUsers x and y are not friends, and Users x and y listened to the same three or more different songs on the same day. Note that friend recommendations are unidirectional, meaning if user x and user y should be recommended to each other, the result table should have both user x recommended to user y and user y recommended to user x. Also, note that the result table should not contain duplicates (i.e., user y should not be recommended to user x multiple times.).\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Listens (user_id int, song_id int, day date) Create table If Not Exists Friendship (user1_id int, user2_id int) Truncate table Listens insert into Listens (user_id, song_id, day) values (\u0026#39;1\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;1\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;1\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;2\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;2\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;2\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;3\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;3\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;3\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;4\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;4\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;4\u0026#39;, \u0026#39;13\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;5\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-16\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;5\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-16\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;5\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-16\u0026#39;) Truncate table Friendship insert into Friendship (user1_id, user2_id) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;) Idea The query result format is in the following example.\n+---------+----------------+ | user_id | recommended_id | +---------+----------------+ | 1 | 3 | | 2 | 3 | | 3 | 1 | | 3 | 2 | +---------+----------------+ Fulfill requirements :\nThe thinking process like as [leetcode][Database][Hard] 1892. Page Recommendations II, so the first step is listing each user and their friends cte_user_friends .\nTo avoid the records that user listen the same song in a day, using the distinct to remove duplicates records.\nFinding the users listening a song in a day via self join cte_listen_distinct , and then confirm the user friendship who in above self join set and remove them.\nFinally, counting the song_id via group by day, user_id, recommended_id, and filtering the counting song_id value large than or equals to 3 after group by statement.\nSolution with cte_user_friends as ( select user1_id as user_id, user2_id as friend_id from Friendship union select user2_id as user_id, user1_id as friend_id from Friendship ), cte_listen_distinct as ( select distinct user_id, song_id, day from Listens ) select distinct a.user_id, b.user_id as recommended_id from cte_listen_distinct a left join cte_listen_distinct b on b.song_id=a.song_id and a.day=b.day left join cte_user_friends c on c.user_id = a.user_id and c.friend_id = b.user_id where c.user_id is null and a.user_id \u0026lt;\u0026gt; b.user_id group by a.day, a.user_id, b.user_id having count(a.song_id) \u0026gt;=3 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/leetcodify-friends-recommendations/","summary":"Description Table: Listens\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | song_id | int | | day | date | +-------------+---------+ There is no primary key for this table. It may contain duplicates. Each row of this table indicates that the user user_id listened to the song song_id on the day day. Table: Friendship\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user1_id | int | | user2_id | int | +---------------+---------+ (user1_id, user2_id) is the primary key for this table.","title":"[leetcode][Database][Hard] 1917. Leetcodify Friends Recommendations"},{"content":"Description Table: Friendship\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user1_id | int | | user2_id | int | +---------------+---------+ (user1_id, user2_id) is the primary key for this table. Each row of this table indicates that the users user1_id and user2_id are friends. Table: Likes\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | page_id | int | +-------------+---------+ (user_id, page_id) is the primary key for this table. Each row of this table indicates that user_id likes page_id. You are implementing a page recommendation system for a social media website. Your system will recommended a page to user_id if the page is liked by at least one friend of user_id and is not liked by user_id.\nWrite an SQL query to find all the possible page recommendations for every user. Each recommendation should appear as a row in the result table with these columns:\nuser_id: The ID of the user that your system is making the recommendation to. page_id: The ID of the page that will be recommended to user_id. friends_likes: The number of the friends of user_id that like page_id. Return result table in any order.\nSQL Schema\nCreate table If Not Exists Friendship (user1_id int, user2_id int) Create table If Not Exists Likes (user_id int, page_id int) Truncate table Friendship insert into Friendship (user1_id, user2_id) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;1\u0026#39;, \u0026#39;3\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;1\u0026#39;, \u0026#39;4\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;2\u0026#39;, \u0026#39;5\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;6\u0026#39;, \u0026#39;1\u0026#39;) Truncate table Likes insert into Likes (user_id, page_id) values (\u0026#39;1\u0026#39;, \u0026#39;88\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;2\u0026#39;, \u0026#39;23\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;3\u0026#39;, \u0026#39;24\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;4\u0026#39;, \u0026#39;56\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;5\u0026#39;, \u0026#39;11\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;6\u0026#39;, \u0026#39;33\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;2\u0026#39;, \u0026#39;77\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;3\u0026#39;, \u0026#39;77\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;6\u0026#39;, \u0026#39;88\u0026#39;) Idea The query result format is in the following example.\n+---------+---------+---------------+ | user_id | page_id | friends_likes | +---------+---------+---------------+ | 1 | 77 | 2 | | 1 | 23 | 1 | | 1 | 24 | 1 | | 1 | 56 | 1 | | 1 | 33 | 1 | | 2 | 24 | 1 | | 2 | 56 | 1 | | 2 | 11 | 1 | | 2 | 88 | 1 | | 3 | 88 | 1 | | 3 | 23 | 1 | | 4 | 88 | 1 | | 4 | 77 | 1 | | 4 | 23 | 1 | | 5 | 77 | 1 | | 5 | 23 | 1 | +---------+---------+---------------+ Fulfill requirements :\nThe thinking process like as [leetcode][Database][Hard]1972. First and Last Call On the Same Day, so the first step is listing each user and their friends cte_all_users.\nFinding the pages which are friends likes, then to find which pages both user and friends likes. Finally, filtering not match the condition : the page user not liked but friends did.\nThis solution have same concept with minus or except, finding the difference set between both user and friends likes pages and only friends like pages.\nSolution with cte_all_users as ( select user1_id as user_id, user2_id as friend from Friendship union select user2_id as user_id, user1_id as friend from Friendship ) select distinct a.user_id, b.page_id , count(a.friend) as friends_likes from cte_all_users a join Likes b on b.user_id = a.friend -- find the pages friend like left join Likes c on c.user_id = a.user_id and b.page_id = c.page_id -- find the pages both user and friend like where c.page_id is null -- filtering not match the condition : the page user not liked but friends did. group by a.user_id, b.page_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/page-recommendations-ii/","summary":"Description Table: Friendship\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user1_id | int | | user2_id | int | +---------------+---------+ (user1_id, user2_id) is the primary key for this table. Each row of this table indicates that the users user1_id and user2_id are friends. Table: Likes\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | page_id | int | +-------------+---------+ (user_id, page_id) is the primary key for this table.","title":"[leetcode][Database][Hard] 1892. Page Recommendations II"},{"content":"Description Table: Tasks\n+----------------+---------+ | Column Name | Type | +----------------+---------+ | task_id | int | | subtasks_count | int | +----------------+---------+ task_id is the primary key for this table. Each row in this table indicates that task_id was divided into subtasks_count subtasks labeled from 1 to subtasks_count. It is guaranteed that 2 \u0026lt;= subtasks_count \u0026lt;= 20. Table: Executed\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | task_id | int | | subtask_id | int | +---------------+---------+ (task_id, subtask_id) is the primary key for this table. Each row in this table indicates that for the task task_id, the subtask with ID subtask_id was executed successfully. It is guaranteed that subtask_id \u0026lt;= subtasks_count for each task_id. Write an SQL query to report the IDs of the missing subtasks for each task_id.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Tasks (task_id int, subtasks_count int) Create table If Not Exists Executed (task_id int, subtask_id int) Truncate table Tasks insert into Tasks (task_id, subtasks_count) values (\u0026#39;1\u0026#39;, \u0026#39;3\u0026#39;) insert into Tasks (task_id, subtasks_count) values (\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;) insert into Tasks (task_id, subtasks_count) values (\u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;) Truncate table Executed insert into Executed (task_id, subtask_id) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;) insert into Executed (task_id, subtask_id) values (\u0026#39;3\u0026#39;, \u0026#39;1\u0026#39;) insert into Executed (task_id, subtask_id) values (\u0026#39;3\u0026#39;, \u0026#39;2\u0026#39;) insert into Executed (task_id, subtask_id) values (\u0026#39;3\u0026#39;, \u0026#39;3\u0026#39;) insert into Executed (task_id, subtask_id) values (\u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;) Idea The query result format is in the following example.\n+---------+------------+ | task_id | subtask_id | +---------+------------+ | 1 | 1 | | 1 | 3 | | 2 | 1 | | 2 | 2 | +---------+------------+ Fulfill requirements :\nUsing the with recursive to generate a serial number for subtasks_id from 1 to 20, then finding not executed subtasks with condition ifnull(task_id) or ifnull(subtask_id) which subtask_id records not in table Executed.\nSolution with recursive cte_subtasks_sn as ( SELECT 1 AS subtask_id UNION ALL SELECT subtask_id + 1 FROM cte_subtasks_sn WHERE subtask_id \u0026lt; 20 ), cte_subtasks_count as ( select a.task_id as task_id, b.subtask_id as subtask_id from Tasks a, cte_subtasks_sn b where b.subtask_id \u0026lt;= a.subtasks_count ) select a.task_id, a.subtask_id from cte_subtasks_count a left join Executed b using(task_id, subtask_id) where ifnull(b.task_id, -1) = -1 or ifnull(b.subtask_id, -1) = -1 order by task_id, subtask_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/find-the-subtasks-that-did-not-execute/","summary":"Description Table: Tasks\n+----------------+---------+ | Column Name | Type | +----------------+---------+ | task_id | int | | subtasks_count | int | +----------------+---------+ task_id is the primary key for this table. Each row in this table indicates that task_id was divided into subtasks_count subtasks labeled from 1 to subtasks_count. It is guaranteed that 2 \u0026lt;= subtasks_count \u0026lt;= 20. Table: Executed\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | task_id | int | | subtask_id | int | +---------------+---------+ (task_id, subtask_id) is the primary key for this table.","title":"[leetcode][Database][Hard] 1767. Find the Subtasks That Did Not Execute"},{"content":"Description Table: Drivers\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | driver_id | int | | join_date | date | +-------------+---------+ driver_id is the primary key for this table. Each row of this table contains the driver\u0026#39;s ID and the date they joined the Hopper company. Table: Rides\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | ride_id | int | | user_id | int | | requested_at | date | +--------------+---------+ ride_id is the primary key for this table. Each row of this table contains the ID of a ride, the user\u0026#39;s ID that requested it, and the day they requested it. There may be some ride requests in this table that were not accepted. Table: AcceptedRides\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | ride_id | int | | driver_id | int | | ride_distance | int | | ride_duration | int | +---------------+---------+ ride_id is the primary key for this table. Each row of this table contains some information about an accepted ride. It is guaranteed that each accepted ride exists in the Rides table. Write an SQL query to compute the average_ride_distance and average_ride_duration of every 3-month window starting from January - March 2020 to October - December 2020. Round average_ride_distance and average_ride_duration to the nearest two decimal places.\nThe average_ride_distance is calculated by summing up the total ride_distance values from the three months and dividing it by 3. The average_ride_duration is calculated in a similar way.\nReturn the result table ordered by month in ascending order, where month is the starting month\u0026rsquo;s number (January is 1, February is 2, etc.).\nSQL Schema\nCreate table If Not Exists Drivers (driver_id int, join_date date) Create table If Not Exists Rides (ride_id int, user_id int, requested_at date) Create table If Not Exists AcceptedRides (ride_id int, driver_id int, ride_distance int, ride_duration int) Truncate table Drivers insert into Drivers (driver_id, join_date) values (\u0026#39;10\u0026#39;, \u0026#39;2019-12-10\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;8\u0026#39;, \u0026#39;2020-1-13\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;5\u0026#39;, \u0026#39;2020-2-16\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;7\u0026#39;, \u0026#39;2020-3-8\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;4\u0026#39;, \u0026#39;2020-5-17\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;1\u0026#39;, \u0026#39;2020-10-24\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;6\u0026#39;, \u0026#39;2021-1-5\u0026#39;) Truncate table Rides insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;6\u0026#39;, \u0026#39;75\u0026#39;, \u0026#39;2019-12-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;1\u0026#39;, \u0026#39;54\u0026#39;, \u0026#39;2020-2-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;10\u0026#39;, \u0026#39;63\u0026#39;, \u0026#39;2020-3-4\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;19\u0026#39;, \u0026#39;39\u0026#39;, \u0026#39;2020-4-6\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;3\u0026#39;, \u0026#39;41\u0026#39;, \u0026#39;2020-6-3\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;13\u0026#39;, \u0026#39;52\u0026#39;, \u0026#39;2020-6-22\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;7\u0026#39;, \u0026#39;69\u0026#39;, \u0026#39;2020-7-16\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;17\u0026#39;, \u0026#39;70\u0026#39;, \u0026#39;2020-8-25\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;20\u0026#39;, \u0026#39;81\u0026#39;, \u0026#39;2020-11-2\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;5\u0026#39;, \u0026#39;57\u0026#39;, \u0026#39;2020-11-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;2\u0026#39;, \u0026#39;42\u0026#39;, \u0026#39;2020-12-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;11\u0026#39;, \u0026#39;68\u0026#39;, \u0026#39;2021-1-11\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;15\u0026#39;, \u0026#39;32\u0026#39;, \u0026#39;2021-1-17\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;12\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-1-19\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;14\u0026#39;, \u0026#39;18\u0026#39;, \u0026#39;2021-1-27\u0026#39;) Truncate table AcceptedRides insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;10\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;63\u0026#39;, \u0026#39;38\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;13\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;73\u0026#39;, \u0026#39;96\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;100\u0026#39;, \u0026#39;28\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;17\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;119\u0026#39;, \u0026#39;68\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;20\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;121\u0026#39;, \u0026#39;92\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;5\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;42\u0026#39;, \u0026#39;101\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;38\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;11\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;37\u0026#39;, \u0026#39;43\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;15\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;108\u0026#39;, \u0026#39;82\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;12\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;38\u0026#39;, \u0026#39;34\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;14\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;90\u0026#39;, \u0026#39;74\u0026#39;) Idea The query result format is in the following example.\n+-------+-----------------------+-----------------------+ | month | average_ride_distance | average_ride_duration | +-------+-----------------------+-----------------------+ | 1 | 21.00 | 12.67 | | 2 | 21.00 | 12.67 | | 3 | 21.00 | 12.67 | | 4 | 24.33 | 32.00 | | 5 | 57.67 | 41.33 | | 6 | 97.33 | 64.00 | | 7 | 73.00 | 32.00 | | 8 | 39.67 | 22.67 | | 9 | 54.33 | 64.33 | | 10 | 56.33 | 77.00 | +-------+-----------------------+-----------------------+ Fulfill requirements :\nBefore all, I would like to share a problem for timeout limit exceeded of this question when I submit my solution. Because of the pervious solution, I used subquery in select column statement to calculate average distance and average duration.\nSo, I’m learned when the query statement meet a large dataset, the subquery will spending more process resource to calculate its.\nFor the above reason, I modifying the output query used function lead() getting next two months distance and duration for each row instead of subquery. From the response of submissions, it’s worked and improve the query performance.\nSolution with cte_month as ( select 1 as month union select 2 as month union select 3 as month union select 4 as month union select 5 as month union select 6 as month union select 7 as month union select 8 as month union select 9 as month union select 10 as month union select 11 as month union select 12 as month ), cte_accepted_rides as ( select c.month, sum(c.distance) as distance, sum(c.duration) as duration from ( select month(b.requested_at) as month, a.ride_distance as distance, a.ride_duration as duration from AcceptedRides a join Rides b using(ride_id) where year(b.requested_at) = 2020 ) c group by c.month ), cte_ride_info as ( select a.month, ifnull(b.distance, 0) as m0_distance, ifnull(b.duration, 0) as m0_duration, ifnull(lead(b.distance, 1) over(order by a.month), 0) as m1_distance, ifnull(lead(b.duration, 1) over(order by a.month), 0) as m1_duration, ifnull(lead(b.distance, 2) over(order by a.month), 0) as m2_distance, ifnull(lead(b.duration, 2) over(order by a.month), 0) as m2_duration from cte_month a left join cte_accepted_rides b on b.month = a.month ) select distinct(a.month), round((m0_distance + m1_distance + m2_distance ) / 3, 2) as average_ride_distance, round((m0_duration + m1_duration + m2_duration ) / 3, 2) as average_ride_duration from cte_ride_info a where a.month \u0026lt;= 10 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/hopper-company-queries-iii/","summary":"Description Table: Drivers\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | driver_id | int | | join_date | date | +-------------+---------+ driver_id is the primary key for this table. Each row of this table contains the driver\u0026#39;s ID and the date they joined the Hopper company. Table: Rides\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | ride_id | int | | user_id | int | | requested_at | date | +--------------+---------+ ride_id is the primary key for this table.","title":"[leetcode][Database][Hard] 1651. Hopper Company Queries III"},{"content":"Description Table: Drivers\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | driver_id | int | | join_date | date | +-------------+---------+ driver_id is the primary key for this table. Each row of this table contains the driver\u0026#39;s ID and the date they joined the Hopper company. Table: Rides\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | ride_id | int | | user_id | int | | requested_at | date | +--------------+---------+ ride_id is the primary key for this table. Each row of this table contains the ID of a ride, the user\u0026#39;s ID that requested it, and the day they requested it. There may be some ride requests in this table that were not accepted. Table: AcceptedRides\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | ride_id | int | | driver_id | int | | ride_distance | int | | ride_duration | int | +---------------+---------+ ride_id is the primary key for this table. Each row of this table contains some information about an accepted ride. It is guaranteed that each accepted ride exists in the Rides table. Write an SQL query to report the percentage of working drivers (working_percentage) for each month of 2020 where:\nNote that if the number of available drivers during a month is zero, we consider the working_percentage to be 0.\nReturn the result table ordered by month in ascending order, where month is the month\u0026rsquo;s number (January is 1, February is 2, etc.). Round working_percentage to the nearest 2 decimal places.\nSQL Schema\nCreate table If Not Exists Drivers (driver_id int, join_date date) Create table If Not Exists Rides (ride_id int, user_id int, requested_at date) Create table If Not Exists AcceptedRides (ride_id int, driver_id int, ride_distance int, ride_duration int) Truncate table Drivers insert into Drivers (driver_id, join_date) values (\u0026#39;10\u0026#39;, \u0026#39;2019-12-10\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;8\u0026#39;, \u0026#39;2020-1-13\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;5\u0026#39;, \u0026#39;2020-2-16\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;7\u0026#39;, \u0026#39;2020-3-8\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;4\u0026#39;, \u0026#39;2020-5-17\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;1\u0026#39;, \u0026#39;2020-10-24\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;6\u0026#39;, \u0026#39;2021-1-5\u0026#39;) Truncate table Rides insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;6\u0026#39;, \u0026#39;75\u0026#39;, \u0026#39;2019-12-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;1\u0026#39;, \u0026#39;54\u0026#39;, \u0026#39;2020-2-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;10\u0026#39;, \u0026#39;63\u0026#39;, \u0026#39;2020-3-4\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;19\u0026#39;, \u0026#39;39\u0026#39;, \u0026#39;2020-4-6\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;3\u0026#39;, \u0026#39;41\u0026#39;, \u0026#39;2020-6-3\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;13\u0026#39;, \u0026#39;52\u0026#39;, \u0026#39;2020-6-22\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;7\u0026#39;, \u0026#39;69\u0026#39;, \u0026#39;2020-7-16\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;17\u0026#39;, \u0026#39;70\u0026#39;, \u0026#39;2020-8-25\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;20\u0026#39;, \u0026#39;81\u0026#39;, \u0026#39;2020-11-2\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;5\u0026#39;, \u0026#39;57\u0026#39;, \u0026#39;2020-11-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;2\u0026#39;, \u0026#39;42\u0026#39;, \u0026#39;2020-12-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;11\u0026#39;, \u0026#39;68\u0026#39;, \u0026#39;2021-1-11\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;15\u0026#39;, \u0026#39;32\u0026#39;, \u0026#39;2021-1-17\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;12\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-1-19\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;14\u0026#39;, \u0026#39;18\u0026#39;, \u0026#39;2021-1-27\u0026#39;) Truncate table AcceptedRides insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;10\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;63\u0026#39;, \u0026#39;38\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;13\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;73\u0026#39;, \u0026#39;96\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;100\u0026#39;, \u0026#39;28\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;17\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;119\u0026#39;, \u0026#39;68\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;20\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;121\u0026#39;, \u0026#39;92\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;5\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;42\u0026#39;, \u0026#39;101\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;38\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;11\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;37\u0026#39;, \u0026#39;43\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;15\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;108\u0026#39;, \u0026#39;82\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;12\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;38\u0026#39;, \u0026#39;34\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;14\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;90\u0026#39;, \u0026#39;74\u0026#39;) Idea The query result format is in the following example.\n+-------+--------------------+ | month | working_percentage | +-------+--------------------+ | 1 | 0.00 | | 2 | 0.00 | | 3 | 25.00 | | 4 | 0.00 | | 5 | 0.00 | | 6 | 20.00 | | 7 | 20.00 | | 8 | 20.00 | | 9 | 0.00 | | 10 | 0.00 | | 11 | 33.33 | | 12 | 16.67 | +-------+--------------------+ Fulfill requirements :\nThe thinking process like as [leetcode][Database][Hard] 1635. Hopper Company Queries I , cumulating available drivers by month who join date less than year 2021.\nIt’s only different to counting _drivers that accepted at least one of each month_ in table AcceptedRides and table Rides instead of ride records.\nSolution with cte_month as ( select 1 as month union select 2 as month union select 3 as month union select 4 as month union select 5 as month union select 6 as month union select 7 as month union select 8 as month union select 9 as month union select 10 as month union select 11 as month union select 12 as month ), cte_monthly_drivers as ( select driver_id, if(year(join_date) \u0026lt; 2020, 1, month(join_date)) as month from Drivers where year(join_date) \u0026lt;= 2020 ), cte_monthly_rides as ( select distinct(a.month), count(a.driver_id) over(partition by a.month) as accepted_ride_drivers from ( select distinct(driver_id) as driver_id, month(b.requested_at) as month from AcceptedRides a join Rides b using (ride_id) where year(b.requested_at) = 2020 ) a ) select opt.month, if( opt.available_drivers=0, 0, round((accepted_ride_drivers / opt.available_drivers)*100, 2) ) as working_percentage from ( select distinct(a.month), count(b.driver_id) over(order by a.month) as available_drivers, ifnull(c.accepted_ride_drivers, 0) as accepted_ride_drivers from cte_month a left join cte_monthly_drivers b on b.month = a.month left join cte_monthly_rides c on c.month = a.month ) opt ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/hopper-company-queries-ii/","summary":"Description Table: Drivers\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | driver_id | int | | join_date | date | +-------------+---------+ driver_id is the primary key for this table. Each row of this table contains the driver\u0026#39;s ID and the date they joined the Hopper company. Table: Rides\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | ride_id | int | | user_id | int | | requested_at | date | +--------------+---------+ ride_id is the primary key for this table.","title":"[leetcode][Database][Hard] 1645. Hopper Company Queries II"},{"content":"Description Table: Drivers\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | driver_id | int | | join_date | date | +-------------+---------+ driver_id is the primary key for this table. Each row of this table contains the driver\u0026#39;s ID and the date they joined the Hopper company. Table: Rides\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | ride_id | int | | user_id | int | | requested_at | date | +--------------+---------+ ride_id is the primary key for this table. Each row of this table contains the ID of a ride, the user\u0026#39;s ID that requested it, and the day they requested it. There may be some ride requests in this table that were not accepted. Table: AcceptedRides\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | ride_id | int | | driver_id | int | | ride_distance | int | | ride_duration | int | +---------------+---------+ ride_id is the primary key for this table. Each row of this table contains some information about an accepted ride. It is guaranteed that each accepted ride exists in the Rides table. Write an SQL query to report the following statistics for each month of 2020:\nThe number of drivers currently with the Hopper company by the end of the month (active_drivers). The number of accepted rides in that month (accepted_rides). Return the result table ordered by month in ascending order, where month is the month\u0026rsquo;s number (January is 1, February is 2, etc.).\nSQL Schema\nCreate table If Not Exists Drivers (driver_id int, join_date date) Create table If Not Exists Rides (ride_id int, user_id int, requested_at date) Create table If Not Exists AcceptedRides (ride_id int, driver_id int, ride_distance int, ride_duration int) Truncate table Drivers insert into Drivers (driver_id, join_date) values (\u0026#39;10\u0026#39;, \u0026#39;2019-12-10\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;8\u0026#39;, \u0026#39;2020-1-13\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;5\u0026#39;, \u0026#39;2020-2-16\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;7\u0026#39;, \u0026#39;2020-3-8\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;4\u0026#39;, \u0026#39;2020-5-17\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;1\u0026#39;, \u0026#39;2020-10-24\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;6\u0026#39;, \u0026#39;2021-1-5\u0026#39;) Truncate table Rides insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;6\u0026#39;, \u0026#39;75\u0026#39;, \u0026#39;2019-12-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;1\u0026#39;, \u0026#39;54\u0026#39;, \u0026#39;2020-2-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;10\u0026#39;, \u0026#39;63\u0026#39;, \u0026#39;2020-3-4\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;19\u0026#39;, \u0026#39;39\u0026#39;, \u0026#39;2020-4-6\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;3\u0026#39;, \u0026#39;41\u0026#39;, \u0026#39;2020-6-3\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;13\u0026#39;, \u0026#39;52\u0026#39;, \u0026#39;2020-6-22\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;7\u0026#39;, \u0026#39;69\u0026#39;, \u0026#39;2020-7-16\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;17\u0026#39;, \u0026#39;70\u0026#39;, \u0026#39;2020-8-25\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;20\u0026#39;, \u0026#39;81\u0026#39;, \u0026#39;2020-11-2\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;5\u0026#39;, \u0026#39;57\u0026#39;, \u0026#39;2020-11-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;2\u0026#39;, \u0026#39;42\u0026#39;, \u0026#39;2020-12-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;11\u0026#39;, \u0026#39;68\u0026#39;, \u0026#39;2021-1-11\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;15\u0026#39;, \u0026#39;32\u0026#39;, \u0026#39;2021-1-17\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;12\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-1-19\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;14\u0026#39;, \u0026#39;18\u0026#39;, \u0026#39;2021-1-27\u0026#39;) Truncate table AcceptedRides insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;10\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;63\u0026#39;, \u0026#39;38\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;13\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;73\u0026#39;, \u0026#39;96\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;100\u0026#39;, \u0026#39;28\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;17\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;119\u0026#39;, \u0026#39;68\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;20\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;121\u0026#39;, \u0026#39;92\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;5\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;42\u0026#39;, \u0026#39;101\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;38\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;11\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;37\u0026#39;, \u0026#39;43\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;15\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;108\u0026#39;, \u0026#39;82\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;12\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;38\u0026#39;, \u0026#39;34\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;14\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;90\u0026#39;, \u0026#39;74\u0026#39;) Idea The query result format is in the following example.\n+-------+----------------+----------------+ | month | active_drivers | accepted_rides | +-------+----------------+----------------+ | 1 | 2 | 0 | | 2 | 3 | 0 | | 3 | 4 | 1 | | 4 | 4 | 0 | | 5 | 5 | 0 | | 6 | 5 | 1 | | 7 | 5 | 1 | | 8 | 5 | 1 | | 9 | 5 | 0 | | 10 | 6 | 0 | | 11 | 6 | 2 | | 12 | 6 | 1 | +-------+----------------+----------------+ Fulfill requirements:\nThe output, it apparently show the computed result of each month. So, we can generate a monthly table that month range between 1 (Jan) to 12 (Dec) as a main table in the query statement, then cumulating the active drivers and counting the accepted rides of each month by left join.\nSolution with cte_month as ( select 1 as month union select 2 as month union select 3 as month union select 4 as month union select 5 as month union select 6 as month union select 7 as month union select 8 as month union select 9 as month union select 10 as month union select 11 as month union select 12 as month ), cte_drivers as ( select driver_id, join_date, if(year(join_date)\u0026lt;2020, 2020, year(join_date)) as join_year, if(year(join_date)\u0026lt;2020, 1, month(join_date)) as join_month from Drivers where year(join_date) \u0026lt;= 2020 ), cte_accepted_rides as ( select distinct(month(b.requested_at)) as month, count(ride_id) over(partition by month(b.requested_at)) as accepted_rides from AcceptedRides a join Rides b using(ride_id) where year(b.requested_at)=2020 ) select distinct(a.month) as month, count(b.driver_id) over(order by a.month) as active_drivers, ifnull(c.accepted_rides, 0) as accepted_rides from cte_month a left join cte_drivers b on b.join_month = a.month left join cte_accepted_rides c on c.month = a.month ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/hopper-company-queries-i/","summary":"Description Table: Drivers\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | driver_id | int | | join_date | date | +-------------+---------+ driver_id is the primary key for this table. Each row of this table contains the driver\u0026#39;s ID and the date they joined the Hopper company. Table: Rides\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | ride_id | int | | user_id | int | | requested_at | date | +--------------+---------+ ride_id is the primary key for this table.","title":"[leetcode][Database][Hard] 1635. Hopper Company Queries I"},{"content":"Description Table: Student\n+---------------------+---------+ | Column Name | Type | +---------------------+---------+ | student_id | int | | student_name | varchar | +---------------------+---------+ student_id is the primary key for this table. student_name is the name of the student. Table: Exam\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | exam_id | int | | student_id | int | | score | int | +---------------+---------+ (exam_id, student_id) is the primary key for this table. Each row of this table indicates that the student with student_id had a score points in the exam with id exam_id. A quiet student is the one who took at least one exam and did not score the high or the low score.\nWrite an SQL query to report the students (student_id, student_name) being quiet in all exams. Do not return the student who has never taken any exam.\nReturn the result table ordered by student_id.\nSQL Schema\nCreate table If Not Exists Student (student_id int, student_name varchar(30)) Create table If Not Exists Exam (exam_id int, student_id int, score int) Truncate table Student insert into Student (student_id, student_name) values (\u0026#39;1\u0026#39;, \u0026#39;Daniel\u0026#39;) insert into Student (student_id, student_name) values (\u0026#39;2\u0026#39;, \u0026#39;Jade\u0026#39;) insert into Student (student_id, student_name) values (\u0026#39;3\u0026#39;, \u0026#39;Stella\u0026#39;) insert into Student (student_id, student_name) values (\u0026#39;4\u0026#39;, \u0026#39;Jonathan\u0026#39;) insert into Student (student_id, student_name) values (\u0026#39;5\u0026#39;, \u0026#39;Will\u0026#39;) Truncate table Exam insert into Exam (exam_id, student_id, score) values (\u0026#39;10\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;70\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;10\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;80\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;10\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;90\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;20\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;80\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;30\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;70\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;30\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;80\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;30\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;90\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;40\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;60\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;40\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;70\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;40\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;80\u0026#39;) Idea The query result format is in the following example.\n+-------------+---------------+ | student_id | student_name | +-------------+---------------+ | 2 | Jade | +-------------+---------------+ It has easy way to find quiet students by ranking score of each exam, getting best and worst rank of each exam, marking best and worst for each student of each exam if they are best score or worst score at the exam.\nFinally, listing the students who never got best mark and worst mark of all exam.\nSolution with rank_score_of_exam as ( select a.exam_id, a.student_id, b.student_name, dense_rank() over(partition by a.exam_id order by a.score) as score_rk from exam a left join student b using(student_id) ), rank_of_exam as ( select exam_id, max(score_rk) as high, min(score_rk) as low from rank_score_of_exam group by exam_id ), mark_best as ( select a.exam_id, a.student_id, a.student_name, if(a.score_rk=b.high, 1, 0) as mark from rank_score_of_exam a left join rank_of_exam b using(exam_id) ), mark_worst as ( select a.exam_id, a.student_id, a.student_name, if(a.score_rk=b.low, 1, 0) as mark from rank_score_of_exam a left join rank_of_exam b using(exam_id) ) select student_id, student_name from ( select exam_id, student_id, student_name, mark from mark_best union all select exam_id, student_id, student_name, mark from mark_worst ) opt group by student_id, student_name having sum(mark) = 0 order by student_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/find-the-quiet-students-in-all-exams/","summary":"Description Table: Student\n+---------------------+---------+ | Column Name | Type | +---------------------+---------+ | student_id | int | | student_name | varchar | +---------------------+---------+ student_id is the primary key for this table. student_name is the name of the student. Table: Exam\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | exam_id | int | | student_id | int | | score | int | +---------------+---------+ (exam_id, student_id) is the primary key for this table.","title":"[leetcode][Database][Hard] 1412. Find the Quiet Students in All Exams"},{"content":"Description Table: UserActivity\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | username | varchar | | activity | varchar | | startDate | Date | | endDate | Date | +---------------+---------+ There is no primary key for this table. It may contain duplicates. This table contains information about the activity performed by each user in a period of time. A person with username performed an activity from startDate to endDate. Write an SQL query to show the second most recent activity of each user.\nIf the user only has one activity, return that one. A user cannot perform more than one activity at the same time.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists UserActivity (username varchar(30), activity varchar(30), startDate date, endDate date) Truncate table UserActivity insert into UserActivity (username, activity, startDate, endDate) values (\u0026#39;Alice\u0026#39;, \u0026#39;Travel\u0026#39;, \u0026#39;2020-02-12\u0026#39;, \u0026#39;2020-02-20\u0026#39;) insert into UserActivity (username, activity, startDate, endDate) values (\u0026#39;Alice\u0026#39;, \u0026#39;Dancing\u0026#39;, \u0026#39;2020-02-21\u0026#39;, \u0026#39;2020-02-23\u0026#39;) insert into UserActivity (username, activity, startDate, endDate) values (\u0026#39;Alice\u0026#39;, \u0026#39;Travel\u0026#39;, \u0026#39;2020-02-24\u0026#39;, \u0026#39;2020-02-28\u0026#39;) insert into UserActivity (username, activity, startDate, endDate) values (\u0026#39;Bob\u0026#39;, \u0026#39;Travel\u0026#39;, \u0026#39;2020-02-11\u0026#39;, \u0026#39;2020-02-18\u0026#39;) Idea The query result format is in the following example.\n+------------+--------------+-------------+-------------+ | username | activity | startDate | endDate | +------------+--------------+-------------+-------------+ | Alice | Dancing | 2020-02-21 | 2020-02-23 | | Bob | Travel | 2020-02-11 | 2020-02-18 | +------------+--------------+-------------+-------------+ Fulfill requirements :\nTo generate a rank table order by endDate of each user via function rank()as rn. It will help to find user who has at least twice activity records in table UserActivity.\nName this with clause as rank_end_date. Find _second mostly recent activity_ record of each user from rand_end_date. User will be missing who has only one activity record.\nName this with clause as activity_twice. Find users who has only once activity record of table UserActivity. Using left join activity_twice to rule out users who has been listing in twice_record.\nName this with clause as activity_once . Finally, union the result of activity_twice and twice_record for output of the query. Solution with rank_end_date as ( select rank() over(partition by username order by endDate desc) as rn, username, activity, startDate, endDate from useractivity ), activity_twice as ( select rn, username, activity, startDate, endDate from rank_end_date where rn = 2 ), activity_once as ( select a.rn, a.username, a.activity, a.startDate, a.endDate from rank_end_date a left join activity_twice b using(username) where ifnull(b.rn, 0) = 0 ) select username, activity, startDate, endDate from activity_once union select username, activity, startDate, endDate from activity_twice ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/get-the-second-most-recent-activity/","summary":"Description Table: UserActivity\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | username | varchar | | activity | varchar | | startDate | Date | | endDate | Date | +---------------+---------+ There is no primary key for this table. It may contain duplicates. This table contains information about the activity performed by each user in a period of time. A person with username performed an activity from startDate to endDate.","title":"[leetcode][Database][Hard] 1369. Get the Second Most Recent Activity"},{"content":"Description Table: Visits\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user_id | int | | visit_date | date | +---------------+---------+ (user_id, visit_date) is the primary key for this table. Each row of this table indicates that user_id has visited the bank in visit_date. Table: Transactions\n+------------------+---------+ | Column Name | Type | +------------------+---------+ | user_id | int | | transaction_date | date | | amount | int | +------------------+---------+ There is no primary key for this table, it may contain duplicates. Each row of this table indicates that user_id has done a transaction of amount in transaction_date. It is guaranteed that the user has visited the bank in the transaction_date.(i.e The Visits table contains (user_id, transaction_date) in one row) A bank wants to draw a chart of the number of transactions bank visitors did in one visit to the bank and the corresponding number of visitors who have done this number of transaction in one visit.\nWrite an SQL query to find how many users visited the bank and didn’t do any transactions, how many visited the bank and did one transaction and so on.\nThe result table will contain two columns:\ntransactions_count which is the number of transactions done in one visit. visits_count which is the corresponding number of users who did transactions_count in one visit to the bank. transactions_count should take all values from 0 to max(transactions_count) done by one or more users.\nReturn the result table ordered by transactions_count.\nSQL Schema\nCreate table If Not Exists Visits (user_id int, visit_date date) Create table If Not Exists Transactions (user_id int, transaction_date date, amount int) Truncate table Visits insert into Visits (user_id, visit_date) values (\u0026#39;1\u0026#39;, \u0026#39;2020-01-01\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;2\u0026#39;, \u0026#39;2020-01-02\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;12\u0026#39;, \u0026#39;2020-01-01\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;19\u0026#39;, \u0026#39;2020-01-03\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;1\u0026#39;, \u0026#39;2020-01-02\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;2\u0026#39;, \u0026#39;2020-01-03\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;1\u0026#39;, \u0026#39;2020-01-04\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;7\u0026#39;, \u0026#39;2020-01-11\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;9\u0026#39;, \u0026#39;2020-01-25\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;8\u0026#39;, \u0026#39;2020-01-28\u0026#39;) Truncate table Transactions insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;1\u0026#39;, \u0026#39;2020-01-02\u0026#39;, \u0026#39;120\u0026#39;) insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;2\u0026#39;, \u0026#39;2020-01-03\u0026#39;, \u0026#39;22\u0026#39;) insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;7\u0026#39;, \u0026#39;2020-01-11\u0026#39;, \u0026#39;232\u0026#39;) insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;1\u0026#39;, \u0026#39;2020-01-04\u0026#39;, \u0026#39;7\u0026#39;) insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;9\u0026#39;, \u0026#39;2020-01-25\u0026#39;, \u0026#39;33\u0026#39;) insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;9\u0026#39;, \u0026#39;2020-01-25\u0026#39;, \u0026#39;66\u0026#39;) insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;8\u0026#39;, \u0026#39;2020-01-28\u0026#39;, \u0026#39;1\u0026#39;) insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;9\u0026#39;, \u0026#39;2020-01-25\u0026#39;, \u0026#39;99\u0026#39;) Idea The output requires result table contains 2 columns : transactions_count and visits_count , but also the column transactions_count range between 0 to max(transactions_count). Finally, output the result table ordered by transactions_count.\nfor example:\n+--------------------+--------------+ | transactions_count | visits_count | +--------------------+--------------+ | 0 | 4 | | 1 | 5 | | 2 | 0 | | 3 | 1 | +--------------------+--------------+ Fultill requirements:\nGenerate a serial number table start with 0 by function row_number() of transactions table due to max(transactions_count) should be less tahn or equals to the number of records of transactions table.\nName this with clause as sn . Compute the transactions_count from records of transactions table, where the records matches the user_id from visits table, but also the transaction_date in visit_date. Name this with clause as . Joint sn and cte_1, where row_number() of sn should be less than or equals tomax(cte_1.transactions_count) . It will help to confirm all of sn.rn rows where thesn.rn less than or equals tothan max(cte_1.transactions_count) should be in result table, because the cte_1 will not include row if the visits_count are zero times.\nName this with clause as cte_2. Finally, named column visits_count for counting how many users group by cte_2.transactions_count , and output it. Solution with sn as ( select row_number() over() -1 rn from transactions ), cte_1 as ( select a.user_id, a.visit_date, count(b.transaction_date)as transactions_count from visits a left join transactions b on b.user_id = a.user_id and b.transaction_date = a.visit_date group by a.user_id, a.visit_date ), cte_2 as ( select user_id, visit_date, transactions_count from cte_1 union all select null, null, rn from sn where rn \u0026lt; (select max(transactions_count) from cte_1) ) select transactions_count, count(user_id) as visits_count from cte_2 group by transactions_count order by transactions_count ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/number-of-transactions-per-visit/","summary":"Description Table: Visits\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user_id | int | | visit_date | date | +---------------+---------+ (user_id, visit_date) is the primary key for this table. Each row of this table indicates that user_id has visited the bank in visit_date. Table: Transactions\n+------------------+---------+ | Column Name | Type | +------------------+---------+ | user_id | int | | transaction_date | date | | amount | int | +------------------+---------+ There is no primary key for this table, it may contain duplicates.","title":"[leetcode][Database][Hard] 1336. Number of Transactions per Visit"},{"content":"記錄在過去面試中常被問到的問題，以及依據自己的理解與查找到的資料，整理對應的回覆\nQ1. K8s 有哪些 components ?\nMaster Node ← 管理 cluster、儲存不同 node 的資訊、規劃 containers 的去向，以及 monitor node 上的 containers\nkube-apiserver ← 管理整個 K8s 的 interface。使用者可以透過下達指令給 kube-apiserver ，以達到管理 K8s resources 的目的。 etcd cluster ← 儲存 K8s cluster 內，所有 node 、 containers 的資訊 kube-scheduler ← 依據 containers 的需求，包含但不限於: cpu、 menory、 affinity 、 taints and tolerations 、anti-* 等等，規劃將 containers 指派給符合需求的 node。 controller manager ← 管控 K8s cluster 內的 resources 。\n如 node controller 管理 nodes : 加入新 node 到 cluster、處理 node 變的不可用的狀況 Work Nodes ← 託管執行應用的 containers\nkubelet ← K8s cluster 在 work nodes 上的代理，偵聽 kube-apiserver 的指令，並依據需求部署或銷毀 containers。 kube-apiserver 週期性的從kubelet 獲取 work node 和 containers 的狀態。 kube-proxy ← 讓 work node 之間的 containers 可以互相溝通。 container runtime engine ← 執行包含應用的 containers Q2. 部署一個應用到 K8s 時，K8s 會如何運作整個流程 ?\nUser 發送 deploy request 給 kube-apiserver。kube-apiserver 驗證 User 並驗證是否為有效的 request。 將有效的 request 更新到 ETCD。 ETCD 回覆 kube-apiserver 更新已完成。 kube-scheduler 會持續監測 kube-apiserver，此時發現有一個新的 pod 還沒有被指派到 work node， kube-scheduler 會選擇符合條件的 work node。 kube-scheduler 通知 kube-apiserver ，規畫將新的 pod 分配到指定的 work node。 kube-apiserver 通知 work node 上的 kubelet，有新的 pod 需要部署到 work node 上。 kubelet 嘗試將新的 pod 部署到 work node 上，並持續監測 pod 的狀態。 新的 pod 開始部署到 work node 。 新的 pod 部署完成。 kubelet 監測到 pod 已部署 (但不保證 pod 上的應用執行是否成功)。 kubelet 通知 kube-apiserver 新的 pod 已部署到 work node。 kube-apiserver 將 pod 部署資訊更新到 ETCD 。 ETCD 回覆 kube-apiserver 更新已完成。 kube-apiserver 回覆 User 新的 pod 已部署到 work node。 Q3. 對於在 K8s 上建置正式環境 (Product) 和測試環境 (Development)的規劃\n在 K8s 分別建立 Product 和 Development 的 namespace，並可透過指令 kubectl apply -f {YAML file} -n [ Product | Development ] ，將需要的 resources 部署到對應的 namespace 。 若需要權限管理，則透過 RBAC 進行設置。我常見的作法是先建立 K8s 的 service account 、建立 Role 或 ClusterRole 並設定可操作的 resources 與對應的 verbs，最後透過 RoleBinding 或 ClusterRoleBinding 將 service account 和 Role(或ClusterRole)進行綁定。 ","permalink":"https://blog.zhengweiliu.com/posts/normal/kubernetes/","summary":"記錄在過去面試中常被問到的問題，以及依據自己的理解與查找到的資料，整理對應的回覆\nQ1. K8s 有哪些 components ?\nQ2. 部署一個應用到 K8s 時，K8s 會如何運作整個流程 ?\nQ3. 對於在 K8s 上建置正式環境 (Product) 和測試環境 (Development)的規劃","title":"整理面試常見問題 — Kubernetes"},{"content":"題目 Table: Failed\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | fail_date | date | +--------------+---------+ fail_date is the primary key for this table. This table contains the days of failed tasks. Table: Succeeded\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | success_date | date | +--------------+---------+ success_date is the primary key for this table. This table contains the days of succeeded tasks. A system is running one task every day. Every task is independent of the previous tasks. The tasks can fail or succeed.\nWrite an SQL query to generate a report of period_state for each continuous interval of days in the period from 2019-01-01 to 2019-12-31.\nperiod_state is \u0026rsquo;failed' if tasks in this interval failed or 'succeeded' if tasks in this interval succeeded. Interval of days are retrieved as start_date and end_date.\nReturn the result table ordered by start_date.\nSQL Schema\nCreate table If Not Exists Failed (fail_date date) Create table If Not Exists Succeeded (success_date date) Truncate table Failed insert into Failed (fail_date) values (\u0026#39;2018-12-28\u0026#39;) insert into Failed (fail_date) values (\u0026#39;2018-12-29\u0026#39;) insert into Failed (fail_date) values (\u0026#39;2019-01-04\u0026#39;) insert into Failed (fail_date) values (\u0026#39;2019-01-05\u0026#39;) Truncate table Succeeded insert into Succeeded (success_date) values (\u0026#39;2018-12-30\u0026#39;) insert into Succeeded (success_date) values (\u0026#39;2018-12-31\u0026#39;) insert into Succeeded (success_date) values (\u0026#39;2019-01-01\u0026#39;) insert into Succeeded (success_date) values (\u0026#39;2019-01-02\u0026#39;) insert into Succeeded (success_date) values (\u0026#39;2019-01-03\u0026#39;) insert into Succeeded (success_date) values (\u0026#39;2019-01-06\u0026#39;) 解題思考 (Variable 版本) 題目要求輸出 period_state 、 start_date 和 end_date 欄位的表格。\n其中將連續日期都為相同 state 的定義為一個 period，且 start_date 和 end_date 將表示該 period 的起始日期和結束日期。\n+--------------+--------------+--------------+ | period_state | start_date | end_date | +--------------+--------------+--------------+ | succeeded | 2019-01-01 | 2019-01-03 | | failed | 2019-01-04 | 2019-01-05 | | succeeded | 2019-01-06 | 2019-01-06 | +--------------+--------------+--------------+ 最開始，我的想法是利用 variable，在每次 period_state 變換時，透過變數將該 period_state第一個日期作為錨點 anchor ，再使用 rank() 對 start_date 做升序排序 rn。 透過 with clause 建立 status 表格，聯合 Failed 表格和 Successed 表格，篩選出 fail_date 、 success_date 介於 2019–01–01 至 2019-12-31 的資料集。 透過 with clause 建立 date_anchor 表格，擴增 anchor_date欄位以便後續找出 end_date 。\nvariable current_state ← 儲存當前 record 的 period_state\nvariable date_anchor ← 儲存當前 period 的第一個日期\n透過子查詢初始化 variable current_state 和 variable date_anchor 查詢 date_anchor 以建立子查詢表格 a 。\n子查詢表格中，利用 rank() 對 start_date 做升序，以標記排序結果 rn 。並利用 max(start_date) over(partition by anchor_date) 取得每個 period 中最大的 start_date ，即該 period 的 end_date 。 查詢 a 作為主要表格，利用 group by end_date 和 having min(rn) 以篩選出符合輸出條件的資料。 解決方案 (Variable 版本) with status as ( select period_state, start_date from ( select fail_date as start_date, \u0026#39;failed\u0026#39; as period_state from Failed union all select success_date as start_date, \u0026#39;succeeded\u0026#39; as period_state from Succeeded ) tmp where start_date between \u0026#39;2019-01-01\u0026#39; and \u0026#39;2019-12-31\u0026#39; order by start_date ), date_anchor as ( select period_state, start_date, anchor_date from( select a.period_state, a.start_date, if(@current_state=a.period_state, @date_anchor, a.start_date) as anchor_date, if(@current_state=a.period_state, @date_anchor, @date_anchor:=a.start_date), @current_state:=a.period_state from status a, (select @current_state:=\u0026#34;initial\u0026#34;, @date_anchor:=\u0026#34;2018-12-31\u0026#34;) init ) anchor ) select a.period_state, a.start_date, a.end_date from ( select period_state, start_date, max(start_date) over(partition by anchor_date) as end_date, rank() over(order by start_date) rn from date_anchor ) a group by end_date having min(rn) order by start_date 解題思考 (Rank版本) 在提交了 variable 版本後，發現連續日期在排序後，其 rn 等差為 1 ，這表示每次 period_state 變換時，取第一個日期作為該 period 的最小值，則該 period 中所有資料都應符合 start_date-rn=min(start_date) 的條件，因此又寫了一個 Rank 版本。 透過 with clause 分別建立 failed_state 、 successeded_state表格。\n篩選資料時間介於 2019–01–01 至 2019-12-31 的資料集，\n擴增 period_state 欄位，\n利用 rank() over(order by ) 標註排序結果 rn 。 做子查詢表格 opt，聯合 failed_state 和 successeded_state 。\n在 failed_state 表格，以 group by period_state, date_add(fail_date, interval -rn day) ，分別找出failed_state 和 successeded_state，每個 period 的 min(date) 和 max(date) ，作為輸出結果的 start_date 和 end_date 欄位。 解決方案 (Rank版本) with failed_state as ( select \u0026#39;failed\u0026#39; as period_state, fail_date, rank() over(order by fail_date) -1 as rn from Failed where fail_date between \u0026#39;2019-01-01\u0026#39; and \u0026#39;2019-12-31\u0026#39; ), succeeded_state as ( select \u0026#39;succeeded\u0026#39; as period_state, success_date, rank() over(order by success_date) -1 as rn from Succeeded where success_date between \u0026#39;2019-01-01\u0026#39; and \u0026#39;2019-12-31\u0026#39; ) select period_state, start_date, end_date from ( select period_state, min(fail_date) as start_date, max(fail_date) as end_date from failed_state group by period_state, date_add(fail_date, interval -rn day) union select period_state, min(success_date) as start_date, max(success_date) as end_date from succeeded_state group by period_state, date_add(success_date, interval -rn day) ) opt ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/report-contiguous-dates/","summary":"題目 Table: Failed +--------------+---------+ | Column Name | Type | +--------------+---------+ | fail_date | date | +--------------+---------+ fail_date is the primary key for this table. This table contains the days of failed tasks. Table: Succeeded +--------------+---------+ | Column Name | Type | +--------------+---------+ | success_date | date | +--------------+---------+ success_date is the primary key for this table. This table contains the days of succeeded tasks. A system is running","title":"[leetcode][Database][Hard] 1225. Report Contiguous Dates"},{"content":"題目\nTable: Employee\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | id | int | | name | varchar | | salary | int | | departmentId | int | +--------------+---------+ id is the primary key column for this table. departmentId is a foreign key of the ID from the Department table. Each row of this table indicates the ID, name, and salary of an employee. It also contains the ID of their department. Table: Department\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | id | int | | name | varchar | +-------------+---------+ id is the primary key column for this table. Each row of this table indicates the ID of a department and its name. A company’s executives are interested in seeing who earns the most money in each of the company’s departments. A high earner in a department is an employee who has a salary in the top three unique salaries for that department.\nWrite an SQL query to find the employees who are high earners in each of the departments.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Employee (id int, name varchar(255), salary int, departmentId int) Create table If Not Exists Department (id int, name varchar(255)) Truncate table Employee insert into Employee (id, name, salary, departmentId) values (\u0026#39;1\u0026#39;, \u0026#39;Joe\u0026#39;, \u0026#39;85000\u0026#39;, \u0026#39;1\u0026#39;) insert into Employee (id, name, salary, departmentId) values (\u0026#39;2\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;80000\u0026#39;, \u0026#39;2\u0026#39;) insert into Employee (id, name, salary, departmentId) values (\u0026#39;3\u0026#39;, \u0026#39;Sam\u0026#39;, \u0026#39;60000\u0026#39;, \u0026#39;2\u0026#39;) insert into Employee (id, name, salary, departmentId) values (\u0026#39;4\u0026#39;, \u0026#39;Max\u0026#39;, \u0026#39;90000\u0026#39;, \u0026#39;1\u0026#39;) insert into Employee (id, name, salary, departmentId) values (\u0026#39;5\u0026#39;, \u0026#39;Janet\u0026#39;, \u0026#39;69000\u0026#39;, \u0026#39;1\u0026#39;) insert into Employee (id, name, salary, departmentId) values (\u0026#39;6\u0026#39;, \u0026#39;Randy\u0026#39;, \u0026#39;85000\u0026#39;, \u0026#39;1\u0026#39;) insert into Employee (id, name, salary, departmentId) values (\u0026#39;7\u0026#39;, \u0026#39;Will\u0026#39;, \u0026#39;70000\u0026#39;, \u0026#39;1\u0026#39;) Truncate table Department insert into Department (id, name) values (\u0026#39;1\u0026#39;, \u0026#39;IT\u0026#39;) insert into Department (id, name) values (\u0026#39;2\u0026#39;, \u0026#39;Sales\u0026#39;) 解題思考\n題目要求輸出每個部門中，收入排名前三高的員工。\n若有收入相同且位於收入排名前三高的員工，也併入輸出結果。 +------------+----------+--------+ | Department | Employee | Salary | +------------+----------+--------+ | IT | Max | 90000 | | IT | Joe | 85000 | | IT | Randy | 85000 | | IT | Will | 70000 | | Sales | Henry | 80000 | | Sales | Sam | 60000 | +------------+----------+--------+ 透過 with clause 建立 employee_info 表格，以提供每個部門內，每位員工的薪水排序結果。\n使用 dense_rank() 函式進行排序，dense_rank() 會以連續數字的方式給予排序結果 rn。如下情況 +------------+----------+--------+--------------+ | Department | Employee | Salary | dense_rank() | +------------+----------+--------+--------------+ | IT | Max | 90000 | 1 | | IT | Joe | 85000 | 2 | | IT | Randy | 85000 | 2 | | IT | Will | 70000 | 3 | | Sales | Henry | 80000 | 1 | | Sales | Sam | 60000 | 2 | +------------+----------+--------+--------------+ 查詢 employee_info 作為主要表格，過濾條件以找出 employee_info.rn \u0026lt; 4 的 record set，即找出每個部門，收入排名前三高的員工。 解決方案\nwith employee_info as ( select Department.id as departmentId , Department.name as Department , Employee.name as Employee, Employee.Salary as Salary, dense_rank() over(partition by Department.id order by Employee.Salary desc ) rn from Employee join Department on Department.id = Employee.departmentId ) select employee_info.Department, employee_info.Employee, employee_info.Salary from employee_info where employee_info.rn \u0026lt; 4 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/department-top-three-salaries/","summary":"題目 Table: Employee +--------------+---------+ | Column Name | Type | +--------------+---------+ | id | int | | name | varchar | | salary | int | | departmentId | int | +--------------+---------+ id is the primary key column for this table. departmentId is a foreign key of the ID from the Department table. Each row of this table indicates the ID, name, and salary of an employee. It also","title":"[leetcode][Database][Hard] 185. Department Top Three Salaries"},{"content":"題目 Table: Trips\n+-------------+----------+ | Column Name | Type | +-------------+----------+ | id | int | | client_id | int | | driver_id | int | | city_id | int | | status | enum | | request_at | date | +-------------+----------+ id is the primary key for this table. The table holds all taxi trips. Each trip has a unique id, while client_id and driver_id are foreign keys to the users_id at the Users table. Status is an ENUM type of (\u0026#39;completed\u0026#39;, \u0026#39;cancelled_by_driver\u0026#39;, \u0026#39;cancelled_by_client\u0026#39;). Table: Users\n+-------------+----------+ | Column Name | Type | +-------------+----------+ | users_id | int | | banned | enum | | role | enum | +-------------+----------+ users_id is the primary key for this table. The table holds all users. Each user has a unique users_id, and role is an ENUM type of (\u0026#39;client\u0026#39;, \u0026#39;driver\u0026#39;, \u0026#39;partner\u0026#39;). banned is an ENUM type of (\u0026#39;Yes\u0026#39;, \u0026#39;No\u0026#39;). The cancellation rate is computed by dividing the number of canceled (by client or driver) requests with unbanned users by the total number of requests with unbanned users on that day.\nWrite a SQL query to find the cancellation rate of requests with unbanned users (both client and driver must not be banned) each day between \u0026quot;2013-10-01\u0026quot; and \u0026quot;2013-10-03\u0026quot;. Round Cancellation Rate to two decimal points.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Trips (id int, client_id int, driver_id int, city_id int, status ENUM(\u0026#39;completed\u0026#39;, \u0026#39;cancelled_by_driver\u0026#39;, \u0026#39;cancelled_by_client\u0026#39;), request_at varchar(50)) Create table If Not Exists Users (users_id int, banned varchar(50), role ENUM(\u0026#39;client\u0026#39;, \u0026#39;driver\u0026#39;, \u0026#39;partner\u0026#39;)) Truncate table Trips insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;2013-10-01\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;cancelled_by_driver\u0026#39;, \u0026#39;2013-10-01\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;3\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;2013-10-01\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;4\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;13\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;cancelled_by_client\u0026#39;, \u0026#39;2013-10-01\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;5\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;2013-10-02\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;6\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;2013-10-02\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;7\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;2013-10-02\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;8\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;2013-10-03\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;9\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;2013-10-03\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;10\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;13\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;cancelled_by_driver\u0026#39;, \u0026#39;2013-10-03\u0026#39;) Truncate table Users insert into Users (users_id, banned, role) values (\u0026#39;1\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;client\u0026#39;) insert into Users (users_id, banned, role) values (\u0026#39;2\u0026#39;, \u0026#39;Yes\u0026#39;, \u0026#39;client\u0026#39;) insert into Users (users_id, banned, role) values (\u0026#39;3\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;client\u0026#39;) insert into Users (users_id, banned, role) values (\u0026#39;4\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;client\u0026#39;) insert into Users (users_id, banned, role) values (\u0026#39;10\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;driver\u0026#39;) insert into Users (users_id, banned, role) values (\u0026#39;11\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;driver\u0026#39;) insert into Users (users_id, banned, role) values (\u0026#39;12\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;driver\u0026#39;) insert into Users (users_id, banned, role) values (\u0026#39;13\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;driver\u0026#39;) 解題思考 題目要求輸出 2013-10-01 至 2013-10-03 ，每日的搭乘請求取消率，且被納入計算的有效搭乘請求必須是未停權乘客 和 未停權駕駛，若有一方被停權，則該筆搭乘請求視為無效資料。\n搭乘請求取消率 ← 該日有效搭乘請求且該請求被拒絕的資料數 / 該日總有效搭乘請求 +------------+-------------------+ | Day | Cancellation Rate | +------------+-------------------+ | 2013-10-01 | 0.33 | | 2013-10-02 | 0.00 | | 2013-10-03 | 0.50 | +------------+-------------------+ 透過 with clause 分別建立未停權乘客 legal_client、未停權駕駛 legal_driver，以及2013-10-01 至 2013-10-03 的搭乘資料 legal_trips。 查詢 legal_trips 作為主要表格，關聯legal_client 和 legal_driver ，以計算每日的 搭乘請求取消率 。 解決方案 with legal_client as ( select * from Users where role = \u0026#39;client\u0026#39; and banned = \u0026#39;No\u0026#39;), legal_driver as ( select * from Users where role = \u0026#39;driver\u0026#39; and banned = \u0026#39;No\u0026#39;), legal_trips as (select * from Trips where request_at between \u0026#39;2013-10-01\u0026#39; and \u0026#39;2013-10-03\u0026#39; ) select legal_trips.request_at as Day, round( count(if(legal_trips.status=\u0026#39;completed\u0026#39;, NULL, legal_trips.id)) / count(legal_trips.id), 2 ) as \u0026#39;Cancellation Rate\u0026#39; from legal_trips join legal_client on legal_client.users_id = legal_trips.client_id join legal_driver on legal_driver.users_id = legal_trips.driver_id group by legal_trips.request_at ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/trips-and-users/","summary":"題目 Table: Trips +-------------+----------+ | Column Name | Type | +-------------+----------+ | id | int | | client_id | int | | driver_id | int | | city_id | int | | status | enum | | request_at | date | +-------------+----------+ id is the primary key for this table. The table holds all taxi trips. Each trip has a unique id, while client_id and driver_id are foreign keys","title":"[leetcode][Database][Hard] 262. Trips and Users"},{"content":"題目\nTable: Employee\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | id | int | | company | varchar | | salary | int | +--------------+---------+ id is the primary key column for this table. Each row of this table indicates the company and the salary of one employee. Write an SQL query to find the rows that contain the median salary of each company. While calculating the median, when you sort the salaries of the company, break the ties by id.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Employee (id int, company varchar(255), salary int) Truncate table Employee insert into Employee (id, company, salary) values (\u0026#39;1\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;2341\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;2\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;341\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;3\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;15\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;4\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;15314\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;5\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;451\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;6\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;513\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;7\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;15\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;8\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;13\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;9\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;1154\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;10\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;1345\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;11\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;1221\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;12\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;234\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;13\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;2345\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;14\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;2645\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;15\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;2645\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;16\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;2652\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;17\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;65\u0026#39;) 解題思考\n題目要求計算並且輸出每間公司的薪資中位數。 +----+---------+--------+ | id | company | salary | +----+---------+--------+ | 5 | A | 451 | | 6 | A | 513 | | 12 | B | 234 | | 9 | B | 1154 | | 14 | C | 2645 | +----+---------+--------+ 做子表 a查詢，每間公司的員工總數 cnt ，並利用 row_numbner()依據每間公司員工薪水 salary 升序，標註排序結果 row_num。 利用子表 a 作為主要表格，過濾row_num以找出介於 cnt div 2和 (cnt div 2)+1 的 record set，即每間公司的新增中位數。 解決方案\nselect a.id, a.company, a.salary from ( select *, count(id) over(PARTITION by company) as cnt, row_number() over(PARTITION by company order by salary) as row_num from employee ) a where a.row_num between a.cnt div 2 and (a.cnt div 2)+1 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/median-employee-salary/","summary":"題目 Table: Employee +--------------+---------+ | Column Name | Type | +--------------+---------+ | id | int | | company | varchar | | salary | int | +--------------+---------+ id is the primary key column for this table. Each row of this table indicates the company and the salary of one employee. Write an SQL query to find the rows that contain the median salary of each company. While calculating the","title":"[leetcode][Database][Hard] 569. Median Employee Salary"},{"content":"題目 Table: Numbers\n+-------------+------+ | Column Name | Type | +-------------+------+ | num | int | | frequency | int | +-------------+------+ num is the primary key for this table. Each row of this table shows the frequency of a number in the database. The median is the value separating the higher half from the lower half of a data sample.\nWrite an SQL query to report the median of all the numbers in the database after decompressing the Numbers table. Round the median to one decimal point.\nSQL Schema\nCreate table If Not Exists Numbers (num int, frequency int) Truncate table Numbers insert into Numbers (num, frequency) values (\u0026#39;0\u0026#39;, \u0026#39;7\u0026#39;) insert into Numbers (num, frequency) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;) insert into Numbers (num, frequency) values (\u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;) insert into Numbers (num, frequency) values (\u0026#39;3\u0026#39;, \u0026#39;1\u0026#39;) 解題思考 題目要求輸出 Numbers 表格的中位數 median。\n本題要求的邏輯為 : 1. 先將 Numbers 內的數字 num 以頻次 frequency 展開為等長的數列\n2. L ← 將所有 num 展開的數列依 num 大小有序串接 3. 求出 L 的中位數 median，並將中位數作為輸出結果 Input: Numbers table: +-----+-----------+ | num | frequency | +-----+-----------+ | 0 | 7 | | 1 | 1 | | 2 | 3 | | 3 | 1 | +-----+-----------+ Output: +--------+ | median | +--------+ | 0.0 | +--------+ Explanation: If we decompress the Numbers table, we will get [0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 3], so the median is (0 + 0) / 2 = 0. 中位數的定義 → 在一組數列 _S_ 中，有一半數字個數會小於中位數，另外一半數字個數會大於中位數。假設\n_L_ ← 數列 _S_ 中，小於等於 Numbers.num 的數字個數\n_R_ ← 數列 _S_ 中，大於等於 Numbers.num 的數字個數\n_N_ ← 數列 _S_ 中，Numbers.num 的數字個數\n則數列 _S_ 的數字個數應等同於 _L + N + R_ ， 對 Numbers 中的所有 num ，該條件都成立 考慮當 _L_ ≠ _R_ 的情況，若Numbers.num是中位數，當 _N_ 加入至個數較少的短邊時 ( _L_ 或 _R_ ) ，被 _N_ 加入的短邊會成為長邊:\n當 _L \u0026lt; R_，_L + N \u0026gt; R _當 _R \u0026lt; L_，_R + N \u0026gt; L _若Numbers.num不是中位數，當 _L \u0026lt; R_ 時，即使把 _N_ 放到短邊，仍會得到 _L + N \u0026lt; R_ 的結果，這表示 _N_ 不在數列中間 考慮當 _L = R_ 的情況，根據定義已知 Numbers.num是中位數 解決方案 select round(avg(n.num),1) median from Numbers n where n.Frequency \u0026gt;= abs( (select sum(Frequency) from Numbers where num\u0026lt;=n.num) - (select sum(Frequency) from Numbers where num\u0026gt;=n.num) ) ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/find-median-given-frequency-of-numbers/","summary":"題目 Table: Numbers +-------------+------+ | Column Name | Type | +-------------+------+ | num | int | | frequency | int | +-------------+------+ num is the primary key for this table. Each row of this table shows the frequency of a number in the database. The median is the value separating the higher half from the lower half of a data sample. Write an SQL query to report the median","title":"[leetcode][Database][Hard] 571. Find Median Given Frequency of Numbers"},{"content":"題目 Table: Employee\n+-------------+------+ | Column Name | Type | +-------------+------+ | id | int | | month | int | | salary | int | +-------------+------+ (id, month) is the primary key for this table. Each row in the table indicates the salary of an employee in one month during the year 2020. Write an SQL query to calculate the cumulative salary summary for every employee in a single unified table.\nThe cumulative salary summary for an employee can be calculated as follows:\nFor each month that the employee worked, sum up the salaries in that month and the previous two months. This is their 3-month sum for that month. If an employee did not work for the company in previous months, their effective salary for those months is 0. Do not include the 3-month sum for the most recent month that the employee worked for in the summary. Do not include the 3-month sum for any month the employee did not work. Return the result table ordered by id in ascending order. In case of a tie, order it by month in descending order.\nSQL Schema\nCreate table If Not Exists Employee (id int, month int, salary int) Truncate table Employee insert into Employee (id, month, salary) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;20\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;20\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;30\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;30\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;3\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;40\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;1\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;40\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;3\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;60\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;1\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;60\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;70\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;1\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;90\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;1\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;90\u0026#39;) 解題思考 題目要求輸出每位員工在每個月分的累計薪資，並且過濾每個員工最後一次薪資紀錄的月份。\n定義 累計薪資 → 該月份薪資與前兩個月的加總 +----+-------+--------+ | id | month | Salary | +----+-------+--------+ | 1 | 7 | 90 | | 1 | 4 | 130 | | 1 | 3 | 90 | | 1 | 2 | 50 | | 1 | 1 | 20 | | 2 | 1 | 20 | | 3 | 3 | 100 | | 3 | 2 | 40 | +----+-------+--------+ 透過 with clause 建立 month_dense_rank 表格，以便找出每位員工需要過濾有薪資紀錄的最後一個月份。\n利用 dense_rank() 函式，依據員工編號 id 與紀錄月份 month 降序，標註每位員工，每個月分薪資紀錄的排序結果。 透過 with clause 建立 cumulate_month_salary 表格，統計每位員工，每個月份的累積薪資。\n利用子查詢關聯外部主表 Employee，並過濾出符合子查詢 id=Employee.id 以及子查詢薪資紀錄月份 month 落在主表 Employee.month-2 和 Employee.month 之間的 record set，依據該 record set 的員工編號 id 對該 record set的薪資 salary 進行加總。 利用子表查詢 month_dense_rank 過濾 month_dense_rank.r_month \u0026gt; 1 的 record set 作為主要表格，並關聯 cumulate_month_salary 表格，最後帶出符合題目要求的輸出結果 員工編號id 、 薪資月份month 、 累積薪資salary 。 解決方案 with month_dense_rnak as ( select id, month, dense_rank() over(partition by id order by month desc) as r_month from Employee ), cumulate_month_salary as ( select id, month, ( select sum(salary) from Employee where id=e.id and month between e.month-2 and e.month group by id ) as salary from Employee e ) select a.id, a.month, b.salary from ( select id, month from month_dense_rnak where r_month \u0026gt; 1 ) a join cumulate_month_salary b on b.id = a.id and b.month = a.month ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/find-cumulative-salary-of-an-employee/","summary":"題目 Table: Employee +-------------+------+ | Column Name | Type | +-------------+------+ | id | int | | month | int | | salary | int | +-------------+------+ (id, month) is the primary key for this table. Each row in the table indicates the salary of an employee in one month during the year 2020. Write an SQL query to calculate the cumulative salary summary for every employee in a","title":"[leetcode][Database][Hard] 579. Find Cumulative Salary of an Employee"},{"content":"題目 Table: Stadium\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | id | int | | visit_date | date | | people | int | +---------------+---------+ visit_date is the primary key for this table. Each row of this table contains the visit date and visit id to the stadium with the number of people during the visit. No two rows will have the same visit_date, and as the id increases, the dates increase as well. Write an SQL query to display the records with three or more rows with consecutive id\u0026rsquo;s, and the number of people is greater than or equal to 100 for each.\nReturn the result table ordered by visit_date in ascending order.\nSQL Schema\nCreate table If Not Exists Stadium (id int, visit_date DATE NULL, people int) Truncate table Stadium insert into Stadium (id, visit_date, people) values (\u0026#39;1\u0026#39;, \u0026#39;2017-01-01\u0026#39;, \u0026#39;10\u0026#39;) insert into Stadium (id, visit_date, people) values (\u0026#39;2\u0026#39;, \u0026#39;2017-01-02\u0026#39;, \u0026#39;109\u0026#39;) insert into Stadium (id, visit_date, people) values (\u0026#39;3\u0026#39;, \u0026#39;2017-01-03\u0026#39;, \u0026#39;150\u0026#39;) insert into Stadium (id, visit_date, people) values (\u0026#39;4\u0026#39;, \u0026#39;2017-01-04\u0026#39;, \u0026#39;99\u0026#39;) insert into Stadium (id, visit_date, people) values (\u0026#39;5\u0026#39;, \u0026#39;2017-01-05\u0026#39;, \u0026#39;145\u0026#39;) insert into Stadium (id, visit_date, people) values (\u0026#39;6\u0026#39;, \u0026#39;2017-01-06\u0026#39;, \u0026#39;1455\u0026#39;) insert into Stadium (id, visit_date, people) values (\u0026#39;7\u0026#39;, \u0026#39;2017-01-07\u0026#39;, \u0026#39;199\u0026#39;) insert into Stadium (id, visit_date, people) values (\u0026#39;8\u0026#39;, \u0026#39;2017-01-09\u0026#39;, \u0026#39;188\u0026#39;) 解題思考 題目要求輸出連續三日以上，拜訪人數達到 100 以上 的日期與人數。 +------+------------+-----------+ | id | visit_date | people | +------+------------+-----------+ | 5 | 2017-01-05 | 145 | | 6 | 2017-01-06 | 1455 | | 7 | 2017-01-07 | 199 | | 8 | 2017-01-09 | 188 | +------+------------+-----------+ 透過 with clause 建立 visit_thresgold 表格，利用 lead()函示擴增後兩日瀏覽人數欄位 next_1、next_2，利用lag() 函示擴增前兩日瀏覽人數欄位 prev_1、prev_2，以 over_threshold 作為當日瀏覽人數欄位。\n判斷每個欄位瀏覽人數是否達到 100 以上，\n若人數達到 100 以上，標記為 1；\n若人數不滿 100，則標記為 0 查詢 stadium 作為主要表格並關聯 visit_threshold，若以下三條判斷式有其中一條成立，則該日日期與瀏覽人數需要包含於輸出結果內\nprev_2 + prev_1 + over_threshold \u0026gt; 2\nprev_1 + over_threshold + next_1 \u0026gt; 2\nover_threshold + next_1 + next_2 \u0026gt; 2 當看到連續這個關鍵字時，我會想到應該使用 lead() 函式或 lag() 函式。lead() 和 lag() 對應的是當前查詢的 record set ；若在子查詢或子表內使用，則對應子查詢或子表的資料範圍。\nlead( column, offset ) over( [partition by column1, column2, …] [order by column1, column2, …])\nlag( column, offset) over( [partition by column1, column2, …] [order by column1, column2, …]) Leetcode 官方對於這題所提供的解決方案是透過 self join T 兩次且不給予關聯條件，並判斷 T1.people 、 T2.people 和 T3.people 是否都有達到 100 以上。 解決方案 with visit_threshold as ( select id, if(people \u0026gt;= 100, 1, 0) as over_threshold, if(lead(people, 1) over() \u0026gt;=100, 1,0) as next_1, if(lead(people, 2) over() \u0026gt;=100, 1,0) as next_2, if(lag(people, 1) over() \u0026gt;=100, 1,0) as prev_1, if(lag(people, 2) over() \u0026gt;=100, 1,0) as prev_2 from stadium ) select A.id, A.visit_date , A.people from stadium A join visit_threshold B using(id) where B.over_threshold+B.next_1+B.next_2 \u0026gt; 2 or B.prev_1+B.prev_2+B.over_threshold \u0026gt; 2 or B.prev_1+B.over_threshold+B.next_1 \u0026gt; 2 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/human-traffic-of-stadium/","summary":"題目 Table: Stadium +---------------+---------+ | Column Name | Type | +---------------+---------+ | id | int | | visit_date | date | | people | int | +---------------+---------+ visit_date is the primary key for this table. Each row of this table contains the visit date and visit id to the stadium with the number of people during the visit. No two rows will have the same visit_date, and as the","title":"[leetcode][Database][Hard] 601. Human Traffic of Stadium"},{"content":"題目 Table: Salary\n+-------------+------+ | Column Name | Type | +-------------+------+ | id | int | | employee_id | int | | amount | int | | pay_date | date | +-------------+------+ id is the primary key column for this table. Each row of this table indicates the salary of an employee in one month. employee_id is a foreign key from the Employee table. Table: Employee\n+---------------+------+ | Column Name | Type | +---------------+------+ | employee_id | int | | department_id | int | +---------------+------+ employee_id is the primary key column for this table. Each row of this table indicates the department of an employee. Write an SQL query to report the comparison result (higher/lower/same) of the average salary of employees in a department to the company’s average salary.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Salary (id int, employee_id int, amount int, pay_date date) Create table If Not Exists Employee (employee_id int, department_id int) Truncate table Salary insert into Salary (id, employee_id, amount, pay_date) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;9000\u0026#39;, \u0026#39;2017/03/31\u0026#39;) insert into Salary (id, employee_id, amount, pay_date) values (\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;6000\u0026#39;, \u0026#39;2017/03/31\u0026#39;) insert into Salary (id, employee_id, amount, pay_date) values (\u0026#39;3\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;10000\u0026#39;, \u0026#39;2017/03/31\u0026#39;) insert into Salary (id, employee_id, amount, pay_date) values (\u0026#39;4\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;7000\u0026#39;, \u0026#39;2017/02/28\u0026#39;) insert into Salary (id, employee_id, amount, pay_date) values (\u0026#39;5\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;6000\u0026#39;, \u0026#39;2017/02/28\u0026#39;) insert into Salary (id, employee_id, amount, pay_date) values (\u0026#39;6\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;8000\u0026#39;, \u0026#39;2017/02/28\u0026#39;) Truncate table Employee insert into Employee (employee_id, department_id) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;) insert into Employee (employee_id, department_id) values (\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;) insert into Employee (employee_id, department_id) values (\u0026#39;3\u0026#39;, \u0026#39;2\u0026#39;) 解題思考 題目要求列出每個月份，每個部門平均薪資，和整個公司平均薪資的比較結果，並顯示高 higher/ 相等 same/ 低 lower的結果。 +-----------+---------------+------------+ | pay_month | department_id | comparison | +-----------+---------------+------------+ | 2017-02 | 1 | same | | 2017-03 | 1 | higher | | 2017-02 | 2 | same | | 2017-03 | 2 | lower | +-----------+---------------+------------+ 透過 with clause 建立 company_salary 表格，帶出每個月份公司的平均薪資。\n選擇 salary 作為主要表格，因為 company_salary 表格不需要區分個別部門的員工薪資。 透過 with clause 建立 department_salary 表格，帶出每個月份，每個部門的平均薪資。\n查詢 employee 作為主要表格並關聯 salary 表格，employee.department_id 可以作為區分員工部門的依據，便能計算出每個部門的平均薪資。 查詢 company_salary 作為主要表格並關聯 department_salary 表格，最後以 pay_month 升序、 department_id 升序輸出最後查詢結果。\n利用 if( condition, statement for condition true, statement for condition false ) 函示，並使用連續的 if() 判斷式來達成顯示高 higher/ 相等 same/ 低 lower的結果。 解決方案 with company_salary as ( select date_format(pay_date, \u0026#34;%Y-%m\u0026#34;) as pay_month, avg(amount) as avg_salary from salary group by month(pay_date) ), department_salary as ( select a.department_id, date_format(b.pay_date, \u0026#34;%Y-%m\u0026#34;) as pay_month, avg(b.amount) as avg_salary from employee a join salary b using (employee_id) group by a.department_id, month(b.pay_date) ) select a.pay_month as pay_month, b.department_id as department_id, if(b.avg_salary\u0026gt;a.avg_salary, \u0026#34;higher\u0026#34;, if(b.avg_salary \u0026lt; a.avg_salary, \u0026#34;lower\u0026#34;, \u0026#34;same\u0026#34;)) as comparison from company_salary a join department_salary b using(pay_month) order by a.pay_month, b.department_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/average-salary-departments-vs-company/","summary":"題目 Table: Salary +-------------+------+ | Column Name | Type | +-------------+------+ | id | int | | employee_id | int | | amount | int | | pay_date | date | +-------------+------+ id is the primary key column for this table. Each row of this table indicates the salary of an employee in one month. employee_id is a foreign key from the Employee table. Table: Employee +---------------+------+ | Column","title":"[leetcode][Database][Hard] 615. Average Salary: Departments VS Company"},{"content":"題目 Table: Student\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | name | varchar | | continent | varchar | +-------------+---------+ There is no primary key for this table. It may contain duplicate rows. Each row of this table indicates the name of a student and the continent they came from. A school has students from Asia, Europe, and America.\nWrite an SQL query to pivot the continent column in the Student table so that each name is sorted alphabetically and displayed underneath its corresponding continent. The output headers should be America, Asia, and Europe, respectively.\nThe test cases are generated so that the student number from America is not less than either Asia or Europe.\nSQL Schema\nCreate table If Not Exists Student (name varchar(50), continent varchar(7)) Truncate table Student insert into Student (name, continent) values (\u0026#39;Jane\u0026#39;, \u0026#39;America\u0026#39;) insert into Student (name, continent) values (\u0026#39;Pascal\u0026#39;, \u0026#39;Europe\u0026#39;) insert into Student (name, continent) values (\u0026#39;Xi\u0026#39;, \u0026#39;Asia\u0026#39;) insert into Student (name, continent) values (\u0026#39;Jack\u0026#39;, \u0026#39;America\u0026#39;) 解題思考 題目要求輸出樞紐表格，該樞紐表格需要使用 America 、 Asia 和 Europe ，並符合該大洲的學生名字 name 列表。 +---------+------+--------+ | America | Asia | Europe | +---------+------+--------+ | Jack | Xi | Pascal | | Jane | null | null | +---------+------+--------+ 使用 left join 便能簡單的達成題目要求。\n需要著手處理的問題點 : 如何讓學生名字並存在同一個 row 。\n利用 row_number() 函式，先將學生依據大洲分類，並給予每個大洲內的學生列表標記排序數字，最後透過選擇學生人數最多的大洲作為主要表格，依次 left join 其他大洲的學生列表，並以 row_number() 的標記數字做為表格關聯的條件。 解決方案 with from_america as (select name, row_number() over() as rn from student where continent = \u0026#39;America\u0026#39; order by name asc), from_asia as (select name, row_number() over() as rn from student where continent = \u0026#39;Asia\u0026#39; order by name asc), from_europe as (select name, row_number() over() as rn from student where continent = \u0026#39;Europe\u0026#39; order by name asc) select a.name as America, b.name as Asia, c.name as Europe from from_america a left join from_asia b using(rn) left join from_europe c using(rn) ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/students-report-by-geography/","summary":"題目 Table: Student +-------------+---------+ | Column Name | Type | +-------------+---------+ | name | varchar | | continent | varchar | +-------------+---------+ There is no primary key for this table. It may contain duplicate rows. Each row of this table indicates the name of a student and the continent they came from. A school has students from Asia, Europe, and America. Write an SQL query to pivot the continent","title":"[leetcode][Database][Hard] 618. Students Report By Geography"},{"content":"題目 Table: Activity\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | player_id | int | | device_id | int | | event_date | date | | games_played | int | +--------------+---------+ (player_id, event_date) is the primary key of this table. This table shows the activity of players of some games. Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on someday using some device. The install date of a player is the first login day of that player.\nWe define day one retention of some date x to be the number of players whose install date is x and they logged back in on the day right after x, divided by the number of players whose install date is x, rounded to 2 decimal places.\nWrite an SQL query to report for each install date, the number of players that installed the game on that day, and the day one retention.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Activity (player_id int, device_id int, event_date date, games_played int) Truncate table Activity insert into Activity (player_id, device_id, event_date, games_played) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;2016-03-01\u0026#39;, \u0026#39;5\u0026#39;) insert into Activity (player_id, device_id, event_date, games_played) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;2016-03-02\u0026#39;, \u0026#39;6\u0026#39;) insert into Activity (player_id, device_id, event_date, games_played) values (\u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2017-06-25\u0026#39;, \u0026#39;1\u0026#39;) insert into Activity (player_id, device_id, event_date, games_played) values (\u0026#39;3\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2016-03-01\u0026#39;, \u0026#39;0\u0026#39;) insert into Activity (player_id, device_id, event_date, games_played) values (\u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;2018-07-03\u0026#39;, \u0026#39;5\u0026#39;) 解題思考 題目要求利用 Activity 表格統計安裝用戶數量，以及用戶首日留存率。\n定義用戶安裝日 → 用戶在 Activity 第一次出現紀錄的 event_date 。\n定義用戶首日留存 → 用戶在安裝日隔天 event_date+1 有 Activity record。\n定義首日留存率計算方式 → 用戶首日留存數 / 安裝日總用戶人數 透過 with clause 建立 player_install_date 表格，找出每位用戶的安裝日 install_dt 透過 with clause 建立 player_day1_back 表格，統計用戶首日留存人數 cnt_login_back_player 。\n選擇 Activity 作為主要表格，使用 left join player_install_date 帶出所有的安裝日 install_dt ，並過濾 Activity 表格中符合條件 Activity.event_date = player_install_date+1 的 record set。 從 player_install_date 帶出 用戶安裝日 統計人數，使用 left join player_day1_back 帶出用戶首日留存 統計人數，便能計算出 首日留存率 。 解決方案 with player_install_date as ( select player_id, min(event_date) as install_dt from activity group by player_id ), player_day1_back as ( select distinct b.install_dt, count(a.player_id) over(partition by a.event_date) as cnt_login_back_player from activity a left join player_install_date b using(player_id) where a.event_date = date_add(b.install_dt, INTERVAL 1 DAY) ) select a.install_dt, a.installs, round(ifnull(b.cnt_login_back_player,0) / a.installs,2) as Day1_retention from ( select distinct install_dt, count(player_id) over(partition by install_dt) as installs from player_install_date ) a left join player_day1_back b on b.install_dt = a.install_dt ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/game-play-analysis-v/","summary":"題目 Table: Activity +--------------+---------+ | Column Name | Type | +--------------+---------+ | player_id | int | | device_id | int | | event_date | date | | games_played | int | +--------------+---------+ (player_id, event_date) is the primary key of this table. This table shows the activity of players of some games. Each row is a record of a player who logged in and played a number of games (possibly","title":"[leetcode][Database][Hard] 1097. Game Play Analysis V"},{"content":"題目 Table: Spending\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | spend_date | date | | platform | enum | | amount | int | +-------------+---------+ The table logs the history of the spending of users that make purchases from an online shopping website that has a desktop and a mobile application. (user_id, spend_date, platform) is the primary key of this table. The platform column is an ENUM type of (\u0026#39;desktop\u0026#39;, \u0026#39;mobile\u0026#39;). Write an SQL query to find the total number of users and the total amount spent using the mobile only, the desktop only, and both mobile and desktop together for each date.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Spending (user_id int, spend_date date, platform ENUM(\u0026#39;desktop\u0026#39;, \u0026#39;mobile\u0026#39;), amount int) Truncate table Spending insert into Spending (user_id, spend_date, platform, amount) values (\u0026#39;1\u0026#39;, \u0026#39;2019-07-01\u0026#39;, \u0026#39;mobile\u0026#39;, \u0026#39;100\u0026#39;) insert into Spending (user_id, spend_date, platform, amount) values (\u0026#39;1\u0026#39;, \u0026#39;2019-07-01\u0026#39;, \u0026#39;desktop\u0026#39;, \u0026#39;100\u0026#39;) insert into Spending (user_id, spend_date, platform, amount) values (\u0026#39;2\u0026#39;, \u0026#39;2019-07-01\u0026#39;, \u0026#39;mobile\u0026#39;, \u0026#39;100\u0026#39;) insert into Spending (user_id, spend_date, platform, amount) values (\u0026#39;2\u0026#39;, \u0026#39;2019-07-02\u0026#39;, \u0026#39;mobile\u0026#39;, \u0026#39;100\u0026#39;) insert into Spending (user_id, spend_date, platform, amount) values (\u0026#39;3\u0026#39;, \u0026#39;2019-07-01\u0026#39;, \u0026#39;desktop\u0026#39;, \u0026#39;100\u0026#39;) insert into Spending (user_id, spend_date, platform, amount) values (\u0026#39;3\u0026#39;, \u0026#39;2019-07-02\u0026#39;, \u0026#39;desktop\u0026#39;, \u0026#39;100\u0026#39;) 解題思考 題目要求統計透過用戶 user在使用PC desktop、手機APP mobile 以及兩者皆有 both 的採購金額 amount 。\n分開計算PC desktop 和手機APPmobile 並不困難，因此需要著手處理的是兩者皆有 both 採購紀錄的用戶，並將這些用戶從PC desktop和手機APPmobile 的統計中分離出來。 透過 with clause 建立 p_user_platform 表格，從 spending 表格帶出 user_id、 spend_date，並透過 group by user_id, spend_date 將資料分組為每個用戶 user_id 在每個 spend_date 的採購資料，並分開加總 platform=\u0026quot;mobile\u0026quot; 、platform=\u0026quot;desktop\u0026quot; 的採購數量 amount以賦予 mobild_amount 和 desktop_amount 欄位。 透過 with clause 建立 p_user_summary 表格，判斷 mobild_amount 和 desktop_amount 的值，將每位用戶在每個 spend_date 的採購情形分類成 desktop、mobile和both 這個步驟也可以在建立 p_user_platform 的過程中進行分類，但我傾向明確每張表格的用途，以便理解每一個 query statement 中引用的資料表格與欄位。 透過 with clause 建立 p_spend_date 表格，並在 spending 表格擷取所有不重複的 spend_date ，並將每個不重複的 spend_date 擴展成帶有 desktop、mobile和both 的 record 。\n這個動作會將每個不重複的 spend_date ，從原先的 1 row 擴展成 3 rows。 透過查詢 p_spend_date 作為主要表格，使用 left join p_user_summary 帶出相對應 spend_date 、platform 的 record set，並利用 spend_date 、platform 統計出該 spend_date 在 desktop、mobile和both 的總採購量 total_amount 和總計人數 total_user 。 解決方案 with p_user_platform as ( select user_id, spend_date, sum(case when platform = \u0026#39;mobile\u0026#39; then amount else 0 end) as mobile_amount, sum(case when platform = \u0026#39;desktop\u0026#39; then amount else 0 end) as desktop_amount from spending group by user_id, spend_date ), p_user_summary as ( select user_id, spend_date, if(mobile_amount \u0026gt; 0, if(desktop_amount \u0026gt; 0, \u0026#39;both\u0026#39;, \u0026#39;mobile\u0026#39;), \u0026#39;desktop\u0026#39;) as platform, mobile_amount + desktop_amount as amount from p_user_platform ), p_spend_date as ( select distinct(spend_date), \u0026#39;desktop\u0026#39; as platform from spending union select distinct(spend_date), \u0026#39;mobile\u0026#39; as platform from spending union select distinct(spend_date), \u0026#39;both\u0026#39; as platform from spending ) select a.spend_date, a.platform, sum(ifnull(b.amount,0)) as total_amount, count(b.user_id) as total_users from p_spend_date a left join p_user_summary b using(spend_date, platform) group by a.spend_date, a.platform ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/user-purchase-platform/","summary":"題目 Table: Spending +-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | spend_date | date | | platform | enum | | amount | int | +-------------+---------+ The table logs the history of the spending of users that make purchases from an online shopping website that has a desktop and a mobile application. (user_id, spend_date, platform) is the primary key of this table. The","title":"[leetcode][Database][Hard] 1127. User Purchase Platform"},{"content":"題目 Table: Users\n+----------------+---------+ | Column Name | Type | +----------------+---------+ | user_id | int | | join_date | date | | favorite_brand | varchar | +----------------+---------+ user_id is the primary key of this table. This table has the info of the users of an online shopping website where users can sell and buy items. Table: Orders\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | order_id | int | | order_date | date | | item_id | int | | buyer_id | int | | seller_id | int | +---------------+---------+ order_id is the primary key of this table. item_id is a foreign key to the Items table. buyer_id and seller_id are foreign keys to the Users table. Table: Items\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | item_id | int | | item_brand | varchar | +---------------+---------+ item_id is the primary key of this table. Write an SQL query to find for each user whether the brand of the second item (by date) they sold is their favorite brand. If a user sold less than two items, report the answer for that user as no. It is guaranteed that no seller sold more than one item on a day.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Users (user_id int, join_date date, favorite_brand varchar(10)) Create table If Not Exists Orders (order_id int, order_date date, item_id int, buyer_id int, seller_id int) Create table If Not Exists Items (item_id int, item_brand varchar(10)) Truncate table Users insert into Users (user_id, join_date, favorite_brand) values (\u0026#39;1\u0026#39;, \u0026#39;2019-01-01\u0026#39;, \u0026#39;Lenovo\u0026#39;) insert into Users (user_id, join_date, favorite_brand) values (\u0026#39;2\u0026#39;, \u0026#39;2019-02-09\u0026#39;, \u0026#39;Samsung\u0026#39;) insert into Users (user_id, join_date, favorite_brand) values (\u0026#39;3\u0026#39;, \u0026#39;2019-01-19\u0026#39;, \u0026#39;LG\u0026#39;) insert into Users (user_id, join_date, favorite_brand) values (\u0026#39;4\u0026#39;, \u0026#39;2019-05-21\u0026#39;, \u0026#39;HP\u0026#39;) Truncate table Orders insert into Orders (order_id, order_date, item_id, buyer_id, seller_id) values (\u0026#39;1\u0026#39;, \u0026#39;2019-08-01\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;) insert into Orders (order_id, order_date, item_id, buyer_id, seller_id) values (\u0026#39;2\u0026#39;, \u0026#39;2019-08-02\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;3\u0026#39;) insert into Orders (order_id, order_date, item_id, buyer_id, seller_id) values (\u0026#39;3\u0026#39;, \u0026#39;2019-08-03\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;) insert into Orders (order_id, order_date, item_id, buyer_id, seller_id) values (\u0026#39;4\u0026#39;, \u0026#39;2019-08-04\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;2\u0026#39;) insert into Orders (order_id, order_date, item_id, buyer_id, seller_id) values (\u0026#39;5\u0026#39;, \u0026#39;2019-08-04\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;) insert into Orders (order_id, order_date, item_id, buyer_id, seller_id) values (\u0026#39;6\u0026#39;, \u0026#39;2019-08-05\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;) Truncate table Items insert into Items (item_id, item_brand) values (\u0026#39;1\u0026#39;, \u0026#39;Samsung\u0026#39;) insert into Items (item_id, item_brand) values (\u0026#39;2\u0026#39;, \u0026#39;Lenovo\u0026#39;) insert into Items (item_id, item_brand) values (\u0026#39;3\u0026#39;, \u0026#39;LG\u0026#39;) insert into Items (item_id, item_brand) values (\u0026#39;4\u0026#39;, \u0026#39;HP\u0026#39;) 解題思考 題目要求判斷每位 seller 賣出的第二項 item 是否為該 seller 的喜愛品牌，並輸出 No 和 Yes 作為每位 seller 的分類結果。若某位 seller 只賣出一項 item，則結果應為 No。 透過 with clause 建立 order_info 表格，利用 rank()函式並依據賣出日期 orders.order_date 為每一筆銷售資料進行排序 rank_sell_item。\n同時，為 order_info 表格關聯 users表格以帶出每位 user的喜愛品牌 users.favorite_brand。 透過 with clause 建立 second_sell_brand 表格，並判斷 second_sell_brand.sell_brand = second_sell_brand.seller_fav_brand 以輸出 Yes 和 No 。 透過查詢 users 作為主要表格，使用 left join second_sell_brand 帶出每位 seller 賣出第二項 item 是否為自己的喜愛品牌結果。\n在建立 second_sell_brand 表格時，由於篩選條件 rank_sell_item=2 會過濾掉只賣出過一次的 user 資訊；而在 orders 表格中也存在某些 user 沒有賣出的紀錄。\n因此需要透過 left join second_sell_brand 保證每位 user 都包含於最終的輸出結果中。 解決方案 with order_info as ( select a.order_id, a.order_date, c.item_brand as sell_brand, a.seller_id, b.favorite_brand as seller_fav_brand, rank() over(partition by a.seller_id order by a.order_date) as rank_sell_item from orders a join users b on b.user_id = a.seller_id join items c on c.item_id = a.item_id order by a.order_id, a.order_date ), second_sell_brand as ( select seller_id, if(sell_brand=seller_fav_brand, \u0026#34;yes\u0026#34;, \u0026#34;no\u0026#34;) as 2nd_item_fav_brand from order_info where rank_sell_item = 2 ) select a.user_id as seller_id, ifnull(b.2nd_item_fav_brand,\u0026#34;no\u0026#34;) as 2nd_item_fav_brand from users a left join second_sell_brand b on b.seller_id = a.user_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/market-analysis-ii/","summary":"題目 Table: Users +----------------+---------+ | Column Name | Type | +----------------+---------+ | user_id | int | | join_date | date | | favorite_brand | varchar | +----------------+---------+ user_id is the primary key of this table. This table has the info of the users of an online shopping website where users can sell and buy items. Table: Orders +---------------+---------+ | Column Name | Type | +---------------+---------+ | order_id | int","title":"[leetcode][Database][Hard] 1159. Market Analysis II"},{"content":"題目 Table: Players\n+-------------+-------+ | Column Name | Type | +-------------+-------+ | player_id | int | | group_id | int | +-------------+-------+ player_id is the primary key of this table. Each row of this table indicates the group of each player. Table: Matches\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | match_id | int | | first_player | int | | second_player | int | | first_score | int | | second_score | int | +---------------+---------+ match_id is the primary key of this table. Each row is a record of a match, first_player and second_player contain the player_id of each match. first_score and second_score contain the number of points of the first_player and second_player respectively. You may assume that, in each match, players belong to the same group. The winner in each group is the player who scored the maximum total points within the group. In the case of a tie, the lowest player_id wins.\nWrite an SQL query to find the winner in each group.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Players (player_id int, group_id int) Create table If Not Exists Matches (match_id int, first_player int, second_player int, first_score int, second_score int) Truncate table Players insert into Players (player_id, group_id) values (\u0026#39;10\u0026#39;, \u0026#39;2\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;15\u0026#39;, \u0026#39;1\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;20\u0026#39;, \u0026#39;3\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;25\u0026#39;, \u0026#39;1\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;30\u0026#39;, \u0026#39;1\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;35\u0026#39;, \u0026#39;2\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;40\u0026#39;, \u0026#39;3\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;45\u0026#39;, \u0026#39;1\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;50\u0026#39;, \u0026#39;2\u0026#39;) Truncate table Matches insert into Matches (match_id, first_player, second_player, first_score, second_score) values (\u0026#39;1\u0026#39;, \u0026#39;15\u0026#39;, \u0026#39;45\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;0\u0026#39;) insert into Matches (match_id, first_player, second_player, first_score, second_score) values (\u0026#39;2\u0026#39;, \u0026#39;30\u0026#39;, \u0026#39;25\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;) insert into Matches (match_id, first_player, second_player, first_score, second_score) values (\u0026#39;3\u0026#39;, \u0026#39;30\u0026#39;, \u0026#39;15\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;0\u0026#39;) insert into Matches (match_id, first_player, second_player, first_score, second_score) values (\u0026#39;4\u0026#39;, \u0026#39;40\u0026#39;, \u0026#39;20\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;2\u0026#39;) insert into Matches (match_id, first_player, second_player, first_score, second_score) values (\u0026#39;5\u0026#39;, \u0026#39;35\u0026#39;, \u0026#39;50\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;) 解題思考 樞紐表要求輸出每個 group 內總計得分最高的玩家，若遇到平分情況則以 player_id 較小的一方獲勝。\n這題的思考方式和 [leetcode][Database][Hard]1972. First and Last Call On the Same Day 雷同。 透過 with clause 建立 max_score_of_player 表格，並分別加總每位 player 的 score 。\n將 score 表格中的 first_player 和 second_player 拆分成兩張子表，並擷取 first_score 和 second_score 以便進行每位 player 的 score 加總。 透過 with clause 建立 group_player_rank 表格，選擇 players 表格作為查詢主表並關聯 max_score_of_player ，並依據 max_score_of_player.score 降序和 players.player_id 升序的方式，利用 rank()函數對players.group 進行排名 rn。 最後，查詢 group_player_rank.rn 為 1 的資料，便能找出每個 group 總計得分最高的 player 。 解決方案 with max_score_of_player as ( select player_id, sum(score) as score from ( select first_player as player_id, first_score as score from matches union all select second_player as player_id, second_score as score from matches ) a group by player_id ), group_player_rank as ( select a.group_id, a.player_id, rank() over(partition by a.group_id order by b.score desc, a.player_id asc) as rn from players a join max_score_of_player b using(player_id) ) select group_id, player_id from group_player_rank where rn = 1 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/tournament-winners/","summary":"題目 Table: Players +-------------+-------+ | Column Name | Type | +-------------+-------+ | player_id | int | | group_id | int | +-------------+-------+ player_id is the primary key of this table. Each row of this table indicates the group of each player. Table: Matches +---------------+---------+ | Column Name | Type | +---------------+---------+ | match_id | int | | first_player | int | | second_player | int | | first_score |","title":"[leetcode][Database][Hard] 1194. Tournament Winners"},{"content":"題目 Table: Product\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | product_id | int | | product_name | varchar | +---------------+---------+ product_id is the primary key for this table. product_name is the name of the product. Table: Sales\n+---------------------+---------+ | Column Name | Type | +---------------------+---------+ | product_id | int | | period_start | date | | period_end | date | | average_daily_sales | int | +---------------------+---------+ product_id is the primary key for this table. period_start and period_end indicate the start and end date for the sales period, and both dates are inclusive. The average_daily_sales column holds the average daily sales amount of the items for the period. The dates of the sales years are between 2018 to 2020. Write an SQL query to report the total sales amount of each item for each year, with corresponding product_name, product_id, product_name, and report_year.\nReturn the result table ordered by product_id and report_year.\nSQL Schema\nCreate table If Not Exists Product (product_id int, product_name varchar(30)) Create table If Not Exists Sales (product_id int, period_start date, period_end date, average_daily_sales int) Truncate table Product insert into Product (product_id, product_name) values (\u0026#39;1\u0026#39;, \u0026#39;LC Phone \u0026#39;) insert into Product (product_id, product_name) values (\u0026#39;2\u0026#39;, \u0026#39;LC T-Shirt\u0026#39;) insert into Product (product_id, product_name) values (\u0026#39;3\u0026#39;, \u0026#39;LC Keychain\u0026#39;) Truncate table Sales insert into Sales (product_id, period_start, period_end, average_daily_sales) values (\u0026#39;1\u0026#39;, \u0026#39;2019-01-25\u0026#39;, \u0026#39;2019-02-28\u0026#39;, \u0026#39;100\u0026#39;) insert into Sales (product_id, period_start, period_end, average_daily_sales) values (\u0026#39;2\u0026#39;, \u0026#39;2018-12-01\u0026#39;, \u0026#39;2020-01-01\u0026#39;, \u0026#39;10\u0026#39;) insert into Sales (product_id, period_start, period_end, average_daily_sales) values (\u0026#39;3\u0026#39;, \u0026#39;2019-12-01\u0026#39;, \u0026#39;2020-01-31\u0026#39;, \u0026#39;1\u0026#39;) 解題思考 透過 with clause 分別建立 report_year 2018、2019 以及 2020 的產品銷售表格 sell_2018、 sell_2019 、 sell_2020\n由於 sales 表格的 period_start 和 period_end 有跨越年度的可能性，而在其他的測試資料集中，也可能含有早於 2018 年的銷售資料，或者晚於 2020 年後的銷售資料，因此需要特別注意時間範圍的切割。 Union sell_2018 、 sell_2019 和 sell_2020 ，並從 union 表格中取出每日平均銷售金額 average_daily_sales 乘以當年度整體銷售天數 datediff(period_end, period_start)+1 ，便可得到當年度的平均銷售總額。 解決方案 with sell_2018 as ( select product_id, if( year(period_start) \u0026lt; \u0026#39;2018\u0026#39;, \u0026#39;2018-01-01\u0026#39;, period_start) as period_start, if( year(period_end) \u0026gt; \u0026#39;2018\u0026#39;, DATE_FORMAT(period_end,\u0026#39;2018-12-31\u0026#39;), period_end ) as period_end, average_daily_sales from sales where year(period_start) \u0026lt;= \u0026#39;2018\u0026#39; ), sell_2019 as ( select product_id, if( year(period_start) \u0026lt; \u0026#39;2019\u0026#39;, \u0026#39;2019-01-01\u0026#39;, period_start) as period_start, if( year(period_end) \u0026gt; \u0026#39;2019\u0026#39;, DATE_FORMAT(period_end,\u0026#39;2019-12-31\u0026#39;), period_end ) as period_end, average_daily_sales from sales where year(period_start) \u0026lt;= \u0026#39;2019\u0026#39; and year(period_end) \u0026gt;= \u0026#39;2019\u0026#39; ), sell_2020 as ( select product_id, if( year(period_start) \u0026lt; \u0026#39;2020\u0026#39;, \u0026#39;2020-01-01\u0026#39;, period_start) as period_start, if( year(period_end) \u0026gt; \u0026#39;2020\u0026#39;, DATE_FORMAT(period_end,\u0026#39;2020-12-31\u0026#39;), period_end ) as period_end, average_daily_sales from sales where year(period_start) \u0026lt;= \u0026#39;2020\u0026#39; and year(period_end) \u0026gt;= \u0026#39;2020\u0026#39; ), product_sell_info as ( select a.product_id, b.product_name, date_format(a.period_start, \u0026#34;%Y\u0026#34;) as report_year, average_daily_sales * (datediff(a.period_end, a.period_start)+1) as total_amount from ( select * from sell_2018 union select * from sell_2019 union select * from sell_2020 ) a left join product b using(product_id) ) select product_id, product_name, report_year, total_amount from product_sell_info order by product_id, report_year ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/total-sales-amount-by-year/","summary":"題目 Table: Product +---------------+---------+ | Column Name | Type | +---------------+---------+ | product_id | int | | product_name | varchar | +---------------+---------+ product_id is the primary key for this table. product_name is the name of the product. Table: Sales +---------------------+---------+ | Column Name | Type | +---------------------+---------+ | product_id | int | | period_start | date | | period_end | date | | average_daily_sales | int | +---------------------+---------+ product_id","title":"[leetcode][Database][Hard] 1384. Total Sales Amount by Year"},{"content":"題目 Table: Orders\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | order_id | int | | customer_id | int | | order_date | date | | item_id | varchar | | quantity | int | +---------------+---------+ (ordered_id, item_id) is the primary key for this table. This table contains information on the orders placed. order_date is the date item_id was ordered by the customer with id customer_id. Table: Items\n+---------------------+---------+ | Column Name | Type | +---------------------+---------+ | item_id | varchar | | item_name | varchar | | item_category | varchar | +---------------------+---------+ item_id is the primary key for this table. item_name is the name of the item. item_category is the category of the item. You are the business owner and would like to obtain a sales report for category items and the day of the week.\nWrite an SQL query to report how many units in each category have been ordered on each day of the week.\nReturn the result table ordered by category.\nSQL Schema\nCreate table If Not Exists Orders (order_id int, customer_id int, order_date date, item_id varchar(30), quantity int) Create table If Not Exists Items (item_id varchar(30), item_name varchar(30), item_category varchar(30)) Truncate table Orders insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2020-06-01\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;10\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2020-06-08\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;10\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;3\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;2020-06-02\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;5\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;4\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2020-06-03\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;5\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;5\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;2020-06-04\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;1\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;6\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;2020-06-05\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;5\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;7\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;2020-06-05\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;10\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;8\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;2020-06-14\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;9\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;2020-06-21\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;5\u0026#39;) Truncate table Items insert into Items (item_id, item_name, item_category) values (\u0026#39;1\u0026#39;, \u0026#39;LC Alg. Book\u0026#39;, \u0026#39;Book\u0026#39;) insert into Items (item_id, item_name, item_category) values (\u0026#39;2\u0026#39;, \u0026#39;LC DB. Book\u0026#39;, \u0026#39;Book\u0026#39;) insert into Items (item_id, item_name, item_category) values (\u0026#39;3\u0026#39;, \u0026#39;LC SmarthPhone\u0026#39;, \u0026#39;Phone\u0026#39;) insert into Items (item_id, item_name, item_category) values (\u0026#39;4\u0026#39;, \u0026#39;LC Phone 2020\u0026#39;, \u0026#39;Phone\u0026#39;) insert into Items (item_id, item_name, item_category) values (\u0026#39;5\u0026#39;, \u0026#39;LC SmartGlass\u0026#39;, \u0026#39;Glasses\u0026#39;) insert into Items (item_id, item_name, item_category) values (\u0026#39;6\u0026#39;, \u0026#39;LC T-Shirt XL\u0026#39;, \u0026#39;T-shirt\u0026#39;) 解題思考 樞紐表要求依據分類 category列出周一至周日的數量 quantity統計，因此可以使用 left join 逐一列出樞紐表周一至周日的欄位。 +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Category | Monday | Tuesday | Wednesday | Thursday | Friday | Saturday | Sunday | +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Book | 20 | 5 | 0 | 0 | 10 | 0 | 0 | | Glasses | 0 | 0 | 0 | 0 | 5 | 0 | 0 | | Phone | 0 | 0 | 5 | 1 | 0 | 0 | 10 | | T-Shirt | 0 | 0 | 0 | 0 | 0 | 0 | 0 | +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ 利用 dayOfWeek() 將 orders.order_date 轉換成周一至周日 day，並透過 with clause 建立新表格 orders_with_group，擷取 orders.order_id 、orders.quantity dayOfWeek() 轉換後的 day 以及和 items 表格關聯後取得的 items.item_category 欄位 依據 orders_with_group加總每個 day 的 quantity ，並透過 with clause 建立新表 sum_quantity_by_category 。\n最後，從 items 表格列出不重複的 item_category ，並利用 left join 分別關聯周一至周日 day 的 quantity 加總。 解決方案 with orders_with_group as ( select a.order_id, dayofweek(a.order_date) as day, b.item_category, a.quantity from orders a join items b using(item_id) ), sum_quantity_by_category as ( select item_category, day, sum(quantity) as sum_quantity from orders_with_group group by item_category, day ) select distinct(a.item_category) as Category, ifnull(Mon.sum_quantity, 0) as Monday, ifnull(Tue.sum_quantity, 0) as Tuesday, ifnull(Wen.sum_quantity, 0) as Wednesday, ifnull(Thu.sum_quantity, 0) as Thursday, ifnull(Fri.sum_quantity, 0) as Friday, ifnull(Sat.sum_quantity, 0) as Saturday, ifnull(Sun.sum_quantity, 0) as Sunday from items a left join ( select item_category, sum_quantity from sum_quantity_by_category where day=2) Mon using(item_category) left join ( select item_category, sum_quantity from sum_quantity_by_category where day=3) Tue using(item_category) left join ( select item_category, sum_quantity from sum_quantity_by_category where day=4) Wen using(item_category) left join ( select item_category, sum_quantity from sum_quantity_by_category where day=5) Thu using(item_category) left join ( select item_category, sum_quantity from sum_quantity_by_category where day=6) Fri using(item_category) left join ( select item_category, sum_quantity from sum_quantity_by_category where day=7) Sat using(item_category) left join ( select item_category, sum_quantity from sum_quantity_by_category where day=1) Sun using(item_category) order by a.item_category /* Map the return value of function dayOfWeek and text 1 = Sunday 2 = Monday 3 = Tuesday 4 = Wednesday 5 = Thursday 6 = Friday 7 = Saturday */ ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/sales-by-day-of-the-week/","summary":"題目 Table: Orders +---------------+---------+ | Column Name | Type | +---------------+---------+ | order_id | int | | customer_id | int | | order_date | date | | item_id | varchar | | quantity | int | +---------------+---------+ (ordered_id, item_id) is the primary key for this table. This table contains information on the orders placed. order_date is the date item_id was ordered by the customer with id customer_id. Table: Items","title":"[leetcode][Database][Hard] 1479. Sales by Day of the Week"},{"content":"題目 Table: Calls\n+--------------+----------+ | Column Name | Type | +--------------+----------+ | caller_id | int | | recipient_id | int | | call_time | datetime | +--------------+----------+ (caller_id, recipient_id, call_time) is the primary key for this table. Each row contains information about the time of a phone call between caller_id and recipient_id. Write an SQL query to report the IDs of the users whose first and last calls on any day were with the same person. Calls are counted regardless of being the caller or the recipient.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Calls (caller_id int, recipient_id int, call_time datetime) Truncate table Calls insert into Calls (caller_id, recipient_id, call_time) values (\u0026#39;8\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;2021-08-24 17:46:07\u0026#39;) insert into Calls (caller_id, recipient_id, call_time) values (\u0026#39;4\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;2021-08-24 19:57:13\u0026#39;) insert into Calls (caller_id, recipient_id, call_time) values (\u0026#39;5\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2021-08-11 05:28:44\u0026#39;) insert into Calls (caller_id, recipient_id, call_time) values (\u0026#39;8\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2021-08-17 04:04:15\u0026#39;) insert into Calls (caller_id, recipient_id, call_time) values (\u0026#39;11\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2021-08-17 13:07:00\u0026#39;) insert into Calls (caller_id, recipient_id, call_time) values (\u0026#39;8\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-08-17 22:22:22\u0026#39;) 解題思考 建立 user_calls的 with clause，以提供最後輸出的表格使用。\nuser_calls將每筆 calls 中的通話紀錄拆分成兩筆 record ，即該筆通話紀錄的兩位 user 分別以自己的角度，紀錄該筆通話的時間以及通話的對象\nA ← communicate → B : A → B and B → A 建立 rank_calls 的 with clause ，對 user_calls 中的通話紀錄進行排序。\n對每個 user_id 的所有 call_time 通話時間做 升序 和 降序，以便找出第一筆通話 first call 和最後一筆通話 last call 。 對 rank_calls 每個 user_id 的 first call 和 last call 進行統計，若不重複的通話對象只有一位，則可以找出 first call 和 last call 都是同一人的 user_id 解決方案 with user_calls as ( select caller_id as user_id, call_time, recipient_id from calls union select recipient_id as user_id, call_time, caller_id as recipient_id from calls ), rank_calls as ( select user_id, recipient_id, date(call_time) as day, dense_rank() over(partition by user_id, date(call_time) order by call_time asc) as rn, dense_rank() over(partition by user_id, date(call_time) order by call_time desc) as rk from user_calls ) select distinct user_id from rank_calls where rn=1 or rk=1 group by user_id, day having count(distinct recipient_id) = 1 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/first-and-last-call-on-the-same-day/","summary":"題目 Table: Calls +--------------+----------+ | Column Name | Type | +--------------+----------+ | caller_id | int | | recipient_id | int | | call_time | datetime | +--------------+----------+ (caller_id, recipient_id, call_time) is the primary key for this table. Each row contains information about the time of a phone call between caller_id and recipient_id. Write an SQL query to report the IDs of the users whose first and last calls on","title":"[leetcode][Database][Hard]1972. First and Last Call On the Same Day"},{"content":"題目 Table: Candidates\n+-------------+------+ | Column Name | Type | +-------------+------+ | employee_id | int | | experience | enum | | salary | int | +-------------+------+ employee_id is the primary key column for this table. experience is an enum with one of the values (\u0026#39;Senior\u0026#39;, \u0026#39;Junior\u0026#39;). Each row of this table indicates the id of a candidate, their monthly salary, and their experience. A company wants to hire new employees. The budget of the company for the salaries is $70000. The company\u0026rsquo;s criteria for hiring are:\nHiring the largest number of seniors. After hiring the maximum number of seniors, use the remaining budget to hire the largest number of juniors. Write an SQL query to find the number of seniors and juniors hired under the mentioned criteria.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Candidates (employee_id int, experience ENUM(\u0026#39;Senior\u0026#39;, \u0026#39;Junior\u0026#39;), salary int) Truncate table Candidates insert into Candidates (employee_id, experience, salary) values (\u0026#39;1\u0026#39;, \u0026#39;Junior\u0026#39;, \u0026#39;10000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;9\u0026#39;, \u0026#39;Junior\u0026#39;, \u0026#39;10000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;2\u0026#39;, \u0026#39;Senior\u0026#39;, \u0026#39;20000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;11\u0026#39;, \u0026#39;Senior\u0026#39;, \u0026#39;20000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;13\u0026#39;, \u0026#39;Senior\u0026#39;, \u0026#39;50000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;4\u0026#39;, \u0026#39;Junior\u0026#39;, \u0026#39;40000\u0026#39;) 解題思考 樞紐表要求輸出 Senior雇員和 Junior雇員的可招聘人數。\n利用子查詢功能，分別帶出 Senior和 Junior的招聘結果，以作為樞紐表的欄位值。 利用 with clause 建立 Senior雇員、 Junior雇員的薪水排序表。\n因題目要求必須先招聘盡可能多的 Senior 雇員，再利用剩餘預算朝聘盡可能多的 Junior 雇員 薪水排序表中，用 row_number() 標註排序結果，排序主要條件為薪資 salary 升序；同時排序結果可做為第幾位雇員，即可招聘的雇員數量 薪水排序表中，依據薪資 salary 和雇員編號 employee_id 累加雇員薪資；同時累加雇員薪資可做為已消耗的預算 budget 查詢招聘 senior 花費的預算 cumulate_budget 。\n取 cumulate_budget \u0026lt;= budget 的 record set，並從中找出 max(rn) 與對應的 senior.cumulate_budget 計算可分配給Junior招聘的剩餘預算 remaining = budget - senior.cumulate_budget ，取 junior.cumulate_budget \u0026lt;= emaining 的 record set，並從中找出 max(rn) 與對應的 junior.cumulate_budget 解決方案 with seniors_salary_rank as ( select employee_id, salary, row_number() over(order by salary, employee_id) as rn, sum(salary) over(order by salary, employee_id) as cumulate_budget from candidates where experience = \u0026#39;senior\u0026#39; order by salary ), juniors_salary_rank as ( select employee_id, salary, row_number() over(order by salary, employee_id asc) as rn, sum(salary) over(order by salary, employee_id) as cumulate_budget from candidates where experience = \u0026#39;junior\u0026#39; order by salary ), hire_seniors as ( select b.rn, 70000 - b.cumulate_budget as remain_budget from ( select max(cumulate_budget) as cumulate_budget from seniors_salary_rank where cumulate_budget \u0026lt;= 70000 ) a join seniors_salary_rank b using(cumulate_budget) ), hire_junior as ( select b.rn, ifnull((select remain_budget from hire_seniors limit 1),70000) - b.cumulate_budget as remain_budget from ( select max(cumulate_budget) as cumulate_budget from juniors_salary_rank where cumulate_budget \u0026lt;= ifnull((select remain_budget from hire_seniors limit 1),70000) ) a join juniors_salary_rank b using(cumulate_budget) ) select distinct(experience) as experience, case when experience = \u0026#39;Senior\u0026#39; then ifnull((select rn from hire_seniors limit 1 ),0) when experience = \u0026#39;Junior\u0026#39; then ifnull((select rn from hire_junior limit 1 ),0) end as accepted_candidates from candidates a ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/the-number-of-seniors-and-juniors-to-join-the-company/","summary":"題目 Table: Candidates +-------------+------+ | Column Name | Type | +-------------+------+ | employee_id | int | | experience | enum | | salary | int | +-------------+------+ employee_id is the primary key column for this table. experience is an enum with one of the values (\u0026#39;Senior\u0026#39;, \u0026#39;Junior\u0026#39;). Each row of this table indicates the id of a candidate, their monthly salary, and their experience. A company wants to hire","title":"[leetcode][Database][Hard] 2004. The Number of Seniors and Juniors to Join the Company"},{"content":"題目 Table: Buses\n+--------------+------+ | Column Name | Type | +--------------+------+ | bus_id | int | | arrival_time | int | | capacity | int | +--------------+------+ bus_id is the primary key column for this table. Each row of this table contains information about the arrival time of a bus at the LeetCode station and its capacity (the number of empty seats it has). No two buses will arrive at the same time and all bus capacities will be positive integers. Table: Passengers\n+--------------+------+ | Column Name | Type | +--------------+------+ | passenger_id | int | | arrival_time | int | +--------------+------+ passenger_id is the primary key column for this table. Each row of this table contains information about the arrival time of a passenger at the LeetCode station. Buses and passengers arrive at the LeetCode station. If a bus arrives at the station at a time tbus and a passenger arrived at a time tpassenger where tpassenger \u0026lt;= tbus and the passenger did not catch any bus, the passenger will use that bus. In addition, each bus has a capacity. If at the moment the bus arrives at the station there are more passengers waiting than its capacity capacity, only capacity passengers will use the bus.\nWrite an SQL query to report the number of users that used each bus.\nReturn the result table ordered by bus_id in ascending order.\nSQL Schema\nCreate table If Not Exists Buses (bus_id int, arrival_time int, capacity int) Create table If Not Exists Passengers (passenger_id int, arrival_time int) Truncate table Buses insert into Buses (bus_id, arrival_time, capacity) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;) insert into Buses (bus_id, arrival_time, capacity) values (\u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;10\u0026#39;) insert into Buses (bus_id, arrival_time, capacity) values (\u0026#39;3\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;2\u0026#39;) Truncate table Passengers insert into Passengers (passenger_id, arrival_time) values (\u0026#39;11\u0026#39;, \u0026#39;1\u0026#39;) insert into Passengers (passenger_id, arrival_time) values (\u0026#39;12\u0026#39;, \u0026#39;1\u0026#39;) insert into Passengers (passenger_id, arrival_time) values (\u0026#39;13\u0026#39;, \u0026#39;5\u0026#39;) insert into Passengers (passenger_id, arrival_time) values (\u0026#39;14\u0026#39;, \u0026#39;6\u0026#39;) insert into Passengers (passenger_id, arrival_time) values (\u0026#39;15\u0026#39;, \u0026#39;7\u0026#39;) 解題思考 乘客 passenger 的抵達時間需要在公車 bus 抵達之前，計算每班公車 bus_id 抵達時，可能潛在的乘客 passenger_id 總數量。\n由於公車 bus_id 的運載能力 capacity 可能無法滿足當前等待中的所有乘客 passenger_id ， 因此先行計算每班公車可能需要負荷的乘客運載量 承上，比較當前公車 bus_id運載能力 capacity 與等待搭乘的乘客數量，並從兩者中取最小值，表達搭乘該班次公車 bus_id 的實際乘客數量 可能存在的乘客數 - 累積的乘客數 = 實際搭乘的乘客數量 建立暫存表初始化 mysql variable ，用以 暫存實際搭乘的乘客數量 和 可能存在的乘客數 解決方案 with people_possiable_take_bus as ( /* Finding the possible passengers with each bus regardless of passenger catch the bus or not, that will cumulate all of passengers whoes previous arrival */ select a.bus_id, a.arrival_time, a.capacity, count(b.passenger_id) as possible_passenger_cnt from buses a left join passengers b on b.arrival_time \u0026lt;= a.arrival_time group by a.bus_id order by a.arrival_time ), alloc_people_take_bus as ( /* Calculating how many passengers can loaded by each bus, and accumulate passengers regardless of passenger catch bus or not, due to caulse `people_possiable_take_bus` pre-calculate the possible passengers with each bus, and the situation for the passengers whoes cannot catch currently or pervious bus includes of the statement `least(capacity, possible_passenger_cnt-@accum_passengers)`, that\u0026#39;s why we can directly to calculate `possible_passenger_cnt-@accum_passengers` and compare bus\u0026#39;s capacity and `possible_passenger_cnt-@accum_passengers` to take the least value */ select bus_id, passengers_cnt from ( select a.bus_id, @passengers_cnt := least(capacity, possible_passenger_cnt-@accum_passengers) as passengers_cnt, @accum_passengers := @accum_passengers + @passengers_cnt from people_possiable_take_bus a, (select @passengers_cnt := 0, @accum_passengers :=0) b ) output ) select bus_id, passengers_cnt from alloc_people_take_bus order by bus_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/the-number-of-passengers-in-each-bus-ii/","summary":"題目 Table: Buses +--------------+------+ | Column Name | Type | +--------------+------+ | bus_id | int | | arrival_time | int | | capacity | int | +--------------+------+ bus_id is the primary key column for this table. Each row of this table contains information about the arrival time of a bus at the LeetCode station and its capacity (the number of empty seats it has). No two buses will arrive","title":"[leetcode][Database][Hard] 2153. The Number of Passengers in Each Bus II"},{"content":"題目 Table: Products\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | product_id | int | | store | varchar | | price | int | +-------------+---------+ (product_id, store) is the primary key for this table. Each row of this table indicates the price of product_id in store. There will be at most 30 different stores in the table. price is the price of the product at this store. Important note: This problem targets those who have a good experience with SQL. If you are a beginner, we recommend that you skip it for now.\nImplement the procedure PivotProducts to reorganize the Products table so that each row has the id of one product and its price in each store. The price should be null if the product is not sold in a store. The columns of the table should contain each store and they should be sorted in lexicographical order.\nThe procedure should return the table after reorganizing it.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Products (product_id int, store varchar(7), price int) Truncate table Products insert into Products (product_id, store, price) values (\u0026#39;1\u0026#39;, \u0026#39;Shop\u0026#39;, \u0026#39;110\u0026#39;) insert into Products (product_id, store, price) values (\u0026#39;1\u0026#39;, \u0026#39;LC_Store\u0026#39;, \u0026#39;100\u0026#39;) insert into Products (product_id, store, price) values (\u0026#39;2\u0026#39;, \u0026#39;Nozama\u0026#39;, \u0026#39;200\u0026#39;) insert into Products (product_id, store, price) values (\u0026#39;2\u0026#39;, \u0026#39;Souq\u0026#39;, \u0026#39;190\u0026#39;) insert into Products (product_id, store, price) values (\u0026#39;3\u0026#39;, \u0026#39;Shop\u0026#39;, \u0026#39;1000\u0026#39;) insert into Products (product_id, store, price) values (\u0026#39;3\u0026#39;, \u0026#39;Souq\u0026#39;, \u0026#39;1900\u0026#39;) 解題思考 題目要求輸出每個產品 product 在每間商店的售價 +------------+----------+--------+------+------+ | product_id | LC_Store | Nozama | Shop | Souq | +------------+----------+--------+------+------+ | 1 | 100 | null | 110 | null | | 2 | null | 200 | null | 190 | | 3 | null | null | 1000 | 1900 | +------------+----------+--------+------+------+ 使用 group_concat() 組合樞紐表欄位的 sql statement，因樞紐表需依據 store 列出欄位，而 store 的數目是不固定的。 使用 prepare statement 執行包含 group_concat() 預先組合好的 sql statement 使用 group product_id 對樞紐表進行總計，因樞紐表要求依據 product 統計在不同 store 中的售價 price 解決方案 CREATE PROCEDURE PivotProducts() BEGIN -- Override GROUP_CONCAT length which has a default limit of 1024 SET SESSION group_concat_max_len = 1000000; -- Store case statement for dynamically generated columns in a variable ie case_stmt SET @case_stmt = NULL; SELECT GROUP_CONCAT(DISTINCT CONCAT(\u0026#39;SUM(CASE WHEN store = \u0026#34;\u0026#39;, store, \u0026#39;\u0026#34; THEN price END) AS \u0026#39;, store)) INTO @case_stmt FROM products; -- Insert above statement (@case_stmt) in the following main query to frame final query SET @sql_query = CONCAT(\u0026#39;SELECT product_id, \u0026#39;, @case_stmt, \u0026#39; FROM products GROUP BY product_id\u0026#39;); -- Execute final query PREPARE final_sql_query FROM @sql_query; EXECUTE final_sql_query; DEALLOCATE PREPARE final_sql_query; END ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/dynamic-pivoting-of-a-table/","summary":"題目 Table: Products +-------------+---------+ | Column Name | Type | +-------------+---------+ | product_id | int | | store | varchar | | price | int | +-------------+---------+ (product_id, store) is the primary key for this table. Each row of this table indicates the price of product_id in store. There will be at most 30 different stores in the table. price is the price of the product at this store.","title":"[leetcode][Database][Hard] 2252. Dynamic Pivoting of a Table"},{"content":"在 [Customer Data Platform 是如何煉成的 (二)]中提到了 User Behavior，但 User Behavior 的資料從哪裡來，又該如何定義呢 ? Customer Data Platform 是如何煉成的 (二) Date: 2022-03-28 \u0026nbsp; Categories: #Customer Data Platform\u0026nbsp; 顯而易見，討論出一個有共識、覺得可行的方式將問題進行轉化，這就屬於洞察(Insights)；而被提出的問題本身，則是被發現的異常(Anomaly)。原文最後提出「具有評分等級的使用者清單」則是貢獻(Contribute)。 有趣的是，當營銷人員依據清單進行預算投放後，便又能獲取新一輪的結果，這個結果除了可供驗證，同時也具備發現新異常的可能性。 ...... 以 GA ( Google Analytics ) 為例，在 Web 或 Mobile APP 中進行 GA 埋 code，這些 code 可以是 GA 預設的事件，如 : Page View 、 Session Engagement 、 Activity User，或者是自定義的 event 等等。 GCP BigQuery 提供的 Public datasets 中也提供已將 GA 資料轉化為 ecommerce 的 dataset ，來源為 Qwiklabs: Predict Visitor Purchases with a Classification Model in BQML\n本篇文章也利用這份公開資料集進行說明 :\nWhat Goal We Need Feature and Label Improve and Tune User Behavior 比較直觀的是對 behavior 的理解，可以想像當消費者在不同的 E-Commerce website 進行瀏覽商品、獲得推薦或是購物車結帳等操作時，由於 website 的設計不同，消費者可能需要跨越不同的頁面、點擊不同的連結，或是輸入不同的資料等等。\n因此，消費者在 website A 與 website B 的 「behavior」也會不同；從另外一個角度來看 : 同一個 website 中，消費者要達成相同目的的操作，必定會在有限個數的操作途徑中完成。而這些途徑也就構成了 User Behavior 基本單位，而有針對性、目的性的對途徑資料進行挑選，也就構成了一個 specific behavior 的定義。\nWhat Goal We Need ? 在一開始會想知道 website 訪問人數、購買次數以及轉化率各是多少；如下圖所示，分別是\n訪問人數 : 約 74 萬 購買次數 : 約 2 萬 轉化率 : 約 2.7% 然而，僅從結果層面獲得的資料，無法描述每項商品的銷售情況，因此對每項商品進行排比\n這可能就是常見的報表內容，即各項商品的銷售情況與 website 的成效指標；但是若更進一步的思考，這份報表所表達的是對資料進行統計後的資訊，或是常見的用詞 Data Informed ；那麼，分析的目標只是產生報表數據嗎 ?\n或者，是希望能從資料中協助識別「哪些訪問者更可能會促成購買商品的事件」呢 ?\nFeature and Label 首先考慮有多少訪問人數進行了購買，包含了第一次瀏覽就購買以及再次訪問後進行購買\n共有 (11,873 / 741,721) = 1.6% 的人是在第二次訪問商品頁面時，才進行購買；雖然沒有一個正確答案，但普遍的原因可能是消費者在購買前會進行商品的比價。\n以此為例，可以從原始資料中列舉一些因素，作為判斷訪問者是否會進行購買的依據 ( feature )，並將再次回訪是否產生購買行為作為答案 ( label ) 對其訓練一個模型 ( model )；期望在下次收集到相關資料時，模型可以識別並告知訪問者是否會產生購買行為。\n第一次挑選 feature 時，對以下兩個因素進行分析\nbounces : 訪問者是否立即離開 website time_on_site : 訪問者在 website 停留的時間 通常在訓練和評估模型之前，直接判斷 feature 的選擇是好或不好都為時過早，但在 time_on_site 排比前 10 的結果中，只有 1 個客戶返回購買；而模型測試的準確率也確實不好。\nImprove and Tune 在原始數據中，可能有更多的 feature 可以幫助 model 進行識別購買行為起到作用；而找出 feature 的方法除了對所有排列組合逐一進行嘗試外，也可以透過與相關人員，如 : 營銷人員、UX 設計師、統計專家或資料科學家等等，進行討論並達成共識後得出。\n在這個案例中，除了 bounces 與 time_on_site 之外，還可以加入以下的 feature\n訪問者第一次訪問時，在結帳過程中經歷了多少次的操作 (距離) 流量的來源 : 透過搜索或是 referring site 等等 設備的類別 : 手機 、 平板 或是 PC 地理資訊 : 來自哪個國家 重新訓練模型後測試的準確率也有所提高，同時模型也能提供一個預測結果，告知該訪問者是否會進行購買行為\nSummary Qwiklabs: Predict Visitor Purchases with a Classification Model in BQML 的案例在最後給出的結論如下\n在前 6% 的首次訪問者中，超過 6% 的人會在後續訪問時產生購買行為 整體而言，只有 0.7% 的首次訪問者，會在後續訪問時產生購買行為 瞄準前 6% 的第一次訪問者名單，會使營銷投資回報率提高 9 倍 因此，若是能夠在得到模型預測的結果後，依據營銷策略進行即時的投放處理，包含但不限於: EDM廣告、Coupon折價券或是限定綑綁折扣等等，建構起一個自動化營銷的方法；這也是一種 Data Driven 的方法。\n整篇文章寫到這裡，從 「What Goal We Need」 中辨別更具價值的目標、「Feature and Label」中定義需產出的貢獻與評斷方式，到最後「Improve and Tune」透過討論達成共識，並進行相對應的調整，讓整體結果能夠產出的更好貢獻，我認為這是一種不斷優化與建立 CDP 的好方法。\n","permalink":"https://blog.zhengweiliu.com/posts/normal/customer-data-platform-3/","summary":"\u003cp\u003e在最後給出的結論如下\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在前 6% 的首次訪問者中，超過 6% 的人會在後續訪問時產生購買行為\u003c/li\u003e\n\u003cli\u003e整體而言，只有 0.7% 的首次訪問者，會在後續訪問時產生購買行為\u003c/li\u003e\n\u003cli\u003e瞄準前 6% 的第一次訪問者名單，會使營銷投資回報率提高 9 倍\n因此，若是能夠在得到模型預測的結果後，依據營銷策略進行即時的投放處理，包含但不限於: EDM廣告、Coupon折價券或是限定綑綁折扣等等，建構起一個自動化營銷的方法；這也是一種 \u003cstrong\u003eData Driven\u003c/strong\u003e 的方法。\u003c/li\u003e\n\u003c/ul\u003e\n","title":"Customer Data Platform 是如何煉成的 (三)"},{"content":"在 [Customer Data Platform 是如何煉成的]中提到，Data Platform 透過洞察與發現 ( Insights Discovery )、貢獻與進化 ( Contribute Evolution) 以及異常偵測 ( Anomaly Detection) 組成一個周而復始的正向循環，讓資料提供具有貢獻的結果。 Customer Data Platform 是如何煉成的 Date: 2022-03-22 \u0026nbsp; Categories: #Customer Data Platform\u0026nbsp; 提到 CDP ( Customer Data Platform ) ，可能就會想到利用顧客相關資料，為顧客分群分類貼標籤，透過網站、經營社群或 APP 進行精準投放廣告，達到再行銷的成果；甚至是透過 Machine Learning，或結合 CRM 、 Google Analytics 等資料，達成預估市場規模、優化推薦商品等目的。 ...... 最近讀到 How to Optimize KPIs by Distilling Data With Machine Learning 這篇文章，原文中提及的例子: 利用 User Behavior 進行機器學習，最後產出一個具備評分的可交付名單，提供給營銷人員進行後續操作；詳細的部分請再點閱原文，以下我想分享閱讀後的心得。\n費米估算 依據維基百科的描述\n一個經典的費米問題的例子是費米提出的「在芝加哥有多少鋼琴調琴師」\n比如說，我們會採用以下的假設\n1. 大約有9,000,000 人生活在芝加哥。\n2. 在芝加哥平均每個家庭有2個人。\n3. 大約在20個家庭中有1個家庭需要定期鋼琴調音。\n4. 定期調琴的鋼琴每年需要調整一次。\n5. 每個調琴師大約需要2小時調琴，包括路上時間。\n6. 每個調琴師每天工作8小時，一周5天，一年50周。\n最後經過計算，大略的估算出 225 個調琴師在芝加哥；在這個問題的時空背景下，事實上， 一共有大約 290 名調琴師在芝加哥。\n量化問題 在調琴師的問題中，在沒有任一組織或權威可以提供完整調琴師清單的情況下，從圍繞調琴師的因素進行分析與展開，對相關的條件訂下一個基準數據，再一步步逼近命題，最終估算出大略的人數；\n而原文中「如何有效利用有限的營銷預算」，換個方式來描述問題則變成「將營銷預算投入給哪些消費者，可以取得最大成效」 : 這使得問題本身可以圍繞著「消費者」這個因素進行分析與展開，或許是分析使用者消費行為、或許是透過 RFM 來進行初步的分析，更可以導入消費者會員等級權益等額外資料進行輔助。\n顯而易見，討論出一個有共識、覺得可行的方式將問題進行轉化，這就屬於洞察(Insights)；而被提出的問題本身，則是被發現的異常(Anomaly)。原文最後提出「具有評分等級的使用者清單」則是貢獻(Contribute)。\n有趣的是，當營銷人員依據清單進行預算投放後，便又能獲取新一輪的結果，這個結果除了可供驗證，同時也具備發現新異常的可能性。\n背後的核心 同樣以調琴師的問題為例\n是否曾考慮過，為什麼「在芝加哥有多少鋼琴調琴師」這個問題會被提出 ?\n這就具備無限多種可能性，如 : 「目前有一個專供調琴師使用的產品，因此想知道芝加哥的市場規模」「芝加哥是否適合擴大鋼琴的販售規模」「如果想投身調琴師的職業，在芝加哥是否適合」 等等\n與原文提出的「如何有效利用有限的營銷預算」一致，「怎麼做能夠提升營銷數字」「某類型的商品是否適合投放營銷預算」等等\n我們希望借鑑成功案例時，不妨先試著共同討論\n我們正面對著什麼樣的情況 ?\n我們希望能改變什麼情況 ?\n以更好的辨認目前的問題，屬於可以分析的「真議題」，或者只是展現一個虛榮數字的「假議題」\n","permalink":"https://blog.zhengweiliu.com/posts/normal/customer-data-platform-2/","summary":"顯而易見，討論出一個有共識、覺得可行的方式將問題進行轉化，這就屬於洞察(Insights)；而被提出的問題本身，則是被發現的異常(Anomaly)。原文最後提出「具有評分等級的使用者清單」則是貢獻(Contribute)。\n有趣的是，當營銷人員依據清單進行預算投放後，便又能獲取新一輪的結果，這個結果除了可供驗證，同時也具備發現新異常的可能性。","title":"Customer Data Platform 是如何煉成的 (二)"},{"content":"提到 CDP ( Customer Data Platform ) ，可能就會想到利用顧客相關資料，為顧客分群分類貼標籤，透過網站、經營社群或 APP 進行精準投放廣告，達到再行銷的成果；甚至是透過 Machine Learning，或結合 CRM 、 Google Analytics 等資料，達成預估市場規模、優化推薦商品等目的。\n似乎只要有充分的資料，就能開始享受 CDP 為行銷帶來諸多好處。然而，具體上 CDP 是怎麼運作的 ? 又該如何善用 CDP 的功能呢 ? 或許可以從瞭解 CDP 是如何構成的開始。\nCustomer and Data Platform 不如簡單粗暴的，試著從名稱上將 Customer 與 Data Platform 分開 ：\n對於 Customer ，或者說是與顧客相關的資料，從會員帳號的建立到商品購買紀錄、網頁瀏覽操作紀錄，甚至是會員權益分級等等；因顧客的主動行為產生，無論是否存在誘因，並且蒐集起來寶貴資料。\n如同每道料理都是由食材原料組成，僅有食材原料卻沒有辦法變成美味的料理；Data Platform 擔任烹飪者的角色，將這些寶貴資料進一步處理，端出一道又一道的營養又可口的美食。\nBring Benefits with Data-Platform 回想一下料理的烹飪過程，從準備材料、對食材進行清洗、將食材切割成適當大小或醃製入味，到觀察火候並依序加入對應的食材，最後出鍋上菜，若上菜後發現味道不好，還可以再依據這次的經驗進行調整與修正。\nData Platform 也對資料進行清理、前處理，對處理後的資料進行重組，最終產生出有貢獻的數據成果，並透過轉譯的方式進行交付；整個流程就像下圖所描述：\n洞察與發現 ( Insights Discovery ) \u0026gt; 貢獻與進化 ( Contribute Evolution) \u0026gt; 異常偵測 ( Anomaly Detection) \u0026gt; 洞察與發現 ( Insights Discovery ) \u0026gt; …\n而在這一循環的流程中，都緊密的圍繞著一個核心 ( Kernel ) 在進行：如同CDP 是的 Kernel 是 Customer 、 麻婆豆腐的 Kernel 是麻婆一樣 。\nData Platform 中的所有處理、步驟以及流程，都是為了核心在服務。\n洞察與發現 ( Insights Discovery ) 如果我們產生一個疑問，大多數的情況下在第一時間，我們都會問 :「發生什麼事 ?」 或是 「某個事件是不是造成什麼影響」，而不會是 「某個具體的量(或者數字)是多少」\n這些問題本身就是 Insight 的催化劑，促使我們想進一步去分析、去理解\n貢獻與進化 ( Contribute Evolution) 提出問題並在分析後，若能找到一些可能的答案，便能利用 IFTTT ( If This Then That ) 進行問題簡化，即 :\n如果發生 A 狀況，那麼會造成什麼結果 / 需要如何應對\n這是一種將數據結果進行翻譯的過程，讓數據變成一個應對清單，並能輕鬆的交付給其他利益相關者，以便他們對結果進行下一步的操作。\n異常偵測 ( Anomaly Detection) 從上一步驟產生的應對清單，在利益相關者對其進行操作之後，如 : 廣告投放 ， 便能對清單進行驗證、討論；或是對特定名單進行持續的觀測，看看是否仍有與數據結果不符，或者相對異常的行為出現。\n當上述的行為出現後，在與相關利益者們進一步討論原因 ( 回到了 Insights Discovery ) ，獲取新的或者增強應對清單 ( 又到了 Contribute Evolution ) ，再次投入進行操作 ( 再持續進行 Anomaly Detection )，以此往復直到這個問題不再需要處理，或者不再具備價值時，就能停止。\n","permalink":"https://blog.zhengweiliu.com/posts/normal/customer-data-platform-1/","summary":"提到 CDP ( Customer Data Platform ) ，可能就會想到利用顧客相關資料，為顧客分群分類貼標籤，透過網站、經營社群或 APP 進行精準投放廣告，達到再行銷的成果；甚至是透過 Machine Learning，或結合 CRM 、 Google Analytics 等資料，達成預估市場規模、優化推薦商品等目的。","title":"Customer Data Platform 是如何煉成的"},{"content":"在 GCP Billing Analytics 中提到過關於 Cloud Functions 的計費超乎預期，進一步分析開發的使用習慣後，也找出部分功能應該將其從 Cloud Functions 搬遷至基於 GCE instances 的服務上，以達到節費的期望。 GCP Billing Analysis Date: 2021-12-27 \u0026nbsp; Categories: #Google Cloud Platform\u0026nbsp; #Analysis\u0026nbsp; 在產品的開發中，團隊消耗成本最高的前幾項排名既在意料之中，Google Compute Engine (GCE)、 Cloud Functions 、 BigQuery 以及 Google Cloud Storage，但細項的部分也在意料之外。 ...... 在原先的設計中，我們將 Cloud Functions 作為 ETL data flow 的其中一個環節，透過 Pub/Sub trigger Cloud Functions 的方式使其運作；考慮到 Pub/Sub subscriber push/pull 的 Ack 等待時間有著最長 600 秒的限制，我將這部分需要搬遷的 Cloud Functions 大致分為兩種需求\n靜態資料源: 在提取資料時，可預期資料是存在且可被存取的 動態資料源: 可能發生資料不存在，或者是無法存取的情況 本篇文章是記錄\n用 Kubernetes Pod 替代 Cloud Function 環節以處理動態資料源的方法 Google Kubernetes Engine: Ingress \u0026amp; Service ASGI 與FastAPI Dockerize \u0026amp; Deployment 靜態資料源的處理方案 \u0026gt; Migrate Google Cloud Functions to Airflow Migrate Google Cloud Functions to Airflow Date: 2022-01-22 \u0026nbsp; Categories: #Google Cloud Platform\u0026nbsp; #Data Engineering\u0026nbsp; 本篇文章是記錄 用 Airflow DAG (Directed Acyclic Graph) 替代 Cloud Function 環節以處理靜態資料源的方法 Airflow GCP Operators 使用 在 DAG 中平行處理(parallel processing)的方式 ...... Design Change Figure 1 是一個常見的使用案例，我將 Cloud Function 的執行邏輯簡略為 4 個部份來進行描述，即: 等待 Request (Accept Request) 、 處理邏輯 (Process)、產出結果 (Result) ，以及回復 Ack (Response HTTP Status Code)\nProcess 的區塊中，若需要向外部資料源提出存取請求，如: 3rd-party API 、爬蟲、網路磁碟機等，獲取相關的資訊後才能繼續進行處理的工作，在本篇文章中則以動態資料源來稱呼這些外部資料源\n對於 Runtime 時可能遭遇錯誤的資料源，可能遇到請求被拒絕(Reject)，如: 403、404或者5系列的錯誤代碼，或是遇到請求的資源本身不存在。\nGoogle Kubernetes Engine: Ingress \u0026amp; Service Figure 2 使用 Kubernetes Pod 替代 Cloud Function ， 因團隊先前已採用 Google Kubernetes Engine (GKE) 進行容器化的部署，這邊也就延續團隊成果。\n我也將 Pub/Sub 的模式從 trigger 更改為 Push Message : 當 Pub/Sub Subscriber Queue 存在訊息時， Subscriber 會推送 Message 到設定好的 Webhook URL，並且遵循 Ack 等待時間有著最長 600 秒的限制。\n關於 Deployment 的部分會在稍後提到，這邊先討論 Ingress 和 Service 的設置\nService Type: NodePort\napiVersion: v1kind: Servicemetadata: name: my-servicespec: type: NodePort selector: app: MyApp ports: # By default and for convenience, the `targetPort` is set to the same value as the `port` field. - port: 80 targetPort: 80 nodePort: 30080 Ingress\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-service-backend annotations: ingress.gcp.kubernetes.io/pre-shared-cert: \u0026#34;k8s-example-com\u0026#34; kubernetes.io/ingress.allow-http: \u0026#34;false\u0026#34; kubernetes.io/ingress.global-static-ip-name: k8s-example-com spec: defaultBackend: service: name: my-services port: number: 80 rules: - host: k8s.example.com http: paths: - path: /my-service pathType: Prefix backend: service: name: my-service port: number: 80 這樣便能將 Ingress 和 Service 設置完成，Ingress 和 Service 需要在同一個 namespace 。\nASGI \u0026amp; FastAPI 考量到團隊開發大部份依賴 Python framework，因此在替代 Cloud Function HTTP Server 的選擇上，最後我採用了基於 ASGI (Asynchronous Server Gateway Interface) 的 FastAPI ，以應付團隊中除了 Pub/Sub 之外的需求。\n對於 WSGI 和 ASGI 的比較，我覺得這篇博客 WSGI与ASGI的区别与联系 說的很清楚，推薦大家可以看一下。\nFastAPI 的文件中也詳細提供了製作 Container Image 的方法，同時也提到了關於部署在 Kubernetes 上的注意事項，有一份詳細、容易使用的官方文件，也是我選擇 FastAPI 的原因之一，並且 FastAPI 也內建了 Swagger UI 和 ReDoc 兩種文件模式，這也是一個加分大項。\nDockerize \u0026amp; Deployment Dockerfile\n依據 FastAPI 文件提供 Dockerfile 撰寫即可，需注意在 uvicorn 的 command加上 --proxy-headers 。\nFROM python:3.8 WORKDIR / COPY ./requirements.txt /requirements.txt RUN pip install --no-cache-dir --upgrade -r /requirements.txt COPY ./ / CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;main:app\u0026#34;, \u0026#34;--proxy-headers\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;80\u0026#34;] 依需求更改 Dockerfile 時需要注意 Docker Build Cache，由於 Docker Build Image 時會一層一層的往上迭代(每一行指令就是一層)， 而每一次 Build Image 都會檢查與上一次的差異，並從影響差異的 最低層 重新迭代，如: 當 requirements.txt 內容有所變更時，即便 source code 沒有改變，該次的 Docker Build 也會從 COPY ./requirements.txt /requirements.txt` 開始從新迭代。\nMain.py\n在 main.py 提供 domain host 之後的完整 URL path ，讓 app 的 route 可以找到對應的端口，並提供 /my-service/health 給 Load Balancer 進行 health check。\nfrom typing import Optional, Dict from fastapi import (FastAPI, status) from fastapi.encoders import jsonable_encoder from pydantic import BaseModel class Message(BaseModel): attrs: Optional[Dict] = None data: str message_id: str publish_time: str class PubSubMessage(BaseModel): message: Message subscription: str app = FastAPI() [@app](http://twitter.com/app \u0026#34;Twitter profile for @app\u0026#34;).get(\u0026#39;/\u0026#39;, status_code=status.HTTP_200_OK) def home(): pass [@app](http://twitter.com/app \u0026#34;Twitter profile for @app\u0026#34;).get(\u0026#39;/my-service/health\u0026#39;, status_code=status.HTTP_200_OK) def health(): pass @app.post(\u0026#39;/my-service/subscriber-webhook\u0026#39;, status_code=status.HTTP_200_OK) def subscriber_webhook(message: PubSubMessage): message_data: Dict = jsonable_encoder(message) return message_data Deployment\n依據 Kubertenes 官方提供的模板撰寫，再依需求進行更改即可。\napiVersion: apps/v1 kind: Deployment metadata: name: subscriber-webhook-deployment labels: app: subscriber-webhook spec: replicas: 3 selector: matchLabels: app: subscriber-webhook template: metadata: labels: app: subscriber-webhook spec: containers: - name: subscriber-webhook image: {REPLACE_YOUR_REGISTRY}/subscriber-webhook:1.0 ports: - containerPort: 80 可視需要加入 readinessProbe 或 livenessProbe\n如果有 Autoscaling 的需求，參考 Horizontal Pod Autoscaling (HPA) 與 範例 修改即可。\n","permalink":"https://blog.zhengweiliu.com/posts/normal/migrate-google-cloud-functions-to-kubernetes/","summary":"\u003cp\u003e本篇文章是記錄\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#1234\"\u003e用 \u003ccode\u003eKubernetes Pod\u003c/code\u003e 替代 \u003ccode\u003eCloud Function\u003c/code\u003e 環節以處理\u003ccode\u003e動態資料源\u003c/code\u003e的方法\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dee1\"\u003e\u003ccode\u003eGoogle Kubernetes Engine: Ingress \u0026amp; Service\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#fa5e\"\u003e\u003ccode\u003eASGI 與FastAPI\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#b458\"\u003e\u003ccode\u003eDockerize \u0026amp; Deployment\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n","title":"Migrate Google Cloud Functions to Kubernetes"},{"content":"在 [GCP Billing Analytics] 中提到過關於 Cloud Functions 的計費超乎預期，進一步分析開發的使用習慣後，也找出部分功能應該將其從 Cloud Functions 搬遷至基於 GCE instances 的服務上，以達到節費的期望。 GCP Billing Analysis Date: 2021-12-27 \u0026nbsp; Categories: #Google Cloud Platform\u0026nbsp; #Analysis\u0026nbsp; 在產品的開發中，團隊消耗成本最高的前幾項排名既在意料之中，Google Compute Engine (GCE)、 Cloud Functions 、 BigQuery 以及 Google Cloud Storage，但細項的部分也在意料之外。 ...... 在原先的設計中，我們將 Cloud Functions 作為 ETL data flow 的其中一個環節，透過 Pub/Sub trigger Cloud Functions 的方式使其運作；考慮到 Pub/Sub subscriber push/pull 的 Ack 等待時間有著最長 600 秒的限制，我將這部分需要搬遷的 Cloud Functions 大致分為兩種需求\n靜態資料源: 在提取資料時，可預期資料是存在且可被存取的 動態資料源: 可能發生資料不存在，或者是無法存取的情況 本篇文章是記錄\n用 Airflow DAG (Directed Acyclic Graph) 替代 Cloud Function 環節以處理靜態資料源的方法 Airflow GCP Operators 使用 在 DAG 中平行處理(parallel processing)的方式 動態資料源的處理方案 \u0026gt; Migrate Google Cloud Functions to Kubernetes\nDesign Change Figure 1 是一個經典的使用案例，透過 GCS notification 的機制，當 bucket 中有檔案 (Object) 異動時，將異動的資訊 publish 到指定的 Pub/Sub Topic。 部署 Cloud Function 可以指定--trigger-topic 接受 Topic 的觸發，使得 Cloud Function 可以接收異動檔案的資訊，如: bucket name、object path ， 進行轉置 (Transform) 處理後將結果存放到 Big Query 。\n這也是我稱呼為靜態資料源的原因\n由於訊息傳遞的時間相對迅速，當 Cloud Function 需要擷取對應的檔案時，該檔案存在於 GCS 上的對應位置\nAirflow DAG \u0026amp; Operators Figure 2 則用 Airflow 2.0 DAG 替代 Cloud Function ， Airflow 是 Python based 的工作流管理系統，可以幫助開發者將工作流程標準化以及執行重複性的工作，我認為滿適合應用於靜態資料源的場景上。\n並且 Airflow 官方也提供對應 GCP 服務 Operators 的文件 與 安裝方式。可以直接使用，也可以參考 Operators 的 Source Code 來重新編寫自定義的 Operator；如 Figure 2 的 PubSubPullOperator 便能直接使用官方提供的 packages ，而 TransformOperator與 BatchInsertOperator 也可以尋找到對應 operator source code 以進行參考與改寫，以後有機會的話在另外撰寫文章記錄。\nParallel Processing in DAG 在 Figure 2 中改為使用 pull message 的方式，因此可以透過 PubSubPullOperator 來設置每次拉取訊息數量的上限；\n然而，考慮到 Airflow schedule 的最小間隔單位為 1 分鐘，一旦 publish message 的數量與日遽增、或是出現 burst 的情形時，僅憑一組 PubSubPullOperator \u0026gt; TransformOperator \u0026gt; BatchInsertOperator 的工作流程設置可能無法消化；因此就需要考量在 DAG 中建立多組的工作流程，以進行平行處理。\nFigure 3 是我理解 Pub/Sub 拉取訊息的工作原理(若有錯誤也煩請指正，感謝)，訊息的傳遞步驟略可簡述為:\nStep 1. Publish message to Topic\nStep 2. Message push into subscriber group queue by fanout mode\nStep 3. Single/Multi puller to pull message from a subscriber group queue\n這樣一想就比較簡單了，只要在 DAG 中建立多組的 PubSubPullOperator \u0026gt; TransformOperator \u0026gt; BatchInsertOperator 工作流程，每個 PubSubPullOperator 都扮演著 Puller 的角色。\n如 Figure 4 展示的工作流程設置，這樣就能達成平行處理的構想啦! 同時，我也採用了 DummyOperator 作為整個 DAG 的起始與完成，主要是希望在使用 Airflow UI 時能夠比較好的表達 DAG 的工作狀態。\nPython example code for parallel processing in DAG PARALLEL: int = 5 start = DummyOperator(task_id=\u0026#39;start\u0026#39;) complete = DummyOperator(task_id=\u0026#39;complete\u0026#39;) for i in range(PARALLEL): pull_message = PubSubPullOperator(task_id=f\u0026#39;pull_message_{i}\u0026#39;) transform = TramsformOperator(task_id=f\u0026#39;transform_{i}\u0026#39;) insert_to_bq = BatchInsertOperator(task_id=f\u0026#39;insert_to_bq_{i}\u0026#39;) start \u0026gt;\u0026gt; pull_message \u0026gt;\u0026gt; transform \u0026gt;\u0026gt; insert_to_bq \u0026gt;\u0026gt; complete 另外，在能估算出單位時間 publish 的 message 數量，便能簡單地將 schedule 間隔時間、單次拉取訊息的數量上限，以及工作流程組數視為調整參數，以調整工作流程的處理效率。\n","permalink":"https://blog.zhengweiliu.com/posts/normal/migrate-google-cloud-functions-to-airflow/","summary":"\u003cp\u003e本篇文章是記錄\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#a63a\"\u003e用 \u003ccode\u003eAirflow DAG\u003c/code\u003e (Directed Acyclic Graph) 替代 \u003ccode\u003eCloud Function\u003c/code\u003e 環節以處理\u003ccode\u003e靜態資料源\u003c/code\u003e的方法\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#7077\"\u003e\u003ccode\u003eAirflow GCP Operators\u003c/code\u003e 使用\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#0481\"\u003e在 \u003ccode\u003eDAG 中平行處理(parallel processing)\u003c/code\u003e的方式\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n","title":"Migrate Google Cloud Functions to Airflow"},{"content":"最近利用 GA4 、 UA ，以及團隊的開發產品所蒐集到的資料，協助團隊進一步了解產品的成效與成本的利用情況。團隊的開發與產品環境皆建立在 Google Cloud Platform (GCP) 上，在分析 GCP billing report 的原始資料時，也引發了我 \u0026ldquo;對於同仁們對於如何利用開發環境\u0026rdquo; 感到好奇，寫下這篇文章作為紀錄。\n在產品的開發中，團隊消耗成本最高的前幾項排名既在意料之中，Google Compute Engine (GCE)、 Cloud Functions 、 BigQuery 以及 Google Cloud Storage，但細項的部分也在意料之外。\nGoogle Compute Engine (GCE) 在 GCP 上，無論我們開啟的是一般的 VM 機器，又或者是 Google Kubernetes Engine (GKE) 的 Node ， 本身所使用的資源單位都可以稱為 Instance ； 換句話說，可以簡單的將 Instance 理解為能夠提供絕大部分 VM 相關功能的資源，如 : vCPU、Memory、Disk、Netwroking 以及機器學習最需要的 GPU (TPU)等等，因此這一部份的資源用量也都會被歸因到 GCE 上。\n將 billing report data 依據 SKU 進行加總並命名為 「Cost」欄位，再對 「Cost」欄位做 kernel density estimation (kde) 後可以得到 「Cost」的群聚密度，同時也能獲取一組較為合理的上下邊界以利取得離群值，「Cost」的離群值對於 billing report 的意義則在於找出異常的費用；以 下將固定使用 kde 取離群值的作法，因此不再一一贅述。\n從離群值得知，Instance Core 、 Instance Ram 以及 GPU 的費用都是比較可觀的\nInstance: 依據 Figure 1. GCE charged detail 給出的資訊，Instance 分為 Custom 與 N1-Predefined 兩種類別，這兩種類別在團隊中分別作為 GKE Node 與 GCE instance 來使用。依據 Google 在 GCE 定價的文件中可以得知， 1. Instance 的 CPU 與 Memory 是分別以 \u0026ldquo;running time\u0026rdquo; 進行收費， 2. custom machine type 會比 predefined machine type 收取更多費用。\n觀察 Custom Instance Core 、 N1 Predefined Instance Core 以及 N1 Predefined Instance Ram 的堆疊圖也可以發現，三者在 8 月至 11 月的費用並沒有出現 burst peak ， 反而在變化上呈現相對平滑的狀況；對於開發團隊來說，這其實不是一個正常的表現: 有限的人力伴隨著開發迭代週期，會出現大量使用 CPU 計算以驗證 feature 的開發情況，也會進行伴隨著壓力測試出現大量載入資料迫使 Memory 使用量增加的情況。\n因此，最可能的情況其實是: 團隊使用了超過需求量的資源。因為供過於求，導致收費並沒有發生變化，尤其是 GKE Node 應該要有卻沒有呈現的 auto-scaling 效益，最終的結論便是資源溢出造成的浪費，我們也在發現後的第一時間即時做出調整與改善。\nGPU: 團隊所開發的產品 ADsvantage | AI 智慧寫手 是一款 AI 智能廣告工具，24 小時智能監控，讓你不必隨時在線，AI 幫你顧廣告， 因此需要 GPU 來訓練 model 以及應用也是很合理的事情 (防不勝防，自己的業配自己寫XD)\nCloud Functions 和 GCE Instance 收費相同， Cloud Functions 也是以 CPU 和 Memory 的 \u0026ldquo;running time\u0026rdquo; 分別進行收費；差別在於 GCE 收費是以 Hour 作單位，而 Cloud Functions 則是以 100 毫秒(ms) 作為計費單位；即使 Cloud Function 調用 (Invocations) 次數達到千萬次，對於調用的收費也遠遠小於 CPU 和 Memory。\n這邊也多提一句，千萬不要把 Cloud Functions 當作 API 來使用， Cloud Functions 有它適合的場景，但顯然不是\u0026quot;永保在線\u0026quot;的服務。\nBigQuery Long-Term Storage 與 Active Storage 的識別條件: 超過90天沒有 modify / 90 天內仍有 modify 的資料表； Analysis 則是相對直覺的 Query 費用。\nFigure 3. BigQuery charged detail 告訴我們，目前開發環境中的 BigQuery 有太多 Long-Term 的資料被儲存著，這部分有屬於 machine learning 的 train data set ，當然也有太久沒有使用過的資料；同時資料的多寡也影響了 Analysis 每次的收費，因此我們對資料集進行了一次評估與審核，剔除掉已不再需要的資料，以期節省費用。\nGoogle Cloud Storage (GCS) GCS 提供了可對 儲存桶(bucket) 內的檔案 (objects) 實施 生命週期(life-cycle)管理的功能: 透過規則的設定，可以將符合規則天數的 objects 從 standard (nearline / coldline) 等級變更為 nearline、coldline 以及 archive 等級，各等級有不同的收費標準。如: archive 儲存的費用相對最低，但 access 的費用最高， standard 的儲存收費相對最高，但不收取 access 的費用 (如果有產生 traffic 則有可能會進行 bandwidth 的收費)。\n在過往的經歷中，即使資料已經沒有被使用到了仍然會\u0026quot;習慣性\u0026quot;的將其進行封存，以待某天會再度使用。然而多的是，我不知道的 archive 一直在被 access 的事\u0026hellip;\n想當然是馬上對 objects 進行盤點，並加入到資料審核與剃除的流程中啦!!\nSummary 經過這次的分析，也確實找出很多以往在開發中總不經意忽略的小事，然而正是這些最重要的小事，在收費上卻往往變成了大事。\n與一起踩過雷的同行共勉之，也希望這次的分析能夠對於日後使用雲端平台資源更加警慎。\n","permalink":"https://blog.zhengweiliu.com/posts/normal/gcp-billing-analytics/","summary":"在產品的開發中，團隊消耗成本最高的前幾項排名既在意料之中，Google Compute Engine (GCE)、 Cloud Functions 、 BigQuery 以及 Google Cloud Storage，但細項的部分也在意料之外。","title":"GCP Billing Analysis"},{"content":"既上次發布 [Google Certified 與 Cloud]後，和 Ryan 討論人流偵測系統中的資料流，以及感測設備是否存活的議題； Ryan 的工作背景是 Compute Vision 相關，相對於 ETL 資料處理流程中屬於提供 E ( extract ) 端服務的角色，也特別重視 extract 的功能是否都能如期發揮作用。 Google Certified 與 Cloud Date: 2021-09-22 \u0026nbsp; Categories: #Google Cloud Platform\u0026nbsp; #Certified\u0026nbsp; 透過幾個問題的交流過程，記錄我對使用雲端平台以及上雲這件事情的想法 拿認證對工作實戰的幫助以及對職涯的幫助，還是說有使用經驗其實不一定要拿認證，以實用性來說是不是熟悉其中幾項服務就足夠了 ? 考取認證僅證明你確實理解官方在這張認證領域上所提出的 Best Practice，並且具備將其轉換應用到實務上的基礎能力, 推薦的學習路徑和學習資源 ：會建議先去拿助理認證，還是可以直衝專家認證 ? ...... ETL | ELT 是流程還是系統 ?\nETL ( Extract-Transform-Load ) 與 ELT ( Extract-Load-Transform ) 是資料處理中常見的處理流程代名詞；個人認為 ETL ≠ ELT ， L | T 的先後順序除了影響處理流程的腳本之外，其實也需要搭配 scenario 來一起討論，同時也可能需要依賴應用系統的受眾群體特徵，搭建出對應的處理框架，以期在合理的效能下達成提供資料的目的。\n在上述過程中可以看出，ETL | ELT 會依據實際狀況而對於框架設計有所改變， ETL | ELT 應屬於流程，在實作完成後才會變成具體的系統；而流程則可以被獨立提出進行討論。\nExtract 是否有在好好運作 ? 資料遺失是否可以避免 ?\n在 Ryan 提出的議題中，extract 的服務由具體的感應偵測設備產生 log 資料，並不斷的往後段進行傳送，以便進行分析或儲存；當 extract device 離線或者是發生故障，若沒有在第一時間進行確認與通知相關人員，往往要等到進行資料統計時才會發現資料遺失。\n為此，主動進行 Check Sensor Is Alive 的機制看起來不可避免，或是有其他的途徑可以達成相同的目的呢 ?\nExtract Device 的資訊屬於已知或者未知 ?\n回到人流偵測的場景中，我本身對於這個應用場景較為陌生，因此參考了《基於影像處理及深度學習的兩階段人流偵測系統》[1]\n傳統計算人流的方法可能會在入口處利用計數 器手動計算，或者透過閘門式機械設備逐一計算，常 見的方法為紅外線感應或旋轉門計數，但是對於公車 候車亭等開放式、半開放式的場域而言，缺少固定入 口來協助逐一計算，因此本研究利用影像處理方法達 到智慧監控的目的。\n透過影像處理方法，對 Camera 蒐集到的畫面特徵擷取並將 ROI ( Region of interest ) 作為機器學習的 input data 之一；從這段描述中可以分析出兩個先決條件\n1\\. Camera 作為蒐集影像的設備，對應了 extract device 的角色\n2\\. Camera 的部署地點是已知條件，換句話說 : extract device 已紀錄在案\n已知條件的 extract device 在處理上會相對便利：由於已經確認該 device 有週期性或者需要不間斷的蒐集資料並回傳，透過設計 Slots 的方式來定期收取資料，並給予一些容許值；對於超出容許值的 device 或許就能夠先進行 Is Alive 的判斷機制，並盡早的通知相關人員或進行對應的處理。\n為什麼需要容許值 ?\n為了方便設計 ETL | ELT ，在設計的最初通常會假設 extract device 蒐集並提供的資料量是 100% ，即沒有任何資料遺失。然而 100% 的資料傳遞表示實作好的系統中，各個環節都不會有任何意外狀況發生，如：extract device 永遠不故障或者不需要汰舊換新、偵測區域的地點永遠不會有施工與環境物變更、永遠都有人流經過 ROI或是 ROI 永遠不變更\u0026hellip;等，往往在實際操作的經驗上，都會面臨因各種調整事件而造成的 incident 。\n因此，透過討論達成共識並給定一個容許值，可以讓整個系統在達成原先設立目的的同時具備一些彈性，並可以在後續的處理 ( Process ) 上考慮加入容許值條件以進行調整。\nCloud IoT Core \u0026amp; Monitoring\nGCP ( Google Cloud Platform ) 上提供了 Cloud IoT Core 的服務，讓 Device 可以直接或者間接將資料回傳至 GCP resources ，並且也提供了 monitoring 的功能來檢視 resource ，包含已記錄在案的 device ， 並提供視覺化的圖形報表以方便檢視功能運作的狀況，如：uptime check。\nAWS 、Azure 或是其他雲端平台可能也有提供相對應的功能組合或者服務，若沒有頭緒的話可以參考 GCP 給出的範例，並嘗試在不同的雲平台中討論合適的解決方案與架構。\nReference [1] 基於影像處理及深度學習的兩階段人流偵測系統, 林泓邦 1 *、林仁信 2 、廖伯翔 3, 中華民國自動機工程學會第二十五屆車輛工程學術研討會論文集, 中華民國一百零九年十月三十日\n","permalink":"https://blog.zhengweiliu.com/posts/normal/etl-elt-iot-device-alive-check/","summary":"在產品的開發中，團隊消耗成本最高的前幾項排名既在意料之中，Google Compute Engine (GCE)、 Cloud Functions 、 BigQuery 以及 Google Cloud Storage，但細項的部分也在意料之外。","title":"ETL | ELT 與 IoT Device Alive Check"},{"content":"轉換到雲端領域工作也過了大半年，這段不算長且還在進行中旅程中也獲取了三張 Google Cloud Platform ( GCP ) 的 Certified : Associate Cloud Engineer | Professional Cloud Architect | Professional Cloud Network Engineer\n每每在考取認證的當下，也試著將這份喜悅分享給社群好友，也因此成為了開啟與好友交流雲端使用經驗的契機。\n最近，和 Enzo 聊到在工作領域深耕的話題。Enzo 對資料科學的領域具有高度熱忱，也希望朝著 Senior Data Engineer 的角色發展；目前對於 Senior Data Engineer 的專業需求中，經常看到需要具備雲端平台的服務或工具等使用經驗；Enzo 除了使用中的 Google Compute Engine ( GCE ) Virtual Machine 服務之外，也希望進一步了解自學 GCP 的必要性與可能性，同時透過考取認證的方式確認自己學習的成果，以及希望將其作為對外證明的一舉兩得好方式。\n和 Enzo 交流討論的過程中，我也從中發現一些值得紀錄的觀點。無論未來的我對這個觀點是抱持著贊同的態度，也或者大相徑庭，都是一種值得回味的思考。\n以下透過幾個問題的交流過程，記錄我對使用雲端平台以及上雲這件事情的想法\n拿認證對工作實戰的幫助以及對職涯的幫助，還是說有使用經驗其實不一定要拿認證，以實用性來說是不是熟悉其中幾項服務就足夠了 ?\n當初考慮轉換工作領域時，我也曾思考過這個問題；再陸續考取認證的過程中，也找到了一個自己認為合適的答案。\n考取認證僅證明你確實理解官方在這張認證領域上所提出的 Best Practice，並且具備將其轉換應用到實務上的基礎能力\n換句話說 : 認證是一個敲門磚。\n對外來說確實也是一個不錯的證明，面對非相同專業領域的人而言，這也代表了官方的背書。\n推薦的學習路徑和學習資源 ：會建議先去拿助理認證，還是可以直衝專家認證 ?\n因為長期使用 GCE 的經驗，促使 Enzo 希望從 GCP 的認證作為起步。\n對於希望學習 GCP 的人而言， Associate Cloud Engineer ( ACE ) 確實是幫助入門 Cloud 的好途徑：由於 Cloud 的資源眾多，若希望執行搬遷上雲的計畫， ACE 會以較為具體的執行方案帶你領略各個 GCP 資源的使用藍圖以及限制。\n舉 Data 相關的工作為例，個人淺見認為不管從事的工作內容是 資料科學家 | 資料分析師 | 資料工程師 ，都無可避免的需要明確的知道資料從哪裡來、資料格式長相如何，以及處理過的資料最終需要往哪邊去。\n因此，可以得到一個較為直覺的論點\n資料流 ( Dataflow ) 與 資料處理 ( Process ) 可以單獨存在與單獨討論\n在過往的開發工作進行時，通常所使用的設備資源都是完整可見的硬體設施，而我們在這些設施上完成相關的開發作業；後來，當發現各家雲平台都有提供 Virtual Machine 的資源時，最直覺的上雲就是將工作搬往雲平台的 Virtual Machine ，藉此省下的硬體設施的費用。\n至此，如果你問我使用雲平台的 Virtual Machine 算不算上雲了，我的回答是看狀況\n雲端與地端的區別在哪裡 ?\n若我們以最廣為人知的定義來討論雲端與地端，則地端屬於公司內部設備，使用內部網路來隔離與外部網路的接觸，並透過層層保護設施來保護內部服務與資料不會輕易的被未授權的使用者存取；\n而雲端則是使用雲平台提供同等硬體設施的虛擬資源，如： GCE、GKE，或者是雲平台提供的虛擬網路定義，如：VPC ( Virtual Private Cloud ) ，同時雲平台也提供底層的封包加解密、密鑰管理以及 Security Command Center 等等服務。\n在使用雲平台的經驗中，可以省下大量需要人力進行底層設施 ( Infrastructure ) 更換以及佈署的時間。\n然而，如果我們用雲平台的資源，做著與地端時期相同的工作，真的就是上雲了嗎 ?\n硬體上雲與服務上雲\n在智慧手機不如現在功能強大的十幾年前，我們大多在個人電腦上使用社群媒體來與朋友進行交流；而現在，我們利用智慧手機來刷社群媒體的時間可能比坐在電腦前的時間還多。\n誠然，我們將地端時期的工作搬遷到雲平台上，可以節省硬體設施等相關費用，但工作上的思考仍然停留在地端時期，我們只是將工作搬遷到另外一個環境上而已。\n同樣是以 Dataflow 與 Process 的角度來看\n地端時期，資料流中的每個端點，如： NAS、SFTP 或是其他伺服器服務，都是明確的執行在具體一台伺服器設備，或是特定的叢集伺服器機群；這樣明確的設備端到設備端的資料流向也是地端時期的明顯特徵之一。\n雲平台提供了 Virtual Machine 的資源，提供了從地端上雲的可能性；然而，我們會發現在這個模式的運作下， Dataflow 與 Process 仍然依賴著設備端到設備端的模式。\n還記得前面提到的 資料流 ( Dataflow ) 與 資料處理 ( Process ) 可以單獨存在與單獨討論 嗎 ? 這表示它們其實並不需要依賴特定的設備端，明確地說\n並不僅僅只能在 Virtual Machine 上實現\n眾多服務為何只取 Virtual Machine ?\n雲平台提供了眾多的資源，幾乎可以滿足絕大部分的資料流設計與脫離 Virtual Machine 執行 Process 的方式，即 serverless，以下列舉幾個雲平台中與 Data 相關的資源\nData Storage Google Cloud Storage 提供 Bucket 資源可以儲存大量文件資料 Google Cloud Pub/Sub 提供 MQTT 服務，作為資料流渠道角色，同時也具備儲存批次資料的能力 Firebase 提供了最適合 Web 與 Mobile App 需要的儲存空間 Serverless Cloud Functions 提供了 serverless 的服務，適用於執行微服務 function、 stateless function 或是單純的 API endpoints Data Process Dataprep 提供了視覺化與簡易編輯的 Transform 功能，也就是 ETL | ELT 中經常被提到的 “ T ” Dataflow 提供了整個資料流的編排功能，可以透過視覺化工具來編排資料流的來源、處理以及目的地；同時也支援將雲平台中與資料相關的資源作為來源端或目的地端，也支援從其他雲平台引入資料 雲端一角\n至此便能總結出幾項要點，用以描述我對雲端的觀點\n將 workload 抽離對於資源的依賴性；workload 應可被單獨設計與討論 雲端資源的提供者並非僅有一家。雲端與地端都能作為資料流來源，同時也能作為資料流去處 節省硬體與底層資源的佈建佈署等成本 資料的尾巴\n為什麼通篇都以資料流作為舉例，並不斷地提起呢 ?\nEvery company is a data company 中給出了很好的解釋，有空的話可以看看\n","permalink":"https://blog.zhengweiliu.com/posts/normal/google-certified-cloud/","summary":"透過幾個問題的交流過程，記錄我對使用雲端平台以及上雲這件事情的想法\n拿認證對工作實戰的幫助以及對職涯的幫助，還是說有使用經驗其實不一定要拿認證，以實用性來說是不是熟悉其中幾項服務就足夠了 ?\n考取認證僅證明你確實理解官方在這張認證領域上所提出的 Best Practice，並且具備將其轉換應用到實務上的基礎能力,\n推薦的學習路徑和學習資源 ：會建議先去拿助理認證，還是可以直衝專家認證 ?","title":"Google Certified 與 Cloud"}]