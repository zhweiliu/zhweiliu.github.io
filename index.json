[{"content":"Description Five silent philosophers sit at a round table with bowls of spaghetti. Forks are placed between each pair of adjacent philosophers.\nEach philosopher must alternately think and eat. However, a philosopher can only eat spaghetti when they have both left and right forks. Each fork can be held by only one philosopher and so a philosopher can use the fork only if it is not being used by another philosopher. After an individual philosopher finishes eating, they need to put down both forks so that the forks become available to others. A philosopher can take the fork on their right or the one on their left as they become available, but cannot start eating before getting both forks.\nEating is not limited by the remaining amounts of spaghetti or stomach space; an infinite supply and an infinite demand are assumed.\nDesign a discipline of behaviour (a concurrent algorithm) such that no philosopher will starve; i.e., each can forever continue to alternate between eating and thinking, assuming that no philosopher can know when others may want to eat or think.\nThe problem statement and the image above are taken from wikipedia.org\nThe philosophers’ ids are numbered from 0 to 4 in a clockwise order. Implement the function void wantsToEat(philosopher, pickLeftFork, pickRightFork, eat, putLeftFork, putRightFork) where:\nphilosopher is the id of the philosopher who wants to eat. pickLeftFork and pickRightFork are functions you can call to pick the corresponding forks of that philosopher. eat is a function you can call to let the philosopher eat once he has picked both forks. putLeftFork and putRightFork are functions you can call to put down the corresponding forks of that philosopher. The philosophers are assumed to be thinking as long as they are not asking to eat (the function is not being called with their number). Five threads, each representing a philosopher, will simultaneously use one object of your class to simulate the process. The function may be called for the same philosopher more than once, even before the last call ends.\nIdea The Dining philosophers problem.\nIn computer science, the dining philosophers problem is an example problem often used in concurrent algorithm design to illustrate synchronization issues and techniques for resolving them.\nFocus on the forks instead of philosophers, because forks are necessary resources if philosopher would like to eat food.\nI used a list to put 5 lock, each lock indicates a fork, let philosopher id + 1 as their left-hand, philosopher id as their right-hand.\nTake the pickup left-hand’s fork first because it’s have a higher number , and put down right fork first because it a higher number fork for right side philosopher.\nSolution from threading import Condition class DiningPhilosophers: def __init__(self) -\u0026gt; None: self._forks = [Condition()] * 5 # call the functions directly to execute, for example, eat() def wantsToEat(self, philosopher: int, pickLeftFork: \u0026#39;Callable[[], None]\u0026#39;, pickRightFork: \u0026#39;Callable[[], None]\u0026#39;, eat: \u0026#39;Callable[[], None]\u0026#39;, putLeftFork: \u0026#39;Callable[[], None]\u0026#39;, putRightFork: \u0026#39;Callable[[], None]\u0026#39;) -\u0026gt; None: with self._forks[(philosopher+1)%5], self._forks[philosopher]: pickLeftFork() pickRightFork() eat() putRightFork() putLeftFork() ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/concurrency/the-dining-philosophers/","summary":"Description Five silent philosophers sit at a round table with bowls of spaghetti. Forks are placed between each pair of adjacent philosophers.\nEach philosopher must alternately think and eat. However, a philosopher can only eat spaghetti when they have both left and right forks. Each fork can be held by only one philosopher and so a philosopher can use the fork only if it is not being used by another philosopher.","title":"[leetcode][Python][Concurrency][Medium] 1226. The Dining Philosophers"},{"content":"Description You have the four functions:\nprintFizz that prints the word \u0026quot;fizz\u0026quot; to the console, printBuzz that prints the word \u0026quot;buzz\u0026quot; to the console, printFizzBuzz that prints the word \u0026quot;fizzbuzz\u0026quot; to the console, and printNumber that prints a given integer to the console. You are given an instance of the class FizzBuzz that has four functions: fizz, buzz, fizzbuzz and number. The same instance of FizzBuzz will be passed to four different threads:\nThread A: calls fizz() that should output the word \u0026quot;fizz\u0026quot;. Thread B: calls buzz() that should output the word \u0026quot;buzz\u0026quot;. Thread C: calls fizzbuzz() that should output the word \u0026quot;fizzbuzz\u0026quot;. Thread D: calls number() that should only output the integers. Modify the given class to output the series [1, 2, \u0026quot;fizz\u0026quot;, 4, \u0026quot;buzz\u0026quot;, ...] where the ith token (1-indexed) of the series is:\n\u0026quot;fizzbuzz\u0026quot; if i is divisible by 3 and 5, \u0026quot;fizz\u0026quot; if i is divisible by 3 and not 5, \u0026quot;buzz\u0026quot; if i is divisible by 5 and not 3, or i if i is not divisible by 3 or 5. Implement the FizzBuzz class:\nFizzBuzz(int n) Initializes the object with the number n that represents the length of the sequence that should be printed. void fizz(printFizz) Calls printFizz to output \u0026quot;fizz\u0026quot;. void buzz(printBuzz) Calls printBuzz to output \u0026quot;buzz\u0026quot;. void fizzbuzz(printFizzBuzz) Calls printFizzBuzz to output \u0026quot;fizzbuzz\u0026quot;. void number(printNumber) Calls printnumber to output the numbers. Idea For example\nInput: n = 15 Output: [1,2,\u0026#34;fizz\u0026#34;,4,\u0026#34;buzz\u0026#34;,\u0026#34;fizz\u0026#34;,7,8,\u0026#34;fizz\u0026#34;,\u0026#34;buzz\u0026#34;,11,\u0026#34;fizz\u0026#34;,13,14,\u0026#34;fizzbuzz\u0026#34;] Basically, I guess it could be used Condition or Lock to solve this question, but its could be bring about not easily to read for the solution.\nAfter study the discussion with other contributors, I agree to use Python threading.Semaphore to solve this question.\nThe Semaphore introduce on official documentation as :\nA semaphore manages an atomic counter representing the number of release() calls minus the number of acquire() calls, plus an initial value. The acquire() method blocks if necessary until it can return without making the counter negative. If not given, value defaults to 1. We can create Semaphore objects for fizz, buzz, fizzbuzz and numbers. And use the for-loops setup their runtimes with fit conditions to n .\nThe semaphore initial values are 0 for fizz, buzz, fizzbuzz, but setup the semaphore initial value 1 for numbers, because we know the serial start with a number, 1 to n , and all conditions of fizz, buzz, fizzbuzz requires divisible by number ,at least 3 , it will help the function number to print numbers without blocking.\nSolution from threading import Semaphore class FizzBuzz: def __init__(self, n: int): self.n = n self._lock_fz = Semaphore(0) self._lock_bz = Semaphore(0) self._lock_fzbz = Semaphore(0) self._lock_num = Semaphore(1) # printFizz() outputs \u0026#34;fizz\u0026#34; def fizz(self, printFizz: \u0026#39;Callable[[], None]\u0026#39;) -\u0026gt; None: for i in range(self.n//3-self.n//15): self._lock_fz.acquire() printFizz() self._lock_num.release() # printBuzz() outputs \u0026#34;buzz\u0026#34; def buzz(self, printBuzz: \u0026#39;Callable[[], None]\u0026#39;) -\u0026gt; None: for i in range(self.n//5-self.n//15): self._lock_bz.acquire() printBuzz() self._lock_num.release() # printFizzBuzz() outputs \u0026#34;fizzbuzz\u0026#34; def fizzbuzz(self, printFizzBuzz: \u0026#39;Callable[[], None]\u0026#39;) -\u0026gt; None: for _ in range(self.n//15): self._lock_fzbz.acquire() printFizzBuzz() self._lock_num.release() # printNumber(x) outputs \u0026#34;x\u0026#34;, where x is an integer. def number(self, printNumber: \u0026#39;Callable[[int], None]\u0026#39;) -\u0026gt; None: for i in range(1, self.n+1): self._lock_num.acquire() if i%3==0 and i%5==0: self._lock_fzbz.release() elif i%3==0: self._lock_fz.release() elif i%5==0: self._lock_bz.release() else: printNumber(i) self._lock_num.release() ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/concurrency/fizz-buzz-multithreaded/","summary":"Description You have the four functions:\nprintFizz that prints the word \u0026quot;fizz\u0026quot; to the console, printBuzz that prints the word \u0026quot;buzz\u0026quot; to the console, printFizzBuzz that prints the word \u0026quot;fizzbuzz\u0026quot; to the console, and printNumber that prints a given integer to the console. You are given an instance of the class FizzBuzz that has four functions: fizz, buzz, fizzbuzz and number. The same instance of FizzBuzz will be passed to four different threads:","title":"[leetcode][Python][Concurrency][Medium] 1195. Fizz Buzz Multithreaded"},{"content":"Description Suppose you are given the following code:\nclass FooBar { public void foo() { for (int i = 0; i \u0026lt; n; i++) { print(\u0026#34;foo\u0026#34;); } } public void bar() { for (int i = 0; i \u0026lt; n; i++) { print(\u0026#34;bar\u0026#34;); } } } The same instance of FooBar will be passed to two different threads:\nthread A will call foo(), while thread B will call bar(). Modify the given program to output \u0026quot;foobar\u0026quot; n times.\nIdea An example for output\nInput: n = 2 Output: \u0026#34;foobarfoobar\u0026#34; Explanation: \u0026#34;foobar\u0026#34; is being output 2 times. Using a flag to switch printFoo() and printBar() when acquire the lock.\nSolution from threading import Condition class FooBar: def __init__(self, n): self.n = n self._lock = Condition() self._is_printed_foo = False def foo(self, printFoo: \u0026#39;Callable[[], None]\u0026#39;) -\u0026gt; None: for i in range(self.n): with self._lock: while self._is_printed_foo: self._lock.wait() # printFoo() outputs \u0026#34;foo\u0026#34;. Do not change or remove this line. printFoo() self._is_printed_foo = True self._lock.notify_all() def bar(self, printBar: \u0026#39;Callable[[], None]\u0026#39;) -\u0026gt; None: for i in range(self.n): with self._lock: while not self._is_printed_foo: self._lock.wait() # printBar() outputs \u0026#34;bar\u0026#34;. Do not change or remove this line. printBar() self._is_printed_foo = False self._lock.notify_all() ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/concurrency/print-foobar-alternately/","summary":"Description Suppose you are given the following code:\nclass FooBar { public void foo() { for (int i = 0; i \u0026lt; n; i++) { print(\u0026#34;foo\u0026#34;); } } public void bar() { for (int i = 0; i \u0026lt; n; i++) { print(\u0026#34;bar\u0026#34;); } } } The same instance of FooBar will be passed to two different threads:\nthread A will call foo(), while thread B will call bar(). Modify the given program to output \u0026quot;foobar\u0026quot; n times.","title":"[leetcode][Python][Concurrency][Medium] 1115. Print FooBar Alternately"},{"content":"asyncio is a library to write concurrent code using the async/await syntax. ---- from Python3.11.1 documentation This article is write down the note with my study of python asyncio package.\nHow does asyncio work ? The main process, which is start run by IDE or command line, have a main thread to execute submit a coroutine to asyncio event loops by asyncio.create_task() or asyncio.run() , the keyword async will packet methods as coroutine. The event loops will monitoring all of the submit task, and choice a not finished, current can going on coroutine to execute its task until it finish, or change status to wait when meet await . When event loops meet await , you should be notify (or notify all) task(s) which status is waiting for blocking, and check the blocking condition is still exist or not. Repeating above step 2 and step 3 until no more coroutines in asyncio event loops (i.e. all of the coroutine will be finish or canceled). async \u0026amp; await Using async to create a coroutine method Using await to call another coroutine _B_ in coroutine _A_.\n_A_ will into wait status until _B_ execute finish and notify. Using asyncio.run() to submit a coroutine Create Task \u0026amp; Submit Coroutine The asyncio.create_task() will submit a coroutine into Task Queue When await _task_1_ be execute, main thread will keep waiting until _task_1_ finish/await, and so on _task_2_ It have concurrency effect like as multi threading( or multi processing ) Timeout \u0026amp; Cancel Using Task.done() to determine a task is finish or not yet. Using Task.cancel() to cancel a task which is executing. Using asyncio.wait_for(task, timeout=wait_duration) for automate cancel a task if execute timeout Sometimes, we want the task keep going on their work until finish, and I just would like to know the task will happened timeout or not. For example: Counting the times of timeout to calculate performance\nGather multi task Using asyncio.gather(task1, task2, …, taskN) Add parameter return_exceptions = True capture exception result ","permalink":"https://blog.zhengweiliu.com/posts/normal/python3-asyncio/","summary":"asyncio is a library to write concurrent code using the async/await syntax.\nHow does \u003cem\u003easyncio\u003c/em\u003e work ?\nasync \u0026amp; await,\u003cbr\u003e\nCreate Task \u0026amp; Submit Coroutine,\nTimeout \u0026amp; Cancel,\nGather multi task\u0026hellip;","title":"Python3 - asyncio"},{"content":"Description Implement a thread-safe bounded blocking queue that has the following methods:\nBoundedBlockingQueue(int capacity) The constructor initializes the queue with a maximum capacity. void enqueue(int element) Adds an element to the front of the queue. If the queue is full, the calling thread is blocked until the queue is no longer full. int dequeue() Returns the element at the rear of the queue and removes it. If the queue is empty, the calling thread is blocked until the queue is no longer empty. int size() Returns the number of elements currently in the queue. Please do not use built-in implementations of bounded blocking queue as this will not be accepted in an interview.\nIdea Your implementation will be tested using multiple threads at the same time. Each thread will either be a producer thread that only makes calls to the enqueue method or a consumer thread that only makes calls to the dequeue method. The size method will be called after every test case.\nInput: 3 4 [\u0026#34;BoundedBlockingQueue\u0026#34;,\u0026#34;enqueue\u0026#34;,\u0026#34;enqueue\u0026#34;,\u0026#34;enqueue\u0026#34;,\u0026#34;dequeue\u0026#34;,\u0026#34;dequeue\u0026#34;,\u0026#34;dequeue\u0026#34;,\u0026#34;enqueue\u0026#34;] [[3],[1],[0],[2],[],[],[],[3]] Output: [1,0,2,1] Explanation: Number of producer threads = 3 Number of consumer threads = 4 BoundedBlockingQueue queue = new BoundedBlockingQueue(3); // initialize the queue with capacity = 3. queue.enqueue(1); // Producer thread P1 enqueues 1 to the queue. queue.enqueue(0); // Producer thread P2 enqueues 0 to the queue. queue.enqueue(2); // Producer thread P3 enqueues 2 to the queue. queue.dequeue(); // Consumer thread C1 calls dequeue. queue.dequeue(); // Consumer thread C2 calls dequeue. queue.dequeue(); // Consumer thread C3 calls dequeue. queue.enqueue(3); // One of the producer threads enqueues 3 to the queue. queue.size(); // 1 element remaining in the queue. Since the number of threads for producer/consumer is greater than 1, we do not know how the threads will be scheduled in the operating system, even though the input seems to imply the ordering. Therefore, any of the output [1,0,2] or [1,2,0] or [0,1,2] or [0,2,1] or [2,0,1] or [2,1,0] will be accepted. I guess the blocking means a task cannot going on its work when some condition cannot fit.\nIn this question, I need to design a bounded blocking queue, the queue have a capacity that means :\nCannot enqueue if queue have no remain space for element Can not dequeue when no more element in the queue The block happened when meet above situation.\nWhile the thread acquire lock, thread must be detect currently status of queue :\nWaiting for next time notify if queue have not remaining space for enqueue Waiting for next time notify if queue have not any element for dequeue Solution from threading import Condition class BoundedBlockingQueue(object): def __init__(self, capacity: int): self.__capacity = capacity self.__lock = Condition() self.__queue = list() def enqueue(self, element: int) -\u0026gt; None: with self.__lock: while self.size() == self.__capacity: self.__lock.wait() self.__queue.insert(0, element) self.__lock.notify_all() def dequeue(self) -\u0026gt; int: ret = None with self.__lock: while self.size() == 0: self.__lock.wait() ret = self.__queue.pop() self.__lock.notify_all() return ret def size(self) -\u0026gt; int: return len(self.__queue) ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/concurrency/design-bounded-blocking-queue/","summary":"Description Implement a thread-safe bounded blocking queue that has the following methods:\nBoundedBlockingQueue(int capacity) The constructor initializes the queue with a maximum capacity. void enqueue(int element) Adds an element to the front of the queue. If the queue is full, the calling thread is blocked until the queue is no longer full. int dequeue() Returns the element at the rear of the queue and removes it. If the queue is empty, the calling thread is blocked until the queue is no longer empty.","title":"[leetcode][Python][Concurrency][Medium] 1188. Design Bounded Blocking Queue"},{"content":"Description Given a URL startUrl and an interface HtmlParser, implement a Multi-threaded web crawler to crawl all links that are under the same hostname as startUrl.\nReturn all URLs obtained by your web crawler in any order.\nYour crawler should:\nStart from the page: startUrl Call HtmlParser.getUrls(url) to get all URLs from a webpage of a given URL. Do not crawl the same link twice. Explore only the links that are under the same hostname as startUrl. As shown in the example URL above, the hostname is example.org. For simplicity\u0026rsquo;s sake, you may assume all URLs use HTTP protocol without any port specified. For example, the URLs http://leetcode.com/problems and http://leetcode.com/contest are under the same hostname, while URLs http://example.org/test and http://example.com/abc are not under the same hostname.\nThe HtmlParser interface is defined as such:\ninterface HtmlParser { // Return a list of all urls from a webpage of given url. // This is a blocking call, that means it will do HTTP request and return when this request is finished. public List\u0026lt;String\u0026gt; getUrls(String url); } Note that getUrls(String url) simulates performing an HTTP request. You can treat it as a blocking function call that waits for an HTTP request to finish. It is guaranteed that getUrls(String url) will return the URLs within 15ms. Single-threaded solutions will exceed the time limit so, can your multi-threaded web crawler do better?\nIdea Below are two examples explaining the functionality of the problem. For custom testing purposes, you’ll have three variables urls, edges and startUrl. Notice that you will only have access to startUrl in your code, while urls and edges are not directly accessible to you in code.\nInput: urls = [ \u0026#34;http://news.yahoo.com\u0026#34;, \u0026#34;http://news.yahoo.com/news\u0026#34;, \u0026#34;http://news.yahoo.com/news/topics/\u0026#34;, \u0026#34;http://news.google.com\u0026#34;, \u0026#34;http://news.yahoo.com/us\u0026#34; ] edges = [[2,0],[2,1],[3,2],[3,1],[0,4]] startUrl = \u0026#34;http://news.yahoo.com/news/topics/\u0026#34; Output: [ \u0026#34;http://news.yahoo.com\u0026#34;, \u0026#34;http://news.yahoo.com/news\u0026#34;, \u0026#34;http://news.yahoo.com/news/topics/\u0026#34;, \u0026#34;http://news.yahoo.com/us\u0026#34; ] Be multi threading(or multi processing), Python recommend use ThreadPoolExecutor (or ProcessPoolExecutor)to protect the threads (or processes) in a safe state when it executing. And this question maybe execute under a virtual environment on leetcode platform, so I guess take the ThreadPoolExecutor is a better choice.\nSo, I write 2 methods of the class Solution , one for extract hostname from url name get_hostname(), another one for filter url which is not visited name visit_url().\nThen, using the ThreadPoolExecutor to submit task visit_url for each url which is in the queue, and call future.result() to execute each visit_url with url.\nFinally, shutdown the ThreadPoolExecutor to release resources and return a list for visit url result.\nSolution # \u0026#34;\u0026#34;\u0026#34; # This is HtmlParser\u0026#39;s API interface. # You should not implement it, or speculate about its implementation # \u0026#34;\u0026#34;\u0026#34; #class HtmlParser(object): # def getUrls(self, url): # \u0026#34;\u0026#34;\u0026#34; # :type url: str # :rtype List[str] # \u0026#34;\u0026#34;\u0026#34; from concurrent.futures import ThreadPoolExecutor from threading import Condition class Solution: def __init__(self) -\u0026gt; None: self._queue = list() self._lock = Condition() self._visited = set() def get_hostname(self, url: str): hostname = \u0026#39;.\u0026#39;.join(url.split(\u0026#39;/\u0026#39;)[2].split(\u0026#39;.\u0026#39;)[1:]) return hostname def visit_url(self, url: str): next_urls: List[str] = self._parser.getUrls(url) with self._lock: for next_url in next_urls: if next_url not in self._visited and self.current_hostname == self.get_hostname(next_url) : self._visited.add(next_url) self._queue.insert(0,next_url) def crawl(self, startUrl: str, htmlParser: \u0026#39;HtmlParser\u0026#39;) -\u0026gt; List[str]: self._queue.insert(0,startUrl) self._visited.add(startUrl) self.current_hostname = self.get_hostname(startUrl) self._parser = htmlParser executor = ThreadPoolExecutor() while self._queue: urls = [self._queue.pop(), ] while self._queue: urls.append(self._queue.pop()) excecutor_list = [executor.submit(self.visit_url, (url)) for url in urls] for future in excecutor_list: future.result() executor.shutdown() return list(self._visited) ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/concurrency/web-crawler-multithreaded/","summary":"Description Given a URL startUrl and an interface HtmlParser, implement a Multi-threaded web crawler to crawl all links that are under the same hostname as startUrl.\nReturn all URLs obtained by your web crawler in any order.\nYour crawler should:\nStart from the page: startUrl Call HtmlParser.getUrls(url) to get all URLs from a webpage of a given URL. Do not crawl the same link twice. Explore only the links that are under the same hostname as startUrl.","title":"[leetcode][Python][Concurrency][Medium] 1242. Web Crawler Multithreaded"},{"content":"Description There is an intersection of two roads. First road is road A where cars travel from North to South in direction 1 and from South to North in direction 2. Second road is road B where cars travel from West to East in direction 3 and from East to West in direction 4.\nThere is a traffic light located on each road before the intersection. A traffic light can either be green or red.\nGreen means cars can cross the intersection in both directions of the road. Red means cars in both directions cannot cross the intersection and must wait until the light turns green. The traffic lights cannot be green on both roads at the same time. That means when the light is green on road A, it is red on road B and when the light is green on road B, it is red on road A.\nInitially, the traffic light is green on road A and red on road B. When the light is green on one road, all cars can cross the intersection in both directions until the light becomes green on the other road. No two cars traveling on different roads should cross at the same time.\nDesign a deadlock-free traffic light controlled system at this intersection.\nImplement the function void carArrived(carId, roadId, direction, turnGreen, crossCar) where:\ncarId is the id of the car that arrived. roadId is the id of the road that the car travels on. direction is the direction of the car. turnGreen is a function you can call to turn the traffic light to green on the current road. crossCar is a function you can call to let the current car cross the intersection. Idea Your answer is considered correct if it avoids cars deadlock in the intersection. Turning the light green on a road when it was already green is considered a wrong answer.\nInput: cars = [1,2,3,4,5], directions = [2,4,3,3,1], arrivalTimes = [10,20,30,40,40] Output: [ \u0026#34;Car 1 Has Passed Road A In Direction 2\u0026#34;, // Traffic light on road A is green, car 1 can cross the intersection. \u0026#34;Traffic Light On Road B Is Green\u0026#34;, // Car 2 requests green light for road B. \u0026#34;Car 2 Has Passed Road B In Direction 4\u0026#34;, // Car 2 crosses as the light is green on road B now. \u0026#34;Car 3 Has Passed Road B In Direction 3\u0026#34;, // Car 3 crosses as the light is green on road B now. \u0026#34;Traffic Light On Road A Is Green\u0026#34;, // Car 5 requests green light for road A. \u0026#34;Car 5 Has Passed Road A In Direction 1\u0026#34;, // Car 5 crosses as the light is green on road A now. \u0026#34;Traffic Light On Road B Is Green\u0026#34;, // Car 4 requests green light for road B. Car 4 blocked until car 5 crosses and then traffic light is green on road B. \u0026#34;Car 4 Has Passed Road B In Direction 3\u0026#34; // Car 4 crosses as the light is green on road B now. ] Explanation: This is a dead-lock free scenario. Note that the scenario when car 4 crosses before turning light into green on road A and allowing car 5 to pass is also correct and Accepted scenario. Fulfill requirements :\nHere an important thing that which road can change the traffic control light, but also cars can pass through fast if current light is GREEN with the road their own.\nSo, change traffic control light to Green if the lock status is release, or wait it until release by notify.\nSolution from threading import Condition class TrafficLight: def __init__(self): self.__lock = Condition() self.__light = 1 def carArrived( self, carId: int, # ID of the car roadId: int, # ID of the road the car travels on. Can be 1 (road A) or 2 (road B) direction: int, # Direction of the car turnGreen: \u0026#39;Callable[[], None]\u0026#39;, # Use turnGreen() to turn light to green on current road crossCar: \u0026#39;Callable[[], None]\u0026#39; # Use crossCar() to make car cross the intersection ) -\u0026gt; None: with self.__lock: if self.__light != roadId: turnGreen() self.__light = roadId crossCar() ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/concurrency/traffic-light-controlled-intersection/","summary":"Description There is an intersection of two roads. First road is road A where cars travel from North to South in direction 1 and from South to North in direction 2. Second road is road B where cars travel from West to East in direction 3 and from East to West in direction 4.\nThere is a traffic light located on each road before the intersection. A traffic light can either be green or red.","title":"[leetcode][Python][Concurrency][Easy] 1279. Traffic Light Controlled Intersection"},{"content":"Description Table: Matches\n+-------------+------+ | Column Name | Type | +-------------+------+ | player_id | int | | match_day | date | | result | enum | +-------------+------+ (player_id, match_day) is the primary key for this table. Each row of this table contains the ID of a player, the day of the match they played, and the result of that match. The result column is an ENUM type of (\u0026#39;Win\u0026#39;, \u0026#39;Draw\u0026#39;, \u0026#39;Lose\u0026#39;). The winning streak of a player is the number of consecutive wins uninterrupted by draws or losses.\nWrite an SQL query to count the longest winning streak for each player.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Matches (player_id int, match_day date, result ENUM(\u0026#39;Win\u0026#39;, \u0026#39;Draw\u0026#39;, \u0026#39;Lose\u0026#39;)) Truncate table Matches insert into Matches (player_id, match_day, result) values (\u0026#39;1\u0026#39;, \u0026#39;2022-01-17\u0026#39;, \u0026#39;Win\u0026#39;) insert into Matches (player_id, match_day, result) values (\u0026#39;1\u0026#39;, \u0026#39;2022-01-18\u0026#39;, \u0026#39;Win\u0026#39;) insert into Matches (player_id, match_day, result) values (\u0026#39;1\u0026#39;, \u0026#39;2022-01-25\u0026#39;, \u0026#39;Win\u0026#39;) insert into Matches (player_id, match_day, result) values (\u0026#39;1\u0026#39;, \u0026#39;2022-01-31\u0026#39;, \u0026#39;Draw\u0026#39;) insert into Matches (player_id, match_day, result) values (\u0026#39;1\u0026#39;, \u0026#39;2022-02-08\u0026#39;, \u0026#39;Win\u0026#39;) insert into Matches (player_id, match_day, result) values (\u0026#39;2\u0026#39;, \u0026#39;2022-02-06\u0026#39;, \u0026#39;Lose\u0026#39;) insert into Matches (player_id, match_day, result) values (\u0026#39;2\u0026#39;, \u0026#39;2022-02-08\u0026#39;, \u0026#39;Lose\u0026#39;) insert into Matches (player_id, match_day, result) values (\u0026#39;3\u0026#39;, \u0026#39;2022-03-30\u0026#39;, \u0026#39;Win\u0026#39;) Idea The query result format is in the following example.\n+-----------+----------------+ | player_id | longest_streak | +-----------+----------------+ | 1 | 3 | | 2 | 0 | | 3 | 1 | +-----------+----------------+ Fulfill requirements :\nI guess that I have to extract each game result which is not a winner, and convert _a split timeline_ for each player to help calculating _longest winning streak_.\nSo, I marked each game and sorted by match_day of each player, that will help to find losing matches, then I can use it as a split timeline to calculate with win of continuous games.\nSolution with game_result_rn as ( select row_number() over(partition by player_id order by match_day) as rn, player_id, result from Matches ), player_lose_game_rn as ( select player_id, rn, ifnull(lead(rn, 1) over(partition by player_id order by rn) ,rn) as next_rn from ( select player_id, 0 as rn from game_result_rn group by player_id union select player_id, rn from game_result_rn where result \u0026lt;\u0026gt; \u0026#39;Win\u0026#39; union select player_id, max(rn)+1 as rn from game_result_rn group by player_id ) a ), count_player_win as ( select distinct a.player_id, count(result) over(partition by a.player_id, b.next_rn) as longest_streak from game_result_rn a left join player_lose_game_rn b on b.player_id=a.player_id where a.rn between b.rn and b.next_rn and a.result = \u0026#39;Win\u0026#39; ) select a.player_id, ifnull(max(b.longest_streak), 0) as longest_streak from ( select distinct player_id from Matches ) a left join count_player_win b using(player_id) group by a.player_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/longest-winning-streak/","summary":"Description Table: Matches\n+-------------+------+ | Column Name | Type | +-------------+------+ | player_id | int | | match_day | date | | result | enum | +-------------+------+ (player_id, match_day) is the primary key for this table. Each row of this table contains the ID of a player, the day of the match they played, and the result of that match. The result column is an ENUM type of (\u0026#39;Win\u0026#39;, \u0026#39;Draw\u0026#39;, \u0026#39;Lose\u0026#39;).","title":"[leetcode][Database][Hard] 2173. Longest Winning Streak"},{"content":"Description Table: Orders\n+--------------+------+ | Column Name | Type | +--------------+------+ | order_id | int | | customer_id | int | | order_date | date | | price | int | +--------------+------+ order_id is the primary key for this table. Each row contains the id of an order, the id of customer that ordered it, the date of the order, and its price. Write an SQL query to report the IDs of the customers with the total purchases strictly increasing yearly.\nThe total purchases of a customer in one year is the sum of the prices of their orders in that year. If for some year the customer did not make any order, we consider the total purchases 0. The first year to consider for each customer is the year of their first order. The last year to consider for each customer is the year of their last order. Return the result table in any order.\nSQL Schema\nCreate table If Not Exists Orders (order_id int, customer_id int, order_date date, price int) Truncate table Orders insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2019-07-01\u0026#39;, \u0026#39;1100\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2019-11-01\u0026#39;, \u0026#39;1200\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;3\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2020-05-26\u0026#39;, \u0026#39;3000\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;4\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2021-08-31\u0026#39;, \u0026#39;3100\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;5\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2022-12-07\u0026#39;, \u0026#39;4700\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;6\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;2015-01-01\u0026#39;, \u0026#39;700\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;7\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;2017-11-07\u0026#39;, \u0026#39;1000\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;8\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2017-01-01\u0026#39;, \u0026#39;900\u0026#39;) insert into Orders (order_id, customer_id, order_date, price) values (\u0026#39;9\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2018-11-07\u0026#39;, \u0026#39;900\u0026#39;) Idea The query result format is in the following example.\n+-------------+ | customer_id | +-------------+ | 1 | +-------------+ Fulfill requirements :\nI need to find the range between the first order year to last order year of each customer, and set price as 0 if the year have not order record(s) of customer.\nThen, I can calculate strictly increasing purchases with each year of each customer via window function lead(column, offset) over().\nMarking the calculate result 1 if the total purchase price of next year larger than current year, otherwise, marking 0 . Name marking result as mark_increase_purchases .\nFinally, to compare the counting result of records and total mark_increase_purchases of each customer, I can get the result for customers are strictly increasing purchases or not. Due to next order after last order of each customer is not exist, so the records counting need to minus 1 .\nSolution with recursive cte_customer_order_year as ( select customer_id, year(min(order_date)) as first_order_year, year(max(order_date)) as last_order_year from Orders group by customer_id ), cte_customer_zero_order as ( select customer_id, first_order_year, last_order_year from cte_customer_order_year union select customer_id, first_order_year+1, last_order_year from cte_customer_zero_order where first_order_year \u0026lt; last_order_year ), cte_orders as ( select distinct customer_id, year(order_date) as order_year, sum(price) over(partition by customer_id, year(order_date) order by year(order_date)) as price from ( select customer_id, makedate(first_order_year, 1) as order_date, 0 as price from cte_customer_zero_order union select customer_id, order_date, price from Orders ) union_orders ) select a.customer_id from ( select customer_id, order_year, if( lead(price, 1) over(partition by customer_id order by order_year)-price \u0026gt; 0, 1, 0 ) as mark_increase_purchases from cte_orders ) a group by a.customer_id having sum(a.mark_increase_purchases) = (count(a.customer_id)-1) ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/customers-with-strictly-increasing-purchases/","summary":"Description Table: Orders\n+--------------+------+ | Column Name | Type | +--------------+------+ | order_id | int | | customer_id | int | | order_date | date | | price | int | +--------------+------+ order_id is the primary key for this table. Each row contains the id of an order, the id of customer that ordered it, the date of the order, and its price. Write an SQL query to report the IDs of the customers with the total purchases strictly increasing yearly.","title":"[leetcode][Database][Hard] 2474. Customers With Strictly Increasing Purchases"},{"content":"Description Table: Products\n+-------------+------+ | Column Name | Type | +-------------+------+ | product_id | int | | price | int | +-------------+------+ product_id is the primary key for this table. Each row in this table shows the ID of a product and the price of one unit. Table: Purchases\n+-------------+------+ | Column Name | Type | +-------------+------+ | invoice_id | int | | product_id | int | | quantity | int | +-------------+------+ (invoice_id, product_id) is the primary key for this table. Each row in this table shows the quantity ordered from one product in an invoice. Write an SQL query to show the details of the invoice with the highest price. If two or more invoices have the same price, return the details of the one with the smallest invoice_id.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Products (product_id int, price int) Create table If Not Exists Purchases (invoice_id int, product_id int, quantity int) Truncate table Products insert into Products (product_id, price) values (\u0026#39;1\u0026#39;, \u0026#39;100\u0026#39;) insert into Products (product_id, price) values (\u0026#39;2\u0026#39;, \u0026#39;200\u0026#39;) Truncate table Purchases insert into Purchases (invoice_id, product_id, quantity) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;) insert into Purchases (invoice_id, product_id, quantity) values (\u0026#39;3\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;) insert into Purchases (invoice_id, product_id, quantity) values (\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;) insert into Purchases (invoice_id, product_id, quantity) values (\u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;4\u0026#39;) insert into Purchases (invoice_id, product_id, quantity) values (\u0026#39;4\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;10\u0026#39;) Idea The query result format is shown in the following example.\n+------------+----------+-------+ | product_id | quantity | price | +------------+----------+-------+ | 2 | 3 | 600 | | 1 | 4 | 400 | +------------+----------+-------+ Fulfill requirements :\nI used the function sum() over() to calculate totally price of each invoice, and raking the above calaulate result by sorted with invoide_id ascending, and totally pricing descending, and name ranking result as rn.\nThen, finding the record which rn equals to 1 for query result.\nSolution with cte as ( select invoice_id, product_id, quantity, price, s_price, row_number() over(order by s_price desc, invoice_id) as rn from ( select a.invoice_id, a.product_id, a.quantity, ifnull(a.quantity * b.price, 0) as price, sum(ifnull(a.quantity * b.price, 0)) over(partition by a.invoice_id) as s_price from Purchases a left join Products b using(product_id) ) detail ) select product_id, quantity, price from cte where invoice_id = (select invoice_id from cte where rn = 1) ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/generate-the-invoice/","summary":"Description Table: Products\n+-------------+------+ | Column Name | Type | +-------------+------+ | product_id | int | | price | int | +-------------+------+ product_id is the primary key for this table. Each row in this table shows the ID of a product and the price of one unit. Table: Purchases\n+-------------+------+ | Column Name | Type | +-------------+------+ | invoice_id | int | | product_id | int | | quantity | int | +-------------+------+ (invoice_id, product_id) is the primary key for this table.","title":"[leetcode][Database][Hard] 2362. Generate the Invoice"},{"content":"Description Table: Keywords\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | topic_id | int | | word | varchar | +-------------+---------+ (topic_id, word) is the primary key for this table. Each row of this table contains the id of a topic and a word that is used to express this topic. There may be more than one word to express the same topic and one word may be used to express multiple topics. Table: Posts\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | post_id | int | | content | varchar | +-------------+---------+ post_id is the primary key for this table. Each row of this table contains the ID of a post and its content. Content will consist only of English letters and spaces. Leetcode has collected some posts from its social media website and is interested in finding the topics of each post. Each topic can be expressed by one or more keywords. If a keyword of a certain topic exists in the content of a post (case insensitive) then the post has this topic.\nWrite an SQL query to find the topics of each post according to the following rules:\nIf the post does not have keywords from any topic, its topic should be \u0026quot;Ambiguous!\u0026quot;. If the post has at least one keyword of any topic, its topic should be a string of the IDs of its topics sorted in ascending order and separated by commas ','. The string should not contain duplicate IDs. Return the result table in any order.\nSQL Schema\nCreate table If Not Exists Keywords (topic_id int, word varchar(25)) Create table If Not Exists Posts (post_id int, content varchar(100)) Truncate table Keywords insert into Keywords (topic_id, word) values (\u0026#39;1\u0026#39;, \u0026#39;handball\u0026#39;) insert into Keywords (topic_id, word) values (\u0026#39;1\u0026#39;, \u0026#39;football\u0026#39;) insert into Keywords (topic_id, word) values (\u0026#39;3\u0026#39;, \u0026#39;WAR\u0026#39;) insert into Keywords (topic_id, word) values (\u0026#39;2\u0026#39;, \u0026#39;Vaccine\u0026#39;) Truncate table Posts insert into Posts (post_id, content) values (\u0026#39;1\u0026#39;, \u0026#39;We call it soccer They call it football hahaha\u0026#39;) insert into Posts (post_id, content) values (\u0026#39;2\u0026#39;, \u0026#39;Americans prefer basketball while Europeans love handball and football\u0026#39;) insert into Posts (post_id, content) values (\u0026#39;3\u0026#39;, \u0026#39;stop the war and play handball\u0026#39;) insert into Posts (post_id, content) values (\u0026#39;4\u0026#39;, \u0026#39;warning I planted some flowers this morning and then got vaccinated\u0026#39;) Idea The query result format is in the following example.\n+---------+------------+ | post_id | topic | +---------+------------+ | 1 | 1 | | 2 | 1 | | 3 | 1,3 | | 4 | Ambiguous! | +---------+------------+ Fulfill requirements :\nI guess that will maybe use the functions SUBSTRING or SUBSTRING_INDE when most people, include me, seem this question at first. But I would like to solve this question via function INSTR .\nAs we can find the definition for INSTR in MySQL official documentation.\nReturns the position of the first occurrence of substring substr in string str.\nThis is the same as the two-argument form of LOCATE(),\nexcept that the order of the arguments is reversed.\nmysql \u0026gt; SELECT INSTR(\u0026#39;foobarbar\u0026#39;, \u0026#39;bar\u0026#39;); -\u0026gt; 4 mysql \u0026gt; SELECT INSTR(\u0026#39;xbar\u0026#39;, \u0026#39;foobar\u0026#39;); -\u0026gt; 0 So, in the first step, an easy way for adding two space characters as prefix and suffix for a post content, it’s help to find the first/last words are keyword or not in a post content via function INSTR to use {SPACE_SYMBOL}{keyword}{SPACE_SYMBOL} as a condition.\nINSTR will return a position(or an index) of the content if a keyword in this content, but also return 0 if a keyword not in this content.\nFinally, using the table Posts as a main table in query, and using left join to associate the result from INSTR to map if topic(s) is/are in a post content. Also, replacing the topic to Ambiguous! if here haven’t a topic in.\nSolution with cte_post as ( select post_id, concat(\u0026#39; \u0026#39;, content, \u0026#39; \u0026#39;) as content from Posts -- The easy way for INSTR() to find keyword ), cte_match_topics as ( select a.post_id, group_concat(distinct b.topic_id separator \u0026#39;,\u0026#39;) as topic from cte_post a, Keywords b where INSTR(a.content, concat(\u0026#39; \u0026#39;, b.word, \u0026#39; \u0026#39;)) \u0026gt; 0 -- find keyword position group by a.post_id ) select distinct(a.post_id), ifnull(b.topic, \u0026#39;Ambiguous!\u0026#39;) as topic from Posts a left join cte_match_topics b using(post_id) order by 1 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/finding-the-topic-of-each-post/","summary":"Description Table: Keywords\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | topic_id | int | | word | varchar | +-------------+---------+ (topic_id, word) is the primary key for this table. Each row of this table contains the id of a topic and a word that is used to express this topic. There may be more than one word to express the same topic and one word may be used to express multiple topics.","title":"[leetcode][Database][Hard] 2199. Finding the Topic of Each Post"},{"content":"Description Table: Candidates\n+-------------+------+ | Column Name | Type | +-------------+------+ | employee_id | int | | experience | enum | | salary | int | +-------------+------+ employee_id is the primary key column for this table. experience is an enum with one of the values (\u0026#39;Senior\u0026#39;, \u0026#39;Junior\u0026#39;). Each row of this table indicates the id of a candidate, their monthly salary, and their experience. The salary of each candidate is guaranteed to be unique. A company wants to hire new employees. The budget of the company for the salaries is $70000. The company\u0026rsquo;s criteria for hiring are:\nKeep hiring the senior with the smallest salary until you cannot hire any more seniors. Use the remaining budget to hire the junior with the smallest salary. Keep hiring the junior with the smallest salary until you cannot hire any more juniors. Write an SQL query to find the ids of seniors and juniors hired under the mentioned criteria.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Candidates (employee_id int, experience ENUM(\u0026#39;Senior\u0026#39;, \u0026#39;Junior\u0026#39;), salary int) Truncate table Candidates insert into Candidates (employee_id, experience, salary) values (\u0026#39;1\u0026#39;, \u0026#39;Junior\u0026#39;, \u0026#39;10000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;9\u0026#39;, \u0026#39;Junior\u0026#39;, \u0026#39;15000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;2\u0026#39;, \u0026#39;Senior\u0026#39;, \u0026#39;20000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;11\u0026#39;, \u0026#39;Senior\u0026#39;, \u0026#39;16000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;13\u0026#39;, \u0026#39;Senior\u0026#39;, \u0026#39;50000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;4\u0026#39;, \u0026#39;Junior\u0026#39;, \u0026#39;40000\u0026#39;) Idea The query result format is in the following example.\n+-------------+ | employee_id | +-------------+ | 11 | | 2 | | 1 | | 9 | +-------------+ Fulfill requirements :\nThe thinking process like as [leetcode][Database][Hard] 2004. The Number of Seniors and Juniors to Join the Company.\nSo we can calculating the cumulative salary, sorted the salary of each employee and partition by different experience at first.\nNext, finding the max cumulative salary which less than budget $70,000 and take employee_id which are fitting the above condition. Also, finding the junior employees by remaining budget from hired senior employees.\nSolution with salary_cumulative as ( select employee_id, experience, salary, sum(salary) over(partition by experience order by salary) as cumulative_salary from Candidates ), senior_hiring as ( select -1 as employee_id, 0 as cumulative_salary union select employee_id, cumulative_salary from salary_cumulative where experience = \u0026#39;Senior\u0026#39; and 70000 - cumulative_salary \u0026gt;=0 ), junior_hiring as ( select employee_id from salary_cumulative join ( select 70000-max(cumulative_salary) as remaining from senior_hiring ) senior where experience = \u0026#39;Junior\u0026#39; and remaining-cumulative_salary\u0026gt;=0 ) select employee_id from senior_hiring where employee_id \u0026gt; 0 union select employee_id from junior_hiring ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/the-number-of-seniors-and-juniors-to-join-the-company-ii/","summary":"Description Table: Candidates\n+-------------+------+ | Column Name | Type | +-------------+------+ | employee_id | int | | experience | enum | | salary | int | +-------------+------+ employee_id is the primary key column for this table. experience is an enum with one of the values (\u0026#39;Senior\u0026#39;, \u0026#39;Junior\u0026#39;). Each row of this table indicates the id of a candidate, their monthly salary, and their experience. The salary of each candidate is guaranteed to be unique.","title":"[leetcode][Database][Hard] 2010. The Number of Seniors and Juniors to Join the Company II"},{"content":"Description Table: Terms\n+-------------+------+ | Column Name | Type | +-------------+------+ | power | int | | factor | int | +-------------+------+ power is the primary key column for this table. Each row of this table contains information about one term of the equation. power is an integer in the range [0, 100]. factor is an integer in the range [-100, 100] and cannot be zero. You have a very powerful program that can solve any equation of one variable in the world. The equation passed to the program must be formatted as follows:\nThe left-hand side (LHS) should contain all the terms. The right-hand side (RHS) should be zero. Each term of the LHS should follow the format \u0026quot;\u0026lt;sign\u0026gt;\u0026lt;fact\u0026gt;X^\u0026lt;pow\u0026gt;\u0026quot; where:\n\u0026lt;sign\u0026gt; is either \u0026quot;+\u0026quot; or \u0026quot;-\u0026quot;.\n\u0026lt;fact\u0026gt; is the absolute value of the factor.\n\u0026lt;pow\u0026gt; is the value of the power. If the power is 1, do not add \u0026quot;^\u0026lt;pow\u0026gt;\u0026quot;.\nFor example, if power = 1 and factor = 3, the term will be \u0026quot;+3X\u0026quot;. If the power is 0, add neither \u0026quot;X\u0026quot; nor \u0026quot;^\u0026lt;pow\u0026gt;\u0026quot;.\nFor example, if power = 0 and factor = -3, the term will be \u0026quot;-3\u0026quot;. The powers in the LHS should be sorted in descending order. Write an SQL query to build the equation.\nSQL Schema\nCreate table If Not Exists Terms (power int, factor int) Truncate table Terms insert into Terms (power, factor) values (\u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;) insert into Terms (power, factor) values (\u0026#39;1\u0026#39;, \u0026#39;-4\u0026#39;) insert into Terms (power, factor) values (\u0026#39;0\u0026#39;, \u0026#39;2\u0026#39;) Idea The query result format is in the following example.\n+-----------------+ | equation | +-----------------+ | -4X^4+1X^2-1X=0 | +-----------------+ Fulfill requirements :\nFor generating the equation by records of table Terms , it can be reorganize to a few parts of a term {factor sign}{absolute factor value}{power of X sign}{power value} For example :\nTerm will be present -4X when the record.factor=-4 , record.power=1 Term will be present +2x^4 when the record.factor=2 , record.power=4 Term will be present +1 when the record.factor=1 , record.power=0 Finally, using the function group_concat() combainating all of the terms from reorganize.\nSolution with builder as ( select 0 as LHS, -1 as rk, \u0026#39;=0\u0026#39; as e union select 0 as LHS, row_number() over(order by power) rk, concat( if(factor \u0026gt;0, \u0026#39;+\u0026#39;, \u0026#39;-\u0026#39;), -- factor sign abs(factor), -- remove factor sign of the value if(power=0, \u0026#39;\u0026#39;, if(power=1, \u0026#39;X\u0026#39;, \u0026#39;X^\u0026#39;) ), -- power of X if(power\u0026lt;2, \u0026#39;\u0026#39;, power) -- show power text when power large than 2 ) e from Terms ) select group_concat( e order by rk desc separator \u0026#39;\u0026#39; ) as equation from builder group by LHS ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/build-the-equation/","summary":"Description Table: Terms\n+-------------+------+ | Column Name | Type | +-------------+------+ | power | int | | factor | int | +-------------+------+ power is the primary key column for this table. Each row of this table contains information about one term of the equation. power is an integer in the range [0, 100]. factor is an integer in the range [-100, 100] and cannot be zero. You have a very powerful program that can solve any equation of one variable in the world.","title":"[leetcode][Database][Hard] 2118. Build the Equation"},{"content":"Description Table: Products\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | product_id | int | | store_name1 | int | | store_name2 | int | | : | int | | : | int | | : | int | | store_namen | int | +-------------+---------+ product_id is the primary key for this table. Each row in this table indicates the product\u0026#39;s price in n different stores. If the product is not available in a store, the price will be null in that store\u0026#39;s column. The names of the stores may change from one testcase to another. There will be at least 1 store and at most 30 stores. Important note: This problem targets those who have a good experience with SQL. If you are a beginner, we recommend that you skip it for now.\nImplement the procedure UnpivotProducts to reorganize the Products table so that each row has the id of one product, the name of a store where it is sold, and its price in that store. If a product is not available in a store, do not include a row with that product_id and store combination in the result table. There should be three columns: product_id, store, and price.\nThe procedure should return the table after reorganizing it.\nReturn the result table in any order.\nSQL Schema\nTruncate table Products insert into Products (product_id, LC_Store, Nozama, Shop, Souq) values (\u0026#39;1\u0026#39;, \u0026#39;100\u0026#39;, \u0026#39;None\u0026#39;, \u0026#39;110\u0026#39;, \u0026#39;None\u0026#39;) insert into Products (product_id, LC_Store, Nozama, Shop, Souq) values (\u0026#39;2\u0026#39;, \u0026#39;None\u0026#39;, \u0026#39;200\u0026#39;, \u0026#39;None\u0026#39;, \u0026#39;190\u0026#39;) insert into Products (product_id, LC_Store, Nozama, Shop, Souq) values (\u0026#39;3\u0026#39;, \u0026#39;None\u0026#39;, \u0026#39;None\u0026#39;, \u0026#39;1000\u0026#39;, \u0026#39;1900\u0026#39;) Idea The query result format is in the following example.\n+------------+----------+-------+ | product_id | store | price | +------------+----------+-------+ | 1 | LC_Store | 100 | | 1 | Shop | 110 | | 2 | Nozama | 200 | | 2 | Souq | 190 | | 3 | Shop | 1000 | | 3 | Souq | 1900 | +------------+----------+-------+ Refer bofeng07\u0026rsquo;s MySQL solution, it a grate idea!\nGetting the column names of table Products from information_schema.columns, using group_concat to combian each column values of each row of table Products by union .\nSolution CREATE PROCEDURE UnpivotProducts() BEGIN set session group_concat_max_len = 1000000; set @macro = null; select group_concat( concat( \u0026#39;select product_id, \u0026#34;\u0026#39;, column_name, \u0026#39;\u0026#34; as store, \u0026#39;, column_name, \u0026#39; as price \u0026#39;, \u0026#39;from Products \u0026#39;, \u0026#39;where \u0026#39;, column_name, \u0026#39; is not null\u0026#39; ) separator \u0026#39; union \u0026#39; ) into @macro from information_schema.columns where table_schema=\u0026#39;test\u0026#39; and table_name=\u0026#39;Products\u0026#39; and column_name != \u0026#39;product_id\u0026#39;; prepare sql_query from @macro; execute sql_query; deallocate prepare sql_query; END ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/dynamic-unpivoting-of-a-table/","summary":"Description Table: Products\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | product_id | int | | store_name1 | int | | store_name2 | int | | : | int | | : | int | | : | int | | store_namen | int | +-------------+---------+ product_id is the primary key for this table. Each row in this table indicates the product\u0026#39;s price in n different stores. If the product is not available in a store, the price will be null in that store\u0026#39;s column.","title":"[leetcode][Database][Hard]2253. Dynamic Unpivoting of a Table"},{"content":"Description Table: Listens\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | song_id | int | | day | date | +-------------+---------+ There is no primary key for this table. It may contain duplicates. Each row of this table indicates that the user user_id listened to the song song_id on the day day. Table: Friendship\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user1_id | int | | user2_id | int | +---------------+---------+ (user1_id, user2_id) is the primary key for this table. Each row of this table indicates that the users user1_id and user2_id are friends. Note that user1_id \u0026lt; user2_id. Write an SQL query to report the similar friends of Leetcodify users. A user x and user y are similar friends if:\nUsers x and y are friends, and Users x and y listened to the same three or more different songs on the same day. Return the result table in any order. Note that you must return the similar pairs of friends the same way they were represented in the input (i.e., always user1_id \u0026lt; user2_id).\nSQL Schema\nCreate table If Not Exists Listens (user_id int, song_id int, day date) Create table If Not Exists Friendship (user1_id int, user2_id int) Truncate table Listens insert into Listens (user_id, song_id, day) values (\u0026#39;1\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;1\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;1\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;2\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;2\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;2\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;3\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;3\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;3\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;4\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;4\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;4\u0026#39;, \u0026#39;13\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;5\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-16\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;5\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-16\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;5\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-16\u0026#39;) Truncate table Friendship insert into Friendship (user1_id, user2_id) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;2\u0026#39;, \u0026#39;5\u0026#39;) Idea The query result format is in the following example.\n+----------+----------+ | user1_id | user2_id | +----------+----------+ | 1 | 2 | +----------+----------+ Fulfill requirements :\nThe thinking process like as [leetcode][Database][Hard] 1917. Leetcodify Friends Recommendations, the different is this question asking to find similar friend (i.e. user x and user y have a pair record in Friendship).\nSo, we can do that with the same logic to find user x and user y are listened to the same three or more different songs on the same day or not.\nListing each user and their friends cte_user_friends . To avoid the records that user listen the same song in a day, using the distinct to remove duplicates records. Finally, counting the song_id via group by day, user_id, recommended_id, and filtering the counting song_id value large than or equals to 3 after group by statement.\nSolution with cte_user_friends as ( select user1_id as user1_id, user2_id as user2_id from Friendship union select user2_id as user1_id, user1_id as user2_id from Friendship ), cte_listen_distinct as ( select distinct user_id, song_id , day from Listens ), cte_similar_friend as ( select a.user_id as user1_id, b.user_id as user2_id from cte_listen_distinct a # user1 left join cte_listen_distinct b on b.song_id=a.song_id and a.day=b.day # user2 left join cte_user_friends c on c.user1_id = a.user_id and c.user2_id = b.user_id where a.user_id \u0026lt;\u0026gt; b.user_id and user1_id is not null group by a.day, a.user_id, b.user_id having count(a.song_id) \u0026gt;=3 ) select distinct b.user1_id, b.user2_id from Friendship a join cte_similar_friend b on b.user1_id = a.user1_id and b.user2_id = a.user2_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/leetcodify-similar-friends/","summary":"Description Table: Listens\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | song_id | int | | day | date | +-------------+---------+ There is no primary key for this table. It may contain duplicates. Each row of this table indicates that the user user_id listened to the song song_id on the day day. Table: Friendship\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user1_id | int | | user2_id | int | +---------------+---------+ (user1_id, user2_id) is the primary key for this table.","title":"[leetcode][Database][Hard] 1919. Leetcodify Similar Friends"},{"content":"Description Table: Listens\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | song_id | int | | day | date | +-------------+---------+ There is no primary key for this table. It may contain duplicates. Each row of this table indicates that the user user_id listened to the song song_id on the day day. Table: Friendship\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user1_id | int | | user2_id | int | +---------------+---------+ (user1_id, user2_id) is the primary key for this table. Each row of this table indicates that the users user1_id and user2_id are friends. Note that user1_id \u0026lt; user2_id. Write an SQL query to recommend friends to Leetcodify users. We recommend user x to user y if:\nUsers x and y are not friends, and Users x and y listened to the same three or more different songs on the same day. Note that friend recommendations are unidirectional, meaning if user x and user y should be recommended to each other, the result table should have both user x recommended to user y and user y recommended to user x. Also, note that the result table should not contain duplicates (i.e., user y should not be recommended to user x multiple times.).\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Listens (user_id int, song_id int, day date) Create table If Not Exists Friendship (user1_id int, user2_id int) Truncate table Listens insert into Listens (user_id, song_id, day) values (\u0026#39;1\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;1\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;1\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;2\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;2\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;2\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;3\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;3\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;3\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;4\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;4\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;4\u0026#39;, \u0026#39;13\u0026#39;, \u0026#39;2021-03-15\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;5\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;2021-03-16\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;5\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-03-16\u0026#39;) insert into Listens (user_id, song_id, day) values (\u0026#39;5\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;2021-03-16\u0026#39;) Truncate table Friendship insert into Friendship (user1_id, user2_id) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;) Idea The query result format is in the following example.\n+---------+----------------+ | user_id | recommended_id | +---------+----------------+ | 1 | 3 | | 2 | 3 | | 3 | 1 | | 3 | 2 | +---------+----------------+ Fulfill requirements :\nThe thinking process like as [leetcode][Database][Hard] 1892. Page Recommendations II, so the first step is listing each user and their friends cte_user_friends .\nTo avoid the records that user listen the same song in a day, using the distinct to remove duplicates records.\nFinding the users listening a song in a day via self join cte_listen_distinct , and then confirm the user friendship who in above self join set and remove them.\nFinally, counting the song_id via group by day, user_id, recommended_id, and filtering the counting song_id value large than or equals to 3 after group by statement.\nSolution with cte_user_friends as ( select user1_id as user_id, user2_id as friend_id from Friendship union select user2_id as user_id, user1_id as friend_id from Friendship ), cte_listen_distinct as ( select distinct user_id, song_id, day from Listens ) select distinct a.user_id, b.user_id as recommended_id from cte_listen_distinct a left join cte_listen_distinct b on b.song_id=a.song_id and a.day=b.day left join cte_user_friends c on c.user_id = a.user_id and c.friend_id = b.user_id where c.user_id is null and a.user_id \u0026lt;\u0026gt; b.user_id group by a.day, a.user_id, b.user_id having count(a.song_id) \u0026gt;=3 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/leetcodify-friends-recommendations/","summary":"Description Table: Listens\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | song_id | int | | day | date | +-------------+---------+ There is no primary key for this table. It may contain duplicates. Each row of this table indicates that the user user_id listened to the song song_id on the day day. Table: Friendship\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user1_id | int | | user2_id | int | +---------------+---------+ (user1_id, user2_id) is the primary key for this table.","title":"[leetcode][Database][Hard] 1917. Leetcodify Friends Recommendations"},{"content":"Description Table: Friendship\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user1_id | int | | user2_id | int | +---------------+---------+ (user1_id, user2_id) is the primary key for this table. Each row of this table indicates that the users user1_id and user2_id are friends. Table: Likes\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | page_id | int | +-------------+---------+ (user_id, page_id) is the primary key for this table. Each row of this table indicates that user_id likes page_id. You are implementing a page recommendation system for a social media website. Your system will recommended a page to user_id if the page is liked by at least one friend of user_id and is not liked by user_id.\nWrite an SQL query to find all the possible page recommendations for every user. Each recommendation should appear as a row in the result table with these columns:\nuser_id: The ID of the user that your system is making the recommendation to. page_id: The ID of the page that will be recommended to user_id. friends_likes: The number of the friends of user_id that like page_id. Return result table in any order.\nSQL Schema\nCreate table If Not Exists Friendship (user1_id int, user2_id int) Create table If Not Exists Likes (user_id int, page_id int) Truncate table Friendship insert into Friendship (user1_id, user2_id) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;1\u0026#39;, \u0026#39;3\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;1\u0026#39;, \u0026#39;4\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;2\u0026#39;, \u0026#39;5\u0026#39;) insert into Friendship (user1_id, user2_id) values (\u0026#39;6\u0026#39;, \u0026#39;1\u0026#39;) Truncate table Likes insert into Likes (user_id, page_id) values (\u0026#39;1\u0026#39;, \u0026#39;88\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;2\u0026#39;, \u0026#39;23\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;3\u0026#39;, \u0026#39;24\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;4\u0026#39;, \u0026#39;56\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;5\u0026#39;, \u0026#39;11\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;6\u0026#39;, \u0026#39;33\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;2\u0026#39;, \u0026#39;77\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;3\u0026#39;, \u0026#39;77\u0026#39;) insert into Likes (user_id, page_id) values (\u0026#39;6\u0026#39;, \u0026#39;88\u0026#39;) Idea The query result format is in the following example.\n+---------+---------+---------------+ | user_id | page_id | friends_likes | +---------+---------+---------------+ | 1 | 77 | 2 | | 1 | 23 | 1 | | 1 | 24 | 1 | | 1 | 56 | 1 | | 1 | 33 | 1 | | 2 | 24 | 1 | | 2 | 56 | 1 | | 2 | 11 | 1 | | 2 | 88 | 1 | | 3 | 88 | 1 | | 3 | 23 | 1 | | 4 | 88 | 1 | | 4 | 77 | 1 | | 4 | 23 | 1 | | 5 | 77 | 1 | | 5 | 23 | 1 | +---------+---------+---------------+ Fulfill requirements :\nThe thinking process like as [leetcode][Database][Hard]1972. First and Last Call On the Same Day, so the first step is listing each user and their friends cte_all_users.\nFinding the pages which are friends likes, then to find which pages both user and friends likes. Finally, filtering not match the condition : the page user not liked but friends did.\nThis solution have same concept with minus or except, finding the difference set between both user and friends likes pages and only friends like pages.\nSolution with cte_all_users as ( select user1_id as user_id, user2_id as friend from Friendship union select user2_id as user_id, user1_id as friend from Friendship ) select distinct a.user_id, b.page_id , count(a.friend) as friends_likes from cte_all_users a join Likes b on b.user_id = a.friend -- find the pages friend like left join Likes c on c.user_id = a.user_id and b.page_id = c.page_id -- find the pages both user and friend like where c.page_id is null -- filtering not match the condition : the page user not liked but friends did. group by a.user_id, b.page_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/page-recommendations-ii/","summary":"Description Table: Friendship\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user1_id | int | | user2_id | int | +---------------+---------+ (user1_id, user2_id) is the primary key for this table. Each row of this table indicates that the users user1_id and user2_id are friends. Table: Likes\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | page_id | int | +-------------+---------+ (user_id, page_id) is the primary key for this table.","title":"[leetcode][Database][Hard] 1892. Page Recommendations II"},{"content":"Description Table: Tasks\n+----------------+---------+ | Column Name | Type | +----------------+---------+ | task_id | int | | subtasks_count | int | +----------------+---------+ task_id is the primary key for this table. Each row in this table indicates that task_id was divided into subtasks_count subtasks labeled from 1 to subtasks_count. It is guaranteed that 2 \u0026lt;= subtasks_count \u0026lt;= 20. Table: Executed\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | task_id | int | | subtask_id | int | +---------------+---------+ (task_id, subtask_id) is the primary key for this table. Each row in this table indicates that for the task task_id, the subtask with ID subtask_id was executed successfully. It is guaranteed that subtask_id \u0026lt;= subtasks_count for each task_id. Write an SQL query to report the IDs of the missing subtasks for each task_id.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Tasks (task_id int, subtasks_count int) Create table If Not Exists Executed (task_id int, subtask_id int) Truncate table Tasks insert into Tasks (task_id, subtasks_count) values (\u0026#39;1\u0026#39;, \u0026#39;3\u0026#39;) insert into Tasks (task_id, subtasks_count) values (\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;) insert into Tasks (task_id, subtasks_count) values (\u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;) Truncate table Executed insert into Executed (task_id, subtask_id) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;) insert into Executed (task_id, subtask_id) values (\u0026#39;3\u0026#39;, \u0026#39;1\u0026#39;) insert into Executed (task_id, subtask_id) values (\u0026#39;3\u0026#39;, \u0026#39;2\u0026#39;) insert into Executed (task_id, subtask_id) values (\u0026#39;3\u0026#39;, \u0026#39;3\u0026#39;) insert into Executed (task_id, subtask_id) values (\u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;) Idea The query result format is in the following example.\n+---------+------------+ | task_id | subtask_id | +---------+------------+ | 1 | 1 | | 1 | 3 | | 2 | 1 | | 2 | 2 | +---------+------------+ Fulfill requirements :\nUsing the with recursive to generate a serial number for subtasks_id from 1 to 20, then finding not executed subtasks with condition ifnull(task_id) or ifnull(subtask_id) which subtask_id records not in table Executed.\nSolution with recursive cte_subtasks_sn as ( SELECT 1 AS subtask_id UNION ALL SELECT subtask_id + 1 FROM cte_subtasks_sn WHERE subtask_id \u0026lt; 20 ), cte_subtasks_count as ( select a.task_id as task_id, b.subtask_id as subtask_id from Tasks a, cte_subtasks_sn b where b.subtask_id \u0026lt;= a.subtasks_count ) select a.task_id, a.subtask_id from cte_subtasks_count a left join Executed b using(task_id, subtask_id) where ifnull(b.task_id, -1) = -1 or ifnull(b.subtask_id, -1) = -1 order by task_id, subtask_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/find-the-subtasks-that-did-not-execute/","summary":"Description Table: Tasks\n+----------------+---------+ | Column Name | Type | +----------------+---------+ | task_id | int | | subtasks_count | int | +----------------+---------+ task_id is the primary key for this table. Each row in this table indicates that task_id was divided into subtasks_count subtasks labeled from 1 to subtasks_count. It is guaranteed that 2 \u0026lt;= subtasks_count \u0026lt;= 20. Table: Executed\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | task_id | int | | subtask_id | int | +---------------+---------+ (task_id, subtask_id) is the primary key for this table.","title":"[leetcode][Database][Hard] 1767. Find the Subtasks That Did Not Execute"},{"content":"Description Table: Drivers\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | driver_id | int | | join_date | date | +-------------+---------+ driver_id is the primary key for this table. Each row of this table contains the driver\u0026#39;s ID and the date they joined the Hopper company. Table: Rides\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | ride_id | int | | user_id | int | | requested_at | date | +--------------+---------+ ride_id is the primary key for this table. Each row of this table contains the ID of a ride, the user\u0026#39;s ID that requested it, and the day they requested it. There may be some ride requests in this table that were not accepted. Table: AcceptedRides\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | ride_id | int | | driver_id | int | | ride_distance | int | | ride_duration | int | +---------------+---------+ ride_id is the primary key for this table. Each row of this table contains some information about an accepted ride. It is guaranteed that each accepted ride exists in the Rides table. Write an SQL query to compute the average_ride_distance and average_ride_duration of every 3-month window starting from January - March 2020 to October - December 2020. Round average_ride_distance and average_ride_duration to the nearest two decimal places.\nThe average_ride_distance is calculated by summing up the total ride_distance values from the three months and dividing it by 3. The average_ride_duration is calculated in a similar way.\nReturn the result table ordered by month in ascending order, where month is the starting month\u0026rsquo;s number (January is 1, February is 2, etc.).\nSQL Schema\nCreate table If Not Exists Drivers (driver_id int, join_date date) Create table If Not Exists Rides (ride_id int, user_id int, requested_at date) Create table If Not Exists AcceptedRides (ride_id int, driver_id int, ride_distance int, ride_duration int) Truncate table Drivers insert into Drivers (driver_id, join_date) values (\u0026#39;10\u0026#39;, \u0026#39;2019-12-10\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;8\u0026#39;, \u0026#39;2020-1-13\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;5\u0026#39;, \u0026#39;2020-2-16\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;7\u0026#39;, \u0026#39;2020-3-8\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;4\u0026#39;, \u0026#39;2020-5-17\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;1\u0026#39;, \u0026#39;2020-10-24\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;6\u0026#39;, \u0026#39;2021-1-5\u0026#39;) Truncate table Rides insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;6\u0026#39;, \u0026#39;75\u0026#39;, \u0026#39;2019-12-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;1\u0026#39;, \u0026#39;54\u0026#39;, \u0026#39;2020-2-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;10\u0026#39;, \u0026#39;63\u0026#39;, \u0026#39;2020-3-4\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;19\u0026#39;, \u0026#39;39\u0026#39;, \u0026#39;2020-4-6\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;3\u0026#39;, \u0026#39;41\u0026#39;, \u0026#39;2020-6-3\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;13\u0026#39;, \u0026#39;52\u0026#39;, \u0026#39;2020-6-22\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;7\u0026#39;, \u0026#39;69\u0026#39;, \u0026#39;2020-7-16\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;17\u0026#39;, \u0026#39;70\u0026#39;, \u0026#39;2020-8-25\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;20\u0026#39;, \u0026#39;81\u0026#39;, \u0026#39;2020-11-2\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;5\u0026#39;, \u0026#39;57\u0026#39;, \u0026#39;2020-11-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;2\u0026#39;, \u0026#39;42\u0026#39;, \u0026#39;2020-12-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;11\u0026#39;, \u0026#39;68\u0026#39;, \u0026#39;2021-1-11\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;15\u0026#39;, \u0026#39;32\u0026#39;, \u0026#39;2021-1-17\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;12\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-1-19\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;14\u0026#39;, \u0026#39;18\u0026#39;, \u0026#39;2021-1-27\u0026#39;) Truncate table AcceptedRides insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;10\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;63\u0026#39;, \u0026#39;38\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;13\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;73\u0026#39;, \u0026#39;96\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;100\u0026#39;, \u0026#39;28\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;17\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;119\u0026#39;, \u0026#39;68\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;20\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;121\u0026#39;, \u0026#39;92\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;5\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;42\u0026#39;, \u0026#39;101\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;38\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;11\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;37\u0026#39;, \u0026#39;43\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;15\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;108\u0026#39;, \u0026#39;82\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;12\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;38\u0026#39;, \u0026#39;34\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;14\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;90\u0026#39;, \u0026#39;74\u0026#39;) Idea The query result format is in the following example.\n+-------+-----------------------+-----------------------+ | month | average_ride_distance | average_ride_duration | +-------+-----------------------+-----------------------+ | 1 | 21.00 | 12.67 | | 2 | 21.00 | 12.67 | | 3 | 21.00 | 12.67 | | 4 | 24.33 | 32.00 | | 5 | 57.67 | 41.33 | | 6 | 97.33 | 64.00 | | 7 | 73.00 | 32.00 | | 8 | 39.67 | 22.67 | | 9 | 54.33 | 64.33 | | 10 | 56.33 | 77.00 | +-------+-----------------------+-----------------------+ Fulfill requirements :\nBefore all, I would like to share a problem for timeout limit exceeded of this question when I submit my solution. Because of the pervious solution, I used subquery in select column statement to calculate average distance and average duration.\nSo, I’m learned when the query statement meet a large dataset, the subquery will spending more process resource to calculate its.\nFor the above reason, I modifying the output query used function lead() getting next two months distance and duration for each row instead of subquery. From the response of submissions, it’s worked and improve the query performance.\nSolution with cte_month as ( select 1 as month union select 2 as month union select 3 as month union select 4 as month union select 5 as month union select 6 as month union select 7 as month union select 8 as month union select 9 as month union select 10 as month union select 11 as month union select 12 as month ), cte_accepted_rides as ( select c.month, sum(c.distance) as distance, sum(c.duration) as duration from ( select month(b.requested_at) as month, a.ride_distance as distance, a.ride_duration as duration from AcceptedRides a join Rides b using(ride_id) where year(b.requested_at) = 2020 ) c group by c.month ), cte_ride_info as ( select a.month, ifnull(b.distance, 0) as m0_distance, ifnull(b.duration, 0) as m0_duration, ifnull(lead(b.distance, 1) over(order by a.month), 0) as m1_distance, ifnull(lead(b.duration, 1) over(order by a.month), 0) as m1_duration, ifnull(lead(b.distance, 2) over(order by a.month), 0) as m2_distance, ifnull(lead(b.duration, 2) over(order by a.month), 0) as m2_duration from cte_month a left join cte_accepted_rides b on b.month = a.month ) select distinct(a.month), round((m0_distance + m1_distance + m2_distance ) / 3, 2) as average_ride_distance, round((m0_duration + m1_duration + m2_duration ) / 3, 2) as average_ride_duration from cte_ride_info a where a.month \u0026lt;= 10 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/hopper-company-queries-iii/","summary":"Description Table: Drivers\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | driver_id | int | | join_date | date | +-------------+---------+ driver_id is the primary key for this table. Each row of this table contains the driver\u0026#39;s ID and the date they joined the Hopper company. Table: Rides\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | ride_id | int | | user_id | int | | requested_at | date | +--------------+---------+ ride_id is the primary key for this table.","title":"[leetcode][Database][Hard] 1651. Hopper Company Queries III"},{"content":"Description Table: Drivers\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | driver_id | int | | join_date | date | +-------------+---------+ driver_id is the primary key for this table. Each row of this table contains the driver\u0026#39;s ID and the date they joined the Hopper company. Table: Rides\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | ride_id | int | | user_id | int | | requested_at | date | +--------------+---------+ ride_id is the primary key for this table. Each row of this table contains the ID of a ride, the user\u0026#39;s ID that requested it, and the day they requested it. There may be some ride requests in this table that were not accepted. Table: AcceptedRides\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | ride_id | int | | driver_id | int | | ride_distance | int | | ride_duration | int | +---------------+---------+ ride_id is the primary key for this table. Each row of this table contains some information about an accepted ride. It is guaranteed that each accepted ride exists in the Rides table. Write an SQL query to report the percentage of working drivers (working_percentage) for each month of 2020 where:\nNote that if the number of available drivers during a month is zero, we consider the working_percentage to be 0.\nReturn the result table ordered by month in ascending order, where month is the month\u0026rsquo;s number (January is 1, February is 2, etc.). Round working_percentage to the nearest 2 decimal places.\nSQL Schema\nCreate table If Not Exists Drivers (driver_id int, join_date date) Create table If Not Exists Rides (ride_id int, user_id int, requested_at date) Create table If Not Exists AcceptedRides (ride_id int, driver_id int, ride_distance int, ride_duration int) Truncate table Drivers insert into Drivers (driver_id, join_date) values (\u0026#39;10\u0026#39;, \u0026#39;2019-12-10\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;8\u0026#39;, \u0026#39;2020-1-13\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;5\u0026#39;, \u0026#39;2020-2-16\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;7\u0026#39;, \u0026#39;2020-3-8\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;4\u0026#39;, \u0026#39;2020-5-17\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;1\u0026#39;, \u0026#39;2020-10-24\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;6\u0026#39;, \u0026#39;2021-1-5\u0026#39;) Truncate table Rides insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;6\u0026#39;, \u0026#39;75\u0026#39;, \u0026#39;2019-12-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;1\u0026#39;, \u0026#39;54\u0026#39;, \u0026#39;2020-2-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;10\u0026#39;, \u0026#39;63\u0026#39;, \u0026#39;2020-3-4\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;19\u0026#39;, \u0026#39;39\u0026#39;, \u0026#39;2020-4-6\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;3\u0026#39;, \u0026#39;41\u0026#39;, \u0026#39;2020-6-3\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;13\u0026#39;, \u0026#39;52\u0026#39;, \u0026#39;2020-6-22\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;7\u0026#39;, \u0026#39;69\u0026#39;, \u0026#39;2020-7-16\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;17\u0026#39;, \u0026#39;70\u0026#39;, \u0026#39;2020-8-25\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;20\u0026#39;, \u0026#39;81\u0026#39;, \u0026#39;2020-11-2\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;5\u0026#39;, \u0026#39;57\u0026#39;, \u0026#39;2020-11-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;2\u0026#39;, \u0026#39;42\u0026#39;, \u0026#39;2020-12-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;11\u0026#39;, \u0026#39;68\u0026#39;, \u0026#39;2021-1-11\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;15\u0026#39;, \u0026#39;32\u0026#39;, \u0026#39;2021-1-17\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;12\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-1-19\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;14\u0026#39;, \u0026#39;18\u0026#39;, \u0026#39;2021-1-27\u0026#39;) Truncate table AcceptedRides insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;10\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;63\u0026#39;, \u0026#39;38\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;13\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;73\u0026#39;, \u0026#39;96\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;100\u0026#39;, \u0026#39;28\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;17\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;119\u0026#39;, \u0026#39;68\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;20\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;121\u0026#39;, \u0026#39;92\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;5\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;42\u0026#39;, \u0026#39;101\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;38\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;11\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;37\u0026#39;, \u0026#39;43\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;15\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;108\u0026#39;, \u0026#39;82\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;12\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;38\u0026#39;, \u0026#39;34\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;14\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;90\u0026#39;, \u0026#39;74\u0026#39;) Idea The query result format is in the following example.\n+-------+--------------------+ | month | working_percentage | +-------+--------------------+ | 1 | 0.00 | | 2 | 0.00 | | 3 | 25.00 | | 4 | 0.00 | | 5 | 0.00 | | 6 | 20.00 | | 7 | 20.00 | | 8 | 20.00 | | 9 | 0.00 | | 10 | 0.00 | | 11 | 33.33 | | 12 | 16.67 | +-------+--------------------+ Fulfill requirements :\nThe thinking process like as [leetcode][Database][Hard] 1635. Hopper Company Queries I , cumulating available drivers by month who join date less than year 2021.\nIt’s only different to counting _drivers that accepted at least one of each month_ in table AcceptedRides and table Rides instead of ride records.\nSolution with cte_month as ( select 1 as month union select 2 as month union select 3 as month union select 4 as month union select 5 as month union select 6 as month union select 7 as month union select 8 as month union select 9 as month union select 10 as month union select 11 as month union select 12 as month ), cte_monthly_drivers as ( select driver_id, if(year(join_date) \u0026lt; 2020, 1, month(join_date)) as month from Drivers where year(join_date) \u0026lt;= 2020 ), cte_monthly_rides as ( select distinct(a.month), count(a.driver_id) over(partition by a.month) as accepted_ride_drivers from ( select distinct(driver_id) as driver_id, month(b.requested_at) as month from AcceptedRides a join Rides b using (ride_id) where year(b.requested_at) = 2020 ) a ) select opt.month, if( opt.available_drivers=0, 0, round((accepted_ride_drivers / opt.available_drivers)*100, 2) ) as working_percentage from ( select distinct(a.month), count(b.driver_id) over(order by a.month) as available_drivers, ifnull(c.accepted_ride_drivers, 0) as accepted_ride_drivers from cte_month a left join cte_monthly_drivers b on b.month = a.month left join cte_monthly_rides c on c.month = a.month ) opt ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/hopper-company-queries-ii/","summary":"Description Table: Drivers\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | driver_id | int | | join_date | date | +-------------+---------+ driver_id is the primary key for this table. Each row of this table contains the driver\u0026#39;s ID and the date they joined the Hopper company. Table: Rides\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | ride_id | int | | user_id | int | | requested_at | date | +--------------+---------+ ride_id is the primary key for this table.","title":"[leetcode][Database][Hard] 1645. Hopper Company Queries II"},{"content":"Description Table: Drivers\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | driver_id | int | | join_date | date | +-------------+---------+ driver_id is the primary key for this table. Each row of this table contains the driver\u0026#39;s ID and the date they joined the Hopper company. Table: Rides\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | ride_id | int | | user_id | int | | requested_at | date | +--------------+---------+ ride_id is the primary key for this table. Each row of this table contains the ID of a ride, the user\u0026#39;s ID that requested it, and the day they requested it. There may be some ride requests in this table that were not accepted. Table: AcceptedRides\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | ride_id | int | | driver_id | int | | ride_distance | int | | ride_duration | int | +---------------+---------+ ride_id is the primary key for this table. Each row of this table contains some information about an accepted ride. It is guaranteed that each accepted ride exists in the Rides table. Write an SQL query to report the following statistics for each month of 2020:\nThe number of drivers currently with the Hopper company by the end of the month (active_drivers). The number of accepted rides in that month (accepted_rides). Return the result table ordered by month in ascending order, where month is the month\u0026rsquo;s number (January is 1, February is 2, etc.).\nSQL Schema\nCreate table If Not Exists Drivers (driver_id int, join_date date) Create table If Not Exists Rides (ride_id int, user_id int, requested_at date) Create table If Not Exists AcceptedRides (ride_id int, driver_id int, ride_distance int, ride_duration int) Truncate table Drivers insert into Drivers (driver_id, join_date) values (\u0026#39;10\u0026#39;, \u0026#39;2019-12-10\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;8\u0026#39;, \u0026#39;2020-1-13\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;5\u0026#39;, \u0026#39;2020-2-16\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;7\u0026#39;, \u0026#39;2020-3-8\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;4\u0026#39;, \u0026#39;2020-5-17\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;1\u0026#39;, \u0026#39;2020-10-24\u0026#39;) insert into Drivers (driver_id, join_date) values (\u0026#39;6\u0026#39;, \u0026#39;2021-1-5\u0026#39;) Truncate table Rides insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;6\u0026#39;, \u0026#39;75\u0026#39;, \u0026#39;2019-12-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;1\u0026#39;, \u0026#39;54\u0026#39;, \u0026#39;2020-2-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;10\u0026#39;, \u0026#39;63\u0026#39;, \u0026#39;2020-3-4\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;19\u0026#39;, \u0026#39;39\u0026#39;, \u0026#39;2020-4-6\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;3\u0026#39;, \u0026#39;41\u0026#39;, \u0026#39;2020-6-3\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;13\u0026#39;, \u0026#39;52\u0026#39;, \u0026#39;2020-6-22\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;7\u0026#39;, \u0026#39;69\u0026#39;, \u0026#39;2020-7-16\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;17\u0026#39;, \u0026#39;70\u0026#39;, \u0026#39;2020-8-25\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;20\u0026#39;, \u0026#39;81\u0026#39;, \u0026#39;2020-11-2\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;5\u0026#39;, \u0026#39;57\u0026#39;, \u0026#39;2020-11-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;2\u0026#39;, \u0026#39;42\u0026#39;, \u0026#39;2020-12-9\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;11\u0026#39;, \u0026#39;68\u0026#39;, \u0026#39;2021-1-11\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;15\u0026#39;, \u0026#39;32\u0026#39;, \u0026#39;2021-1-17\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;12\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-1-19\u0026#39;) insert into Rides (ride_id, user_id, requested_at) values (\u0026#39;14\u0026#39;, \u0026#39;18\u0026#39;, \u0026#39;2021-1-27\u0026#39;) Truncate table AcceptedRides insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;10\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;63\u0026#39;, \u0026#39;38\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;13\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;73\u0026#39;, \u0026#39;96\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;100\u0026#39;, \u0026#39;28\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;17\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;119\u0026#39;, \u0026#39;68\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;20\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;121\u0026#39;, \u0026#39;92\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;5\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;42\u0026#39;, \u0026#39;101\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;38\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;11\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;37\u0026#39;, \u0026#39;43\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;15\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;108\u0026#39;, \u0026#39;82\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;12\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;38\u0026#39;, \u0026#39;34\u0026#39;) insert into AcceptedRides (ride_id, driver_id, ride_distance, ride_duration) values (\u0026#39;14\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;90\u0026#39;, \u0026#39;74\u0026#39;) Idea The query result format is in the following example.\n+-------+----------------+----------------+ | month | active_drivers | accepted_rides | +-------+----------------+----------------+ | 1 | 2 | 0 | | 2 | 3 | 0 | | 3 | 4 | 1 | | 4 | 4 | 0 | | 5 | 5 | 0 | | 6 | 5 | 1 | | 7 | 5 | 1 | | 8 | 5 | 1 | | 9 | 5 | 0 | | 10 | 6 | 0 | | 11 | 6 | 2 | | 12 | 6 | 1 | +-------+----------------+----------------+ Fulfill requirements:\nThe output, it apparently show the computed result of each month. So, we can generate a monthly table that month range between 1 (Jan) to 12 (Dec) as a main table in the query statement, then cumulating the active drivers and counting the accepted rides of each month by left join.\nSolution with cte_month as ( select 1 as month union select 2 as month union select 3 as month union select 4 as month union select 5 as month union select 6 as month union select 7 as month union select 8 as month union select 9 as month union select 10 as month union select 11 as month union select 12 as month ), cte_drivers as ( select driver_id, join_date, if(year(join_date)\u0026lt;2020, 2020, year(join_date)) as join_year, if(year(join_date)\u0026lt;2020, 1, month(join_date)) as join_month from Drivers where year(join_date) \u0026lt;= 2020 ), cte_accepted_rides as ( select distinct(month(b.requested_at)) as month, count(ride_id) over(partition by month(b.requested_at)) as accepted_rides from AcceptedRides a join Rides b using(ride_id) where year(b.requested_at)=2020 ) select distinct(a.month) as month, count(b.driver_id) over(order by a.month) as active_drivers, ifnull(c.accepted_rides, 0) as accepted_rides from cte_month a left join cte_drivers b on b.join_month = a.month left join cte_accepted_rides c on c.month = a.month ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/hopper-company-queries-i/","summary":"Description Table: Drivers\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | driver_id | int | | join_date | date | +-------------+---------+ driver_id is the primary key for this table. Each row of this table contains the driver\u0026#39;s ID and the date they joined the Hopper company. Table: Rides\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | ride_id | int | | user_id | int | | requested_at | date | +--------------+---------+ ride_id is the primary key for this table.","title":"[leetcode][Database][Hard] 1635. Hopper Company Queries I"},{"content":"Description Table: Student\n+---------------------+---------+ | Column Name | Type | +---------------------+---------+ | student_id | int | | student_name | varchar | +---------------------+---------+ student_id is the primary key for this table. student_name is the name of the student. Table: Exam\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | exam_id | int | | student_id | int | | score | int | +---------------+---------+ (exam_id, student_id) is the primary key for this table. Each row of this table indicates that the student with student_id had a score points in the exam with id exam_id. A quiet student is the one who took at least one exam and did not score the high or the low score.\nWrite an SQL query to report the students (student_id, student_name) being quiet in all exams. Do not return the student who has never taken any exam.\nReturn the result table ordered by student_id.\nSQL Schema\nCreate table If Not Exists Student (student_id int, student_name varchar(30)) Create table If Not Exists Exam (exam_id int, student_id int, score int) Truncate table Student insert into Student (student_id, student_name) values (\u0026#39;1\u0026#39;, \u0026#39;Daniel\u0026#39;) insert into Student (student_id, student_name) values (\u0026#39;2\u0026#39;, \u0026#39;Jade\u0026#39;) insert into Student (student_id, student_name) values (\u0026#39;3\u0026#39;, \u0026#39;Stella\u0026#39;) insert into Student (student_id, student_name) values (\u0026#39;4\u0026#39;, \u0026#39;Jonathan\u0026#39;) insert into Student (student_id, student_name) values (\u0026#39;5\u0026#39;, \u0026#39;Will\u0026#39;) Truncate table Exam insert into Exam (exam_id, student_id, score) values (\u0026#39;10\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;70\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;10\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;80\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;10\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;90\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;20\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;80\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;30\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;70\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;30\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;80\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;30\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;90\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;40\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;60\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;40\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;70\u0026#39;) insert into Exam (exam_id, student_id, score) values (\u0026#39;40\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;80\u0026#39;) Idea The query result format is in the following example.\n+-------------+---------------+ | student_id | student_name | +-------------+---------------+ | 2 | Jade | +-------------+---------------+ It has easy way to find quiet students by ranking score of each exam, getting best and worst rank of each exam, marking best and worst for each student of each exam if they are best score or worst score at the exam.\nFinally, listing the students who never got best mark and worst mark of all exam.\nSolution with rank_score_of_exam as ( select a.exam_id, a.student_id, b.student_name, dense_rank() over(partition by a.exam_id order by a.score) as score_rk from exam a left join student b using(student_id) ), rank_of_exam as ( select exam_id, max(score_rk) as high, min(score_rk) as low from rank_score_of_exam group by exam_id ), mark_best as ( select a.exam_id, a.student_id, a.student_name, if(a.score_rk=b.high, 1, 0) as mark from rank_score_of_exam a left join rank_of_exam b using(exam_id) ), mark_worst as ( select a.exam_id, a.student_id, a.student_name, if(a.score_rk=b.low, 1, 0) as mark from rank_score_of_exam a left join rank_of_exam b using(exam_id) ) select student_id, student_name from ( select exam_id, student_id, student_name, mark from mark_best union all select exam_id, student_id, student_name, mark from mark_worst ) opt group by student_id, student_name having sum(mark) = 0 order by student_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/find-the-quiet-students-in-all-exams/","summary":"Description Table: Student\n+---------------------+---------+ | Column Name | Type | +---------------------+---------+ | student_id | int | | student_name | varchar | +---------------------+---------+ student_id is the primary key for this table. student_name is the name of the student. Table: Exam\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | exam_id | int | | student_id | int | | score | int | +---------------+---------+ (exam_id, student_id) is the primary key for this table.","title":"[leetcode][Database][Hard] 1412. Find the Quiet Students in All Exams"},{"content":"Description Table: UserActivity\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | username | varchar | | activity | varchar | | startDate | Date | | endDate | Date | +---------------+---------+ There is no primary key for this table. It may contain duplicates. This table contains information about the activity performed by each user in a period of time. A person with username performed an activity from startDate to endDate. Write an SQL query to show the second most recent activity of each user.\nIf the user only has one activity, return that one. A user cannot perform more than one activity at the same time.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists UserActivity (username varchar(30), activity varchar(30), startDate date, endDate date) Truncate table UserActivity insert into UserActivity (username, activity, startDate, endDate) values (\u0026#39;Alice\u0026#39;, \u0026#39;Travel\u0026#39;, \u0026#39;2020-02-12\u0026#39;, \u0026#39;2020-02-20\u0026#39;) insert into UserActivity (username, activity, startDate, endDate) values (\u0026#39;Alice\u0026#39;, \u0026#39;Dancing\u0026#39;, \u0026#39;2020-02-21\u0026#39;, \u0026#39;2020-02-23\u0026#39;) insert into UserActivity (username, activity, startDate, endDate) values (\u0026#39;Alice\u0026#39;, \u0026#39;Travel\u0026#39;, \u0026#39;2020-02-24\u0026#39;, \u0026#39;2020-02-28\u0026#39;) insert into UserActivity (username, activity, startDate, endDate) values (\u0026#39;Bob\u0026#39;, \u0026#39;Travel\u0026#39;, \u0026#39;2020-02-11\u0026#39;, \u0026#39;2020-02-18\u0026#39;) Idea The query result format is in the following example.\n+------------+--------------+-------------+-------------+ | username | activity | startDate | endDate | +------------+--------------+-------------+-------------+ | Alice | Dancing | 2020-02-21 | 2020-02-23 | | Bob | Travel | 2020-02-11 | 2020-02-18 | +------------+--------------+-------------+-------------+ Fulfill requirements :\nTo generate a rank table order by endDate of each user via function rank()as rn. It will help to find user who has at least twice activity records in table UserActivity.\nName this with clause as rank_end_date. Find _second mostly recent activity_ record of each user from rand_end_date. User will be missing who has only one activity record.\nName this with clause as activity_twice. Find users who has only once activity record of table UserActivity. Using left join activity_twice to rule out users who has been listing in twice_record.\nName this with clause as activity_once . Finally, union the result of activity_twice and twice_record for output of the query. Solution with rank_end_date as ( select rank() over(partition by username order by endDate desc) as rn, username, activity, startDate, endDate from useractivity ), activity_twice as ( select rn, username, activity, startDate, endDate from rank_end_date where rn = 2 ), activity_once as ( select a.rn, a.username, a.activity, a.startDate, a.endDate from rank_end_date a left join activity_twice b using(username) where ifnull(b.rn, 0) = 0 ) select username, activity, startDate, endDate from activity_once union select username, activity, startDate, endDate from activity_twice ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/get-the-second-most-recent-activity/","summary":"Description Table: UserActivity\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | username | varchar | | activity | varchar | | startDate | Date | | endDate | Date | +---------------+---------+ There is no primary key for this table. It may contain duplicates. This table contains information about the activity performed by each user in a period of time. A person with username performed an activity from startDate to endDate.","title":"[leetcode][Database][Hard] 1369. Get the Second Most Recent Activity"},{"content":"Description Table: Visits\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user_id | int | | visit_date | date | +---------------+---------+ (user_id, visit_date) is the primary key for this table. Each row of this table indicates that user_id has visited the bank in visit_date. Table: Transactions\n+------------------+---------+ | Column Name | Type | +------------------+---------+ | user_id | int | | transaction_date | date | | amount | int | +------------------+---------+ There is no primary key for this table, it may contain duplicates. Each row of this table indicates that user_id has done a transaction of amount in transaction_date. It is guaranteed that the user has visited the bank in the transaction_date.(i.e The Visits table contains (user_id, transaction_date) in one row) A bank wants to draw a chart of the number of transactions bank visitors did in one visit to the bank and the corresponding number of visitors who have done this number of transaction in one visit.\nWrite an SQL query to find how many users visited the bank and didn’t do any transactions, how many visited the bank and did one transaction and so on.\nThe result table will contain two columns:\ntransactions_count which is the number of transactions done in one visit. visits_count which is the corresponding number of users who did transactions_count in one visit to the bank. transactions_count should take all values from 0 to max(transactions_count) done by one or more users.\nReturn the result table ordered by transactions_count.\nSQL Schema\nCreate table If Not Exists Visits (user_id int, visit_date date) Create table If Not Exists Transactions (user_id int, transaction_date date, amount int) Truncate table Visits insert into Visits (user_id, visit_date) values (\u0026#39;1\u0026#39;, \u0026#39;2020-01-01\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;2\u0026#39;, \u0026#39;2020-01-02\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;12\u0026#39;, \u0026#39;2020-01-01\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;19\u0026#39;, \u0026#39;2020-01-03\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;1\u0026#39;, \u0026#39;2020-01-02\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;2\u0026#39;, \u0026#39;2020-01-03\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;1\u0026#39;, \u0026#39;2020-01-04\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;7\u0026#39;, \u0026#39;2020-01-11\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;9\u0026#39;, \u0026#39;2020-01-25\u0026#39;) insert into Visits (user_id, visit_date) values (\u0026#39;8\u0026#39;, \u0026#39;2020-01-28\u0026#39;) Truncate table Transactions insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;1\u0026#39;, \u0026#39;2020-01-02\u0026#39;, \u0026#39;120\u0026#39;) insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;2\u0026#39;, \u0026#39;2020-01-03\u0026#39;, \u0026#39;22\u0026#39;) insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;7\u0026#39;, \u0026#39;2020-01-11\u0026#39;, \u0026#39;232\u0026#39;) insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;1\u0026#39;, \u0026#39;2020-01-04\u0026#39;, \u0026#39;7\u0026#39;) insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;9\u0026#39;, \u0026#39;2020-01-25\u0026#39;, \u0026#39;33\u0026#39;) insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;9\u0026#39;, \u0026#39;2020-01-25\u0026#39;, \u0026#39;66\u0026#39;) insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;8\u0026#39;, \u0026#39;2020-01-28\u0026#39;, \u0026#39;1\u0026#39;) insert into Transactions (user_id, transaction_date, amount) values (\u0026#39;9\u0026#39;, \u0026#39;2020-01-25\u0026#39;, \u0026#39;99\u0026#39;) Idea The output requires result table contains 2 columns : transactions_count and visits_count , but also the column transactions_count range between 0 to max(transactions_count). Finally, output the result table ordered by transactions_count.\nfor example:\n+--------------------+--------------+ | transactions_count | visits_count | +--------------------+--------------+ | 0 | 4 | | 1 | 5 | | 2 | 0 | | 3 | 1 | +--------------------+--------------+ Fultill requirements:\nGenerate a serial number table start with 0 by function row_number() of transactions table due to max(transactions_count) should be less tahn or equals to the number of records of transactions table.\nName this with clause as sn . Compute the transactions_count from records of transactions table, where the records matches the user_id from visits table, but also the transaction_date in visit_date. Name this with clause as . Joint sn and cte_1, where row_number() of sn should be less than or equals tomax(cte_1.transactions_count) . It will help to confirm all of sn.rn rows where thesn.rn less than or equals tothan max(cte_1.transactions_count) should be in result table, because the cte_1 will not include row if the visits_count are zero times.\nName this with clause as cte_2. Finally, named column visits_count for counting how many users group by cte_2.transactions_count , and output it. Solution with sn as ( select row_number() over() -1 rn from transactions ), cte_1 as ( select a.user_id, a.visit_date, count(b.transaction_date)as transactions_count from visits a left join transactions b on b.user_id = a.user_id and b.transaction_date = a.visit_date group by a.user_id, a.visit_date ), cte_2 as ( select user_id, visit_date, transactions_count from cte_1 union all select null, null, rn from sn where rn \u0026lt; (select max(transactions_count) from cte_1) ) select transactions_count, count(user_id) as visits_count from cte_2 group by transactions_count order by transactions_count ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/number-of-transactions-per-visit/","summary":"Description Table: Visits\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | user_id | int | | visit_date | date | +---------------+---------+ (user_id, visit_date) is the primary key for this table. Each row of this table indicates that user_id has visited the bank in visit_date. Table: Transactions\n+------------------+---------+ | Column Name | Type | +------------------+---------+ | user_id | int | | transaction_date | date | | amount | int | +------------------+---------+ There is no primary key for this table, it may contain duplicates.","title":"[leetcode][Database][Hard] 1336. Number of Transactions per Visit"},{"content":"記錄在過去面試中常被問到的問題，以及依據自己的理解與查找到的資料，整理對應的回覆\nQ1. K8s 有哪些 components ?\nMaster Node ← 管理 cluster、儲存不同 node 的資訊、規劃 containers 的去向，以及 monitor node 上的 containers\nkube-apiserver ← 管理整個 K8s 的 interface。使用者可以透過下達指令給 kube-apiserver ，以達到管理 K8s resources 的目的。 etcd cluster ← 儲存 K8s cluster 內，所有 node 、 containers 的資訊 kube-scheduler ← 依據 containers 的需求，包含但不限於: cpu、 menory、 affinity 、 taints and tolerations 、anti-* 等等，規劃將 containers 指派給符合需求的 node。 controller manager ← 管控 K8s cluster 內的 resources 。\n如 node controller 管理 nodes : 加入新 node 到 cluster、處理 node 變的不可用的狀況 Work Nodes ← 託管執行應用的 containers\nkubelet ← K8s cluster 在 work nodes 上的代理，偵聽 kube-apiserver 的指令，並依據需求部署或銷毀 containers。 kube-apiserver 週期性的從kubelet 獲取 work node 和 containers 的狀態。 kube-proxy ← 讓 work node 之間的 containers 可以互相溝通。 container runtime engine ← 執行包含應用的 containers Q2. 部署一個應用到 K8s 時，K8s 會如何運作整個流程 ?\nUser 發送 deploy request 給 kube-apiserver。kube-apiserver 驗證 User 並驗證是否為有效的 request。 將有效的 request 更新到 ETCD。 ETCD 回覆 kube-apiserver 更新已完成。 kube-scheduler 會持續監測 kube-apiserver，此時發現有一個新的 pod 還沒有被指派到 work node， kube-scheduler 會選擇符合條件的 work node。 kube-scheduler 通知 kube-apiserver ，規畫將新的 pod 分配到指定的 work node。 kube-apiserver 通知 work node 上的 kubelet，有新的 pod 需要部署到 work node 上。 kubelet 嘗試將新的 pod 部署到 work node 上，並持續監測 pod 的狀態。 新的 pod 開始部署到 work node 。 新的 pod 部署完成。 kubelet 監測到 pod 已部署 (但不保證 pod 上的應用執行是否成功)。 kubelet 通知 kube-apiserver 新的 pod 已部署到 work node。 kube-apiserver 將 pod 部署資訊更新到 ETCD 。 ETCD 回覆 kube-apiserver 更新已完成。 kube-apiserver 回覆 User 新的 pod 已部署到 work node。 Q3. 對於在 K8s 上建置正式環境 (Product) 和測試環境 (Development)的規劃\n在 K8s 分別建立 Product 和 Development 的 namespace，並可透過指令 kubectl apply -f {YAML file} -n [ Product | Development ] ，將需要的 resources 部署到對應的 namespace 。 若需要權限管理，則透過 RBAC 進行設置。我常見的作法是先建立 K8s 的 service account 、建立 Role 或 ClusterRole 並設定可操作的 resources 與對應的 verbs，最後透過 RoleBinding 或 ClusterRoleBinding 將 service account 和 Role(或ClusterRole)進行綁定。 ","permalink":"https://blog.zhengweiliu.com/posts/normal/kubernetes/","summary":"記錄在過去面試中常被問到的問題，以及依據自己的理解與查找到的資料，整理對應的回覆\nQ1. K8s 有哪些 components ?\nQ2. 部署一個應用到 K8s 時，K8s 會如何運作整個流程 ?\nQ3. 對於在 K8s 上建置正式環境 (Product) 和測試環境 (Development)的規劃","title":"整理面試常見問題 — Kubernetes"},{"content":"題目 Table: Failed\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | fail_date | date | +--------------+---------+ fail_date is the primary key for this table. This table contains the days of failed tasks. Table: Succeeded\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | success_date | date | +--------------+---------+ success_date is the primary key for this table. This table contains the days of succeeded tasks. A system is running one task every day. Every task is independent of the previous tasks. The tasks can fail or succeed.\nWrite an SQL query to generate a report of period_state for each continuous interval of days in the period from 2019-01-01 to 2019-12-31.\nperiod_state is \u0026rsquo;failed' if tasks in this interval failed or 'succeeded' if tasks in this interval succeeded. Interval of days are retrieved as start_date and end_date.\nReturn the result table ordered by start_date.\nSQL Schema\nCreate table If Not Exists Failed (fail_date date) Create table If Not Exists Succeeded (success_date date) Truncate table Failed insert into Failed (fail_date) values (\u0026#39;2018-12-28\u0026#39;) insert into Failed (fail_date) values (\u0026#39;2018-12-29\u0026#39;) insert into Failed (fail_date) values (\u0026#39;2019-01-04\u0026#39;) insert into Failed (fail_date) values (\u0026#39;2019-01-05\u0026#39;) Truncate table Succeeded insert into Succeeded (success_date) values (\u0026#39;2018-12-30\u0026#39;) insert into Succeeded (success_date) values (\u0026#39;2018-12-31\u0026#39;) insert into Succeeded (success_date) values (\u0026#39;2019-01-01\u0026#39;) insert into Succeeded (success_date) values (\u0026#39;2019-01-02\u0026#39;) insert into Succeeded (success_date) values (\u0026#39;2019-01-03\u0026#39;) insert into Succeeded (success_date) values (\u0026#39;2019-01-06\u0026#39;) 解題思考 (Variable 版本) 題目要求輸出 period_state 、 start_date 和 end_date 欄位的表格。\n其中將連續日期都為相同 state 的定義為一個 period，且 start_date 和 end_date 將表示該 period 的起始日期和結束日期。\n+--------------+--------------+--------------+ | period_state | start_date | end_date | +--------------+--------------+--------------+ | succeeded | 2019-01-01 | 2019-01-03 | | failed | 2019-01-04 | 2019-01-05 | | succeeded | 2019-01-06 | 2019-01-06 | +--------------+--------------+--------------+ 最開始，我的想法是利用 variable，在每次 period_state 變換時，透過變數將該 period_state第一個日期作為錨點 anchor ，再使用 rank() 對 start_date 做升序排序 rn。 透過 with clause 建立 status 表格，聯合 Failed 表格和 Successed 表格，篩選出 fail_date 、 success_date 介於 2019–01–01 至 2019-12-31 的資料集。 透過 with clause 建立 date_anchor 表格，擴增 anchor_date欄位以便後續找出 end_date 。\nvariable current_state ← 儲存當前 record 的 period_state\nvariable date_anchor ← 儲存當前 period 的第一個日期\n透過子查詢初始化 variable current_state 和 variable date_anchor 查詢 date_anchor 以建立子查詢表格 a 。\n子查詢表格中，利用 rank() 對 start_date 做升序，以標記排序結果 rn 。並利用 max(start_date) over(partition by anchor_date) 取得每個 period 中最大的 start_date ，即該 period 的 end_date 。 查詢 a 作為主要表格，利用 group by end_date 和 having min(rn) 以篩選出符合輸出條件的資料。 解決方案 (Variable 版本) with status as ( select period_state, start_date from ( select fail_date as start_date, \u0026#39;failed\u0026#39; as period_state from Failed union all select success_date as start_date, \u0026#39;succeeded\u0026#39; as period_state from Succeeded ) tmp where start_date between \u0026#39;2019-01-01\u0026#39; and \u0026#39;2019-12-31\u0026#39; order by start_date ), date_anchor as ( select period_state, start_date, anchor_date from( select a.period_state, a.start_date, if(@current_state=a.period_state, @date_anchor, a.start_date) as anchor_date, if(@current_state=a.period_state, @date_anchor, @date_anchor:=a.start_date), @current_state:=a.period_state from status a, (select @current_state:=\u0026#34;initial\u0026#34;, @date_anchor:=\u0026#34;2018-12-31\u0026#34;) init ) anchor ) select a.period_state, a.start_date, a.end_date from ( select period_state, start_date, max(start_date) over(partition by anchor_date) as end_date, rank() over(order by start_date) rn from date_anchor ) a group by end_date having min(rn) order by start_date 解題思考 (Rank版本) 在提交了 variable 版本後，發現連續日期在排序後，其 rn 等差為 1 ，這表示每次 period_state 變換時，取第一個日期作為該 period 的最小值，則該 period 中所有資料都應符合 start_date-rn=min(start_date) 的條件，因此又寫了一個 Rank 版本。 透過 with clause 分別建立 failed_state 、 successeded_state表格。\n篩選資料時間介於 2019–01–01 至 2019-12-31 的資料集，\n擴增 period_state 欄位，\n利用 rank() over(order by ) 標註排序結果 rn 。 做子查詢表格 opt，聯合 failed_state 和 successeded_state 。\n在 failed_state 表格，以 group by period_state, date_add(fail_date, interval -rn day) ，分別找出failed_state 和 successeded_state，每個 period 的 min(date) 和 max(date) ，作為輸出結果的 start_date 和 end_date 欄位。 解決方案 (Rank版本) with failed_state as ( select \u0026#39;failed\u0026#39; as period_state, fail_date, rank() over(order by fail_date) -1 as rn from Failed where fail_date between \u0026#39;2019-01-01\u0026#39; and \u0026#39;2019-12-31\u0026#39; ), succeeded_state as ( select \u0026#39;succeeded\u0026#39; as period_state, success_date, rank() over(order by success_date) -1 as rn from Succeeded where success_date between \u0026#39;2019-01-01\u0026#39; and \u0026#39;2019-12-31\u0026#39; ) select period_state, start_date, end_date from ( select period_state, min(fail_date) as start_date, max(fail_date) as end_date from failed_state group by period_state, date_add(fail_date, interval -rn day) union select period_state, min(success_date) as start_date, max(success_date) as end_date from succeeded_state group by period_state, date_add(success_date, interval -rn day) ) opt ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/report-contiguous-dates/","summary":"題目 Table: Failed\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | fail_date | date | +--------------+---------+ fail_date is the primary key for this table. This table contains the days of failed tasks. Table: Succeeded\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | success_date | date | +--------------+---------+ success_date is the primary key for this table. This table contains the days of succeeded tasks. A system is running one task every day.","title":"[leetcode][Database][Hard] 1225. Report Contiguous Dates"},{"content":"題目\nTable: Employee\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | id | int | | name | varchar | | salary | int | | departmentId | int | +--------------+---------+ id is the primary key column for this table. departmentId is a foreign key of the ID from the Department table. Each row of this table indicates the ID, name, and salary of an employee. It also contains the ID of their department. Table: Department\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | id | int | | name | varchar | +-------------+---------+ id is the primary key column for this table. Each row of this table indicates the ID of a department and its name. A company’s executives are interested in seeing who earns the most money in each of the company’s departments. A high earner in a department is an employee who has a salary in the top three unique salaries for that department.\nWrite an SQL query to find the employees who are high earners in each of the departments.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Employee (id int, name varchar(255), salary int, departmentId int) Create table If Not Exists Department (id int, name varchar(255)) Truncate table Employee insert into Employee (id, name, salary, departmentId) values (\u0026#39;1\u0026#39;, \u0026#39;Joe\u0026#39;, \u0026#39;85000\u0026#39;, \u0026#39;1\u0026#39;) insert into Employee (id, name, salary, departmentId) values (\u0026#39;2\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;80000\u0026#39;, \u0026#39;2\u0026#39;) insert into Employee (id, name, salary, departmentId) values (\u0026#39;3\u0026#39;, \u0026#39;Sam\u0026#39;, \u0026#39;60000\u0026#39;, \u0026#39;2\u0026#39;) insert into Employee (id, name, salary, departmentId) values (\u0026#39;4\u0026#39;, \u0026#39;Max\u0026#39;, \u0026#39;90000\u0026#39;, \u0026#39;1\u0026#39;) insert into Employee (id, name, salary, departmentId) values (\u0026#39;5\u0026#39;, \u0026#39;Janet\u0026#39;, \u0026#39;69000\u0026#39;, \u0026#39;1\u0026#39;) insert into Employee (id, name, salary, departmentId) values (\u0026#39;6\u0026#39;, \u0026#39;Randy\u0026#39;, \u0026#39;85000\u0026#39;, \u0026#39;1\u0026#39;) insert into Employee (id, name, salary, departmentId) values (\u0026#39;7\u0026#39;, \u0026#39;Will\u0026#39;, \u0026#39;70000\u0026#39;, \u0026#39;1\u0026#39;) Truncate table Department insert into Department (id, name) values (\u0026#39;1\u0026#39;, \u0026#39;IT\u0026#39;) insert into Department (id, name) values (\u0026#39;2\u0026#39;, \u0026#39;Sales\u0026#39;) 解題思考\n題目要求輸出每個部門中，收入排名前三高的員工。\n若有收入相同且位於收入排名前三高的員工，也併入輸出結果。 +------------+----------+--------+ | Department | Employee | Salary | +------------+----------+--------+ | IT | Max | 90000 | | IT | Joe | 85000 | | IT | Randy | 85000 | | IT | Will | 70000 | | Sales | Henry | 80000 | | Sales | Sam | 60000 | +------------+----------+--------+ 透過 with clause 建立 employee_info 表格，以提供每個部門內，每位員工的薪水排序結果。\n使用 dense_rank() 函式進行排序，dense_rank() 會以連續數字的方式給予排序結果 rn。如下情況 +------------+----------+--------+--------------+ | Department | Employee | Salary | dense_rank() | +------------+----------+--------+--------------+ | IT | Max | 90000 | 1 | | IT | Joe | 85000 | 2 | | IT | Randy | 85000 | 2 | | IT | Will | 70000 | 3 | | Sales | Henry | 80000 | 1 | | Sales | Sam | 60000 | 2 | +------------+----------+--------+--------------+ 查詢 employee_info 作為主要表格，過濾條件以找出 employee_info.rn \u0026lt; 4 的 record set，即找出每個部門，收入排名前三高的員工。 解決方案\nwith employee_info as ( select Department.id as departmentId , Department.name as Department , Employee.name as Employee, Employee.Salary as Salary, dense_rank() over(partition by Department.id order by Employee.Salary desc ) rn from Employee join Department on Department.id = Employee.departmentId ) select employee_info.Department, employee_info.Employee, employee_info.Salary from employee_info where employee_info.rn \u0026lt; 4 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/department-top-three-salaries/","summary":"題目\nTable: Employee\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | id | int | | name | varchar | | salary | int | | departmentId | int | +--------------+---------+ id is the primary key column for this table. departmentId is a foreign key of the ID from the Department table. Each row of this table indicates the ID, name, and salary of an employee. It also contains the ID of their department.","title":"[leetcode][Database][Hard] 185. Department Top Three Salaries"},{"content":"題目 Table: Trips\n+-------------+----------+ | Column Name | Type | +-------------+----------+ | id | int | | client_id | int | | driver_id | int | | city_id | int | | status | enum | | request_at | date | +-------------+----------+ id is the primary key for this table. The table holds all taxi trips. Each trip has a unique id, while client_id and driver_id are foreign keys to the users_id at the Users table. Status is an ENUM type of (\u0026#39;completed\u0026#39;, \u0026#39;cancelled_by_driver\u0026#39;, \u0026#39;cancelled_by_client\u0026#39;). Table: Users\n+-------------+----------+ | Column Name | Type | +-------------+----------+ | users_id | int | | banned | enum | | role | enum | +-------------+----------+ users_id is the primary key for this table. The table holds all users. Each user has a unique users_id, and role is an ENUM type of (\u0026#39;client\u0026#39;, \u0026#39;driver\u0026#39;, \u0026#39;partner\u0026#39;). banned is an ENUM type of (\u0026#39;Yes\u0026#39;, \u0026#39;No\u0026#39;). The cancellation rate is computed by dividing the number of canceled (by client or driver) requests with unbanned users by the total number of requests with unbanned users on that day.\nWrite a SQL query to find the cancellation rate of requests with unbanned users (both client and driver must not be banned) each day between \u0026quot;2013-10-01\u0026quot; and \u0026quot;2013-10-03\u0026quot;. Round Cancellation Rate to two decimal points.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Trips (id int, client_id int, driver_id int, city_id int, status ENUM(\u0026#39;completed\u0026#39;, \u0026#39;cancelled_by_driver\u0026#39;, \u0026#39;cancelled_by_client\u0026#39;), request_at varchar(50)) Create table If Not Exists Users (users_id int, banned varchar(50), role ENUM(\u0026#39;client\u0026#39;, \u0026#39;driver\u0026#39;, \u0026#39;partner\u0026#39;)) Truncate table Trips insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;2013-10-01\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;cancelled_by_driver\u0026#39;, \u0026#39;2013-10-01\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;3\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;2013-10-01\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;4\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;13\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;cancelled_by_client\u0026#39;, \u0026#39;2013-10-01\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;5\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;2013-10-02\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;6\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;2013-10-02\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;7\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;2013-10-02\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;8\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;2013-10-03\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;9\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;2013-10-03\u0026#39;) insert into Trips (id, client_id, driver_id, city_id, status, request_at) values (\u0026#39;10\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;13\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;cancelled_by_driver\u0026#39;, \u0026#39;2013-10-03\u0026#39;) Truncate table Users insert into Users (users_id, banned, role) values (\u0026#39;1\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;client\u0026#39;) insert into Users (users_id, banned, role) values (\u0026#39;2\u0026#39;, \u0026#39;Yes\u0026#39;, \u0026#39;client\u0026#39;) insert into Users (users_id, banned, role) values (\u0026#39;3\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;client\u0026#39;) insert into Users (users_id, banned, role) values (\u0026#39;4\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;client\u0026#39;) insert into Users (users_id, banned, role) values (\u0026#39;10\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;driver\u0026#39;) insert into Users (users_id, banned, role) values (\u0026#39;11\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;driver\u0026#39;) insert into Users (users_id, banned, role) values (\u0026#39;12\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;driver\u0026#39;) insert into Users (users_id, banned, role) values (\u0026#39;13\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;driver\u0026#39;) 解題思考 題目要求輸出 2013-10-01 至 2013-10-03 ，每日的搭乘請求取消率，且被納入計算的有效搭乘請求必須是未停權乘客 和 未停權駕駛，若有一方被停權，則該筆搭乘請求視為無效資料。\n搭乘請求取消率 ← 該日有效搭乘請求且該請求被拒絕的資料數 / 該日總有效搭乘請求 +------------+-------------------+ | Day | Cancellation Rate | +------------+-------------------+ | 2013-10-01 | 0.33 | | 2013-10-02 | 0.00 | | 2013-10-03 | 0.50 | +------------+-------------------+ 透過 with clause 分別建立未停權乘客 legal_client、未停權駕駛 legal_driver，以及2013-10-01 至 2013-10-03 的搭乘資料 legal_trips。 查詢 legal_trips 作為主要表格，關聯legal_client 和 legal_driver ，以計算每日的 搭乘請求取消率 。 解決方案 with legal_client as ( select * from Users where role = \u0026#39;client\u0026#39; and banned = \u0026#39;No\u0026#39;), legal_driver as ( select * from Users where role = \u0026#39;driver\u0026#39; and banned = \u0026#39;No\u0026#39;), legal_trips as (select * from Trips where request_at between \u0026#39;2013-10-01\u0026#39; and \u0026#39;2013-10-03\u0026#39; ) select legal_trips.request_at as Day, round( count(if(legal_trips.status=\u0026#39;completed\u0026#39;, NULL, legal_trips.id)) / count(legal_trips.id), 2 ) as \u0026#39;Cancellation Rate\u0026#39; from legal_trips join legal_client on legal_client.users_id = legal_trips.client_id join legal_driver on legal_driver.users_id = legal_trips.driver_id group by legal_trips.request_at ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/trips-and-users/","summary":"題目 Table: Trips\n+-------------+----------+ | Column Name | Type | +-------------+----------+ | id | int | | client_id | int | | driver_id | int | | city_id | int | | status | enum | | request_at | date | +-------------+----------+ id is the primary key for this table. The table holds all taxi trips. Each trip has a unique id, while client_id and driver_id are foreign keys to the users_id at the Users table.","title":"[leetcode][Database][Hard] 262. Trips and Users"},{"content":"題目\nTable: Employee\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | id | int | | company | varchar | | salary | int | +--------------+---------+ id is the primary key column for this table. Each row of this table indicates the company and the salary of one employee. Write an SQL query to find the rows that contain the median salary of each company. While calculating the median, when you sort the salaries of the company, break the ties by id.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Employee (id int, company varchar(255), salary int) Truncate table Employee insert into Employee (id, company, salary) values (\u0026#39;1\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;2341\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;2\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;341\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;3\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;15\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;4\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;15314\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;5\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;451\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;6\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;513\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;7\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;15\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;8\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;13\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;9\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;1154\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;10\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;1345\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;11\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;1221\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;12\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;234\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;13\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;2345\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;14\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;2645\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;15\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;2645\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;16\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;2652\u0026#39;) insert into Employee (id, company, salary) values (\u0026#39;17\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;65\u0026#39;) 解題思考\n題目要求計算並且輸出每間公司的薪資中位數。 +----+---------+--------+ | id | company | salary | +----+---------+--------+ | 5 | A | 451 | | 6 | A | 513 | | 12 | B | 234 | | 9 | B | 1154 | | 14 | C | 2645 | +----+---------+--------+ 做子表 a查詢，每間公司的員工總數 cnt ，並利用 row_numbner()依據每間公司員工薪水 salary 升序，標註排序結果 row_num。 利用子表 a 作為主要表格，過濾row_num以找出介於 cnt div 2和 (cnt div 2)+1 的 record set，即每間公司的新增中位數。 解決方案\nselect a.id, a.company, a.salary from ( select *, count(id) over(PARTITION by company) as cnt, row_number() over(PARTITION by company order by salary) as row_num from employee ) a where a.row_num between a.cnt div 2 and (a.cnt div 2)+1 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/median-employee-salary/","summary":"題目\nTable: Employee\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | id | int | | company | varchar | | salary | int | +--------------+---------+ id is the primary key column for this table. Each row of this table indicates the company and the salary of one employee. Write an SQL query to find the rows that contain the median salary of each company. While calculating the median, when you sort the salaries of the company, break the ties by id.","title":"[leetcode][Database][Hard] 569. Median Employee Salary"},{"content":"題目 Table: Numbers\n+-------------+------+ | Column Name | Type | +-------------+------+ | num | int | | frequency | int | +-------------+------+ num is the primary key for this table. Each row of this table shows the frequency of a number in the database. The median is the value separating the higher half from the lower half of a data sample.\nWrite an SQL query to report the median of all the numbers in the database after decompressing the Numbers table. Round the median to one decimal point.\nSQL Schema\nCreate table If Not Exists Numbers (num int, frequency int) Truncate table Numbers insert into Numbers (num, frequency) values (\u0026#39;0\u0026#39;, \u0026#39;7\u0026#39;) insert into Numbers (num, frequency) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;) insert into Numbers (num, frequency) values (\u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;) insert into Numbers (num, frequency) values (\u0026#39;3\u0026#39;, \u0026#39;1\u0026#39;) 解題思考 題目要求輸出 Numbers 表格的中位數 median。\n本題要求的邏輯為 : 1. 先將 Numbers 內的數字 num 以頻次 frequency 展開為等長的數列\n2. L ← 將所有 num 展開的數列依 num 大小有序串接 3. 求出 L 的中位數 median，並將中位數作為輸出結果 Input: Numbers table: +-----+-----------+ | num | frequency | +-----+-----------+ | 0 | 7 | | 1 | 1 | | 2 | 3 | | 3 | 1 | +-----+-----------+ Output: +--------+ | median | +--------+ | 0.0 | +--------+ Explanation: If we decompress the Numbers table, we will get [0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 3], so the median is (0 + 0) / 2 = 0. 中位數的定義 → 在一組數列 _S_ 中，有一半數字個數會小於中位數，另外一半數字個數會大於中位數。假設\n_L_ ← 數列 _S_ 中，小於等於 Numbers.num 的數字個數\n_R_ ← 數列 _S_ 中，大於等於 Numbers.num 的數字個數\n_N_ ← 數列 _S_ 中，Numbers.num 的數字個數\n則數列 _S_ 的數字個數應等同於 _L + N + R_ ， 對 Numbers 中的所有 num ，該條件都成立 考慮當 _L_ ≠ _R_ 的情況，若Numbers.num是中位數，當 _N_ 加入至個數較少的短邊時 ( _L_ 或 _R_ ) ，被 _N_ 加入的短邊會成為長邊:\n當 _L \u0026lt; R_，_L + N \u0026gt; R _當 _R \u0026lt; L_，_R + N \u0026gt; L _若Numbers.num不是中位數，當 _L \u0026lt; R_ 時，即使把 _N_ 放到短邊，仍會得到 _L + N \u0026lt; R_ 的結果，這表示 _N_ 不在數列中間 考慮當 _L = R_ 的情況，根據定義已知 Numbers.num是中位數 解決方案 select round(avg(n.num),1) median from Numbers n where n.Frequency \u0026gt;= abs( (select sum(Frequency) from Numbers where num\u0026lt;=n.num) - (select sum(Frequency) from Numbers where num\u0026gt;=n.num) ) ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/find-median-given-frequency-of-numbers/","summary":"題目 Table: Numbers\n+-------------+------+ | Column Name | Type | +-------------+------+ | num | int | | frequency | int | +-------------+------+ num is the primary key for this table. Each row of this table shows the frequency of a number in the database. The median is the value separating the higher half from the lower half of a data sample.\nWrite an SQL query to report the median of all the numbers in the database after decompressing the Numbers table.","title":"[leetcode][Database][Hard] 571. Find Median Given Frequency of Numbers"},{"content":"題目 Table: Employee\n+-------------+------+ | Column Name | Type | +-------------+------+ | id | int | | month | int | | salary | int | +-------------+------+ (id, month) is the primary key for this table. Each row in the table indicates the salary of an employee in one month during the year 2020. Write an SQL query to calculate the cumulative salary summary for every employee in a single unified table.\nThe cumulative salary summary for an employee can be calculated as follows:\nFor each month that the employee worked, sum up the salaries in that month and the previous two months. This is their 3-month sum for that month. If an employee did not work for the company in previous months, their effective salary for those months is 0. Do not include the 3-month sum for the most recent month that the employee worked for in the summary. Do not include the 3-month sum for any month the employee did not work. Return the result table ordered by id in ascending order. In case of a tie, order it by month in descending order.\nSQL Schema\nCreate table If Not Exists Employee (id int, month int, salary int) Truncate table Employee insert into Employee (id, month, salary) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;20\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;20\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;30\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;30\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;3\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;40\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;1\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;40\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;3\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;60\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;1\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;60\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;70\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;1\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;90\u0026#39;) insert into Employee (id, month, salary) values (\u0026#39;1\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;90\u0026#39;) 解題思考 題目要求輸出每位員工在每個月分的累計薪資，並且過濾每個員工最後一次薪資紀錄的月份。\n定義 累計薪資 → 該月份薪資與前兩個月的加總 +----+-------+--------+ | id | month | Salary | +----+-------+--------+ | 1 | 7 | 90 | | 1 | 4 | 130 | | 1 | 3 | 90 | | 1 | 2 | 50 | | 1 | 1 | 20 | | 2 | 1 | 20 | | 3 | 3 | 100 | | 3 | 2 | 40 | +----+-------+--------+ 透過 with clause 建立 month_dense_rank 表格，以便找出每位員工需要過濾有薪資紀錄的最後一個月份。\n利用 dense_rank() 函式，依據員工編號 id 與紀錄月份 month 降序，標註每位員工，每個月分薪資紀錄的排序結果。 透過 with clause 建立 cumulate_month_salary 表格，統計每位員工，每個月份的累積薪資。\n利用子查詢關聯外部主表 Employee，並過濾出符合子查詢 id=Employee.id 以及子查詢薪資紀錄月份 month 落在主表 Employee.month-2 和 Employee.month 之間的 record set，依據該 record set 的員工編號 id 對該 record set的薪資 salary 進行加總。 利用子表查詢 month_dense_rank 過濾 month_dense_rank.r_month \u0026gt; 1 的 record set 作為主要表格，並關聯 cumulate_month_salary 表格，最後帶出符合題目要求的輸出結果 員工編號id 、 薪資月份month 、 累積薪資salary 。 解決方案 with month_dense_rnak as ( select id, month, dense_rank() over(partition by id order by month desc) as r_month from Employee ), cumulate_month_salary as ( select id, month, ( select sum(salary) from Employee where id=e.id and month between e.month-2 and e.month group by id ) as salary from Employee e ) select a.id, a.month, b.salary from ( select id, month from month_dense_rnak where r_month \u0026gt; 1 ) a join cumulate_month_salary b on b.id = a.id and b.month = a.month ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/find-cumulative-salary-of-an-employee/","summary":"題目 Table: Employee\n+-------------+------+ | Column Name | Type | +-------------+------+ | id | int | | month | int | | salary | int | +-------------+------+ (id, month) is the primary key for this table. Each row in the table indicates the salary of an employee in one month during the year 2020. Write an SQL query to calculate the cumulative salary summary for every employee in a single unified table.","title":"[leetcode][Database][Hard] 579. Find Cumulative Salary of an Employee"},{"content":"題目 Table: Stadium\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | id | int | | visit_date | date | | people | int | +---------------+---------+ visit_date is the primary key for this table. Each row of this table contains the visit date and visit id to the stadium with the number of people during the visit. No two rows will have the same visit_date, and as the id increases, the dates increase as well. Write an SQL query to display the records with three or more rows with consecutive id\u0026rsquo;s, and the number of people is greater than or equal to 100 for each.\nReturn the result table ordered by visit_date in ascending order.\nSQL Schema\nCreate table If Not Exists Stadium (id int, visit_date DATE NULL, people int) Truncate table Stadium insert into Stadium (id, visit_date, people) values (\u0026#39;1\u0026#39;, \u0026#39;2017-01-01\u0026#39;, \u0026#39;10\u0026#39;) insert into Stadium (id, visit_date, people) values (\u0026#39;2\u0026#39;, \u0026#39;2017-01-02\u0026#39;, \u0026#39;109\u0026#39;) insert into Stadium (id, visit_date, people) values (\u0026#39;3\u0026#39;, \u0026#39;2017-01-03\u0026#39;, \u0026#39;150\u0026#39;) insert into Stadium (id, visit_date, people) values (\u0026#39;4\u0026#39;, \u0026#39;2017-01-04\u0026#39;, \u0026#39;99\u0026#39;) insert into Stadium (id, visit_date, people) values (\u0026#39;5\u0026#39;, \u0026#39;2017-01-05\u0026#39;, \u0026#39;145\u0026#39;) insert into Stadium (id, visit_date, people) values (\u0026#39;6\u0026#39;, \u0026#39;2017-01-06\u0026#39;, \u0026#39;1455\u0026#39;) insert into Stadium (id, visit_date, people) values (\u0026#39;7\u0026#39;, \u0026#39;2017-01-07\u0026#39;, \u0026#39;199\u0026#39;) insert into Stadium (id, visit_date, people) values (\u0026#39;8\u0026#39;, \u0026#39;2017-01-09\u0026#39;, \u0026#39;188\u0026#39;) 解題思考 題目要求輸出連續三日以上，拜訪人數達到 100 以上 的日期與人數。 +------+------------+-----------+ | id | visit_date | people | +------+------------+-----------+ | 5 | 2017-01-05 | 145 | | 6 | 2017-01-06 | 1455 | | 7 | 2017-01-07 | 199 | | 8 | 2017-01-09 | 188 | +------+------------+-----------+ 透過 with clause 建立 visit_thresgold 表格，利用 lead()函示擴增後兩日瀏覽人數欄位 next_1、next_2，利用lag() 函示擴增前兩日瀏覽人數欄位 prev_1、prev_2，以 over_threshold 作為當日瀏覽人數欄位。\n判斷每個欄位瀏覽人數是否達到 100 以上，\n若人數達到 100 以上，標記為 1；\n若人數不滿 100，則標記為 0 查詢 stadium 作為主要表格並關聯 visit_threshold，若以下三條判斷式有其中一條成立，則該日日期與瀏覽人數需要包含於輸出結果內\nprev_2 + prev_1 + over_threshold \u0026gt; 2\nprev_1 + over_threshold + next_1 \u0026gt; 2\nover_threshold + next_1 + next_2 \u0026gt; 2 當看到連續這個關鍵字時，我會想到應該使用 lead() 函式或 lag() 函式。lead() 和 lag() 對應的是當前查詢的 record set ；若在子查詢或子表內使用，則對應子查詢或子表的資料範圍。\nlead( column, offset ) over( [partition by column1, column2, …] [order by column1, column2, …])\nlag( column, offset) over( [partition by column1, column2, …] [order by column1, column2, …]) Leetcode 官方對於這題所提供的解決方案是透過 self join T 兩次且不給予關聯條件，並判斷 T1.people 、 T2.people 和 T3.people 是否都有達到 100 以上。 解決方案 with visit_threshold as ( select id, if(people \u0026gt;= 100, 1, 0) as over_threshold, if(lead(people, 1) over() \u0026gt;=100, 1,0) as next_1, if(lead(people, 2) over() \u0026gt;=100, 1,0) as next_2, if(lag(people, 1) over() \u0026gt;=100, 1,0) as prev_1, if(lag(people, 2) over() \u0026gt;=100, 1,0) as prev_2 from stadium ) select A.id, A.visit_date , A.people from stadium A join visit_threshold B using(id) where B.over_threshold+B.next_1+B.next_2 \u0026gt; 2 or B.prev_1+B.prev_2+B.over_threshold \u0026gt; 2 or B.prev_1+B.over_threshold+B.next_1 \u0026gt; 2 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/human-traffic-of-stadium/","summary":"題目 Table: Stadium\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | id | int | | visit_date | date | | people | int | +---------------+---------+ visit_date is the primary key for this table. Each row of this table contains the visit date and visit id to the stadium with the number of people during the visit. No two rows will have the same visit_date, and as the id increases, the dates increase as well.","title":"[leetcode][Database][Hard] 601. Human Traffic of Stadium"},{"content":"題目 Table: Salary\n+-------------+------+ | Column Name | Type | +-------------+------+ | id | int | | employee_id | int | | amount | int | | pay_date | date | +-------------+------+ id is the primary key column for this table. Each row of this table indicates the salary of an employee in one month. employee_id is a foreign key from the Employee table. Table: Employee\n+---------------+------+ | Column Name | Type | +---------------+------+ | employee_id | int | | department_id | int | +---------------+------+ employee_id is the primary key column for this table. Each row of this table indicates the department of an employee. Write an SQL query to report the comparison result (higher/lower/same) of the average salary of employees in a department to the company’s average salary.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Salary (id int, employee_id int, amount int, pay_date date) Create table If Not Exists Employee (employee_id int, department_id int) Truncate table Salary insert into Salary (id, employee_id, amount, pay_date) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;9000\u0026#39;, \u0026#39;2017/03/31\u0026#39;) insert into Salary (id, employee_id, amount, pay_date) values (\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;6000\u0026#39;, \u0026#39;2017/03/31\u0026#39;) insert into Salary (id, employee_id, amount, pay_date) values (\u0026#39;3\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;10000\u0026#39;, \u0026#39;2017/03/31\u0026#39;) insert into Salary (id, employee_id, amount, pay_date) values (\u0026#39;4\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;7000\u0026#39;, \u0026#39;2017/02/28\u0026#39;) insert into Salary (id, employee_id, amount, pay_date) values (\u0026#39;5\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;6000\u0026#39;, \u0026#39;2017/02/28\u0026#39;) insert into Salary (id, employee_id, amount, pay_date) values (\u0026#39;6\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;8000\u0026#39;, \u0026#39;2017/02/28\u0026#39;) Truncate table Employee insert into Employee (employee_id, department_id) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;) insert into Employee (employee_id, department_id) values (\u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;) insert into Employee (employee_id, department_id) values (\u0026#39;3\u0026#39;, \u0026#39;2\u0026#39;) 解題思考 題目要求列出每個月份，每個部門平均薪資，和整個公司平均薪資的比較結果，並顯示高 higher/ 相等 same/ 低 lower的結果。 +-----------+---------------+------------+ | pay_month | department_id | comparison | +-----------+---------------+------------+ | 2017-02 | 1 | same | | 2017-03 | 1 | higher | | 2017-02 | 2 | same | | 2017-03 | 2 | lower | +-----------+---------------+------------+ 透過 with clause 建立 company_salary 表格，帶出每個月份公司的平均薪資。\n選擇 salary 作為主要表格，因為 company_salary 表格不需要區分個別部門的員工薪資。 透過 with clause 建立 department_salary 表格，帶出每個月份，每個部門的平均薪資。\n查詢 employee 作為主要表格並關聯 salary 表格，employee.department_id 可以作為區分員工部門的依據，便能計算出每個部門的平均薪資。 查詢 company_salary 作為主要表格並關聯 department_salary 表格，最後以 pay_month 升序、 department_id 升序輸出最後查詢結果。\n利用 if( condition, statement for condition true, statement for condition false ) 函示，並使用連續的 if() 判斷式來達成顯示高 higher/ 相等 same/ 低 lower的結果。 解決方案 with company_salary as ( select date_format(pay_date, \u0026#34;%Y-%m\u0026#34;) as pay_month, avg(amount) as avg_salary from salary group by month(pay_date) ), department_salary as ( select a.department_id, date_format(b.pay_date, \u0026#34;%Y-%m\u0026#34;) as pay_month, avg(b.amount) as avg_salary from employee a join salary b using (employee_id) group by a.department_id, month(b.pay_date) ) select a.pay_month as pay_month, b.department_id as department_id, if(b.avg_salary\u0026gt;a.avg_salary, \u0026#34;higher\u0026#34;, if(b.avg_salary \u0026lt; a.avg_salary, \u0026#34;lower\u0026#34;, \u0026#34;same\u0026#34;)) as comparison from company_salary a join department_salary b using(pay_month) order by a.pay_month, b.department_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/average-salary-departments-vs-company/","summary":"題目 Table: Salary\n+-------------+------+ | Column Name | Type | +-------------+------+ | id | int | | employee_id | int | | amount | int | | pay_date | date | +-------------+------+ id is the primary key column for this table. Each row of this table indicates the salary of an employee in one month. employee_id is a foreign key from the Employee table. Table: Employee\n+---------------+------+ | Column Name | Type | +---------------+------+ | employee_id | int | | department_id | int | +---------------+------+ employee_id is the primary key column for this table.","title":"[leetcode][Database][Hard] 615. Average Salary: Departments VS Company"},{"content":"題目 Table: Student\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | name | varchar | | continent | varchar | +-------------+---------+ There is no primary key for this table. It may contain duplicate rows. Each row of this table indicates the name of a student and the continent they came from. A school has students from Asia, Europe, and America.\nWrite an SQL query to pivot the continent column in the Student table so that each name is sorted alphabetically and displayed underneath its corresponding continent. The output headers should be America, Asia, and Europe, respectively.\nThe test cases are generated so that the student number from America is not less than either Asia or Europe.\nSQL Schema\nCreate table If Not Exists Student (name varchar(50), continent varchar(7)) Truncate table Student insert into Student (name, continent) values (\u0026#39;Jane\u0026#39;, \u0026#39;America\u0026#39;) insert into Student (name, continent) values (\u0026#39;Pascal\u0026#39;, \u0026#39;Europe\u0026#39;) insert into Student (name, continent) values (\u0026#39;Xi\u0026#39;, \u0026#39;Asia\u0026#39;) insert into Student (name, continent) values (\u0026#39;Jack\u0026#39;, \u0026#39;America\u0026#39;) 解題思考 題目要求輸出樞紐表格，該樞紐表格需要使用 America 、 Asia 和 Europe ，並符合該大洲的學生名字 name 列表。 +---------+------+--------+ | America | Asia | Europe | +---------+------+--------+ | Jack | Xi | Pascal | | Jane | null | null | +---------+------+--------+ 使用 left join 便能簡單的達成題目要求。\n需要著手處理的問題點 : 如何讓學生名字並存在同一個 row 。\n利用 row_number() 函式，先將學生依據大洲分類，並給予每個大洲內的學生列表標記排序數字，最後透過選擇學生人數最多的大洲作為主要表格，依次 left join 其他大洲的學生列表，並以 row_number() 的標記數字做為表格關聯的條件。 解決方案 with from_america as (select name, row_number() over() as rn from student where continent = \u0026#39;America\u0026#39; order by name asc), from_asia as (select name, row_number() over() as rn from student where continent = \u0026#39;Asia\u0026#39; order by name asc), from_europe as (select name, row_number() over() as rn from student where continent = \u0026#39;Europe\u0026#39; order by name asc) select a.name as America, b.name as Asia, c.name as Europe from from_america a left join from_asia b using(rn) left join from_europe c using(rn) ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/students-report-by-geography/","summary":"題目 Table: Student\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | name | varchar | | continent | varchar | +-------------+---------+ There is no primary key for this table. It may contain duplicate rows. Each row of this table indicates the name of a student and the continent they came from. A school has students from Asia, Europe, and America.\nWrite an SQL query to pivot the continent column in the Student table so that each name is sorted alphabetically and displayed underneath its corresponding continent.","title":"[leetcode][Database][Hard] 618. Students Report By Geography"},{"content":"題目 Table: Activity\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | player_id | int | | device_id | int | | event_date | date | | games_played | int | +--------------+---------+ (player_id, event_date) is the primary key of this table. This table shows the activity of players of some games. Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on someday using some device. The install date of a player is the first login day of that player.\nWe define day one retention of some date x to be the number of players whose install date is x and they logged back in on the day right after x, divided by the number of players whose install date is x, rounded to 2 decimal places.\nWrite an SQL query to report for each install date, the number of players that installed the game on that day, and the day one retention.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Activity (player_id int, device_id int, event_date date, games_played int) Truncate table Activity insert into Activity (player_id, device_id, event_date, games_played) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;2016-03-01\u0026#39;, \u0026#39;5\u0026#39;) insert into Activity (player_id, device_id, event_date, games_played) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;2016-03-02\u0026#39;, \u0026#39;6\u0026#39;) insert into Activity (player_id, device_id, event_date, games_played) values (\u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2017-06-25\u0026#39;, \u0026#39;1\u0026#39;) insert into Activity (player_id, device_id, event_date, games_played) values (\u0026#39;3\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2016-03-01\u0026#39;, \u0026#39;0\u0026#39;) insert into Activity (player_id, device_id, event_date, games_played) values (\u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;2018-07-03\u0026#39;, \u0026#39;5\u0026#39;) 解題思考 題目要求利用 Activity 表格統計安裝用戶數量，以及用戶首日留存率。\n定義用戶安裝日 → 用戶在 Activity 第一次出現紀錄的 event_date 。\n定義用戶首日留存 → 用戶在安裝日隔天 event_date+1 有 Activity record。\n定義首日留存率計算方式 → 用戶首日留存數 / 安裝日總用戶人數 透過 with clause 建立 player_install_date 表格，找出每位用戶的安裝日 install_dt 透過 with clause 建立 player_day1_back 表格，統計用戶首日留存人數 cnt_login_back_player 。\n選擇 Activity 作為主要表格，使用 left join player_install_date 帶出所有的安裝日 install_dt ，並過濾 Activity 表格中符合條件 Activity.event_date = player_install_date+1 的 record set。 從 player_install_date 帶出 用戶安裝日 統計人數，使用 left join player_day1_back 帶出用戶首日留存 統計人數，便能計算出 首日留存率 。 解決方案 with player_install_date as ( select player_id, min(event_date) as install_dt from activity group by player_id ), player_day1_back as ( select distinct b.install_dt, count(a.player_id) over(partition by a.event_date) as cnt_login_back_player from activity a left join player_install_date b using(player_id) where a.event_date = date_add(b.install_dt, INTERVAL 1 DAY) ) select a.install_dt, a.installs, round(ifnull(b.cnt_login_back_player,0) / a.installs,2) as Day1_retention from ( select distinct install_dt, count(player_id) over(partition by install_dt) as installs from player_install_date ) a left join player_day1_back b on b.install_dt = a.install_dt ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/game-play-analysis-v/","summary":"題目 Table: Activity\n+--------------+---------+ | Column Name | Type | +--------------+---------+ | player_id | int | | device_id | int | | event_date | date | | games_played | int | +--------------+---------+ (player_id, event_date) is the primary key of this table. This table shows the activity of players of some games. Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on someday using some device.","title":"[leetcode][Database][Hard] 1097. Game Play Analysis V"},{"content":"題目 Table: Spending\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | spend_date | date | | platform | enum | | amount | int | +-------------+---------+ The table logs the history of the spending of users that make purchases from an online shopping website that has a desktop and a mobile application. (user_id, spend_date, platform) is the primary key of this table. The platform column is an ENUM type of (\u0026#39;desktop\u0026#39;, \u0026#39;mobile\u0026#39;). Write an SQL query to find the total number of users and the total amount spent using the mobile only, the desktop only, and both mobile and desktop together for each date.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Spending (user_id int, spend_date date, platform ENUM(\u0026#39;desktop\u0026#39;, \u0026#39;mobile\u0026#39;), amount int) Truncate table Spending insert into Spending (user_id, spend_date, platform, amount) values (\u0026#39;1\u0026#39;, \u0026#39;2019-07-01\u0026#39;, \u0026#39;mobile\u0026#39;, \u0026#39;100\u0026#39;) insert into Spending (user_id, spend_date, platform, amount) values (\u0026#39;1\u0026#39;, \u0026#39;2019-07-01\u0026#39;, \u0026#39;desktop\u0026#39;, \u0026#39;100\u0026#39;) insert into Spending (user_id, spend_date, platform, amount) values (\u0026#39;2\u0026#39;, \u0026#39;2019-07-01\u0026#39;, \u0026#39;mobile\u0026#39;, \u0026#39;100\u0026#39;) insert into Spending (user_id, spend_date, platform, amount) values (\u0026#39;2\u0026#39;, \u0026#39;2019-07-02\u0026#39;, \u0026#39;mobile\u0026#39;, \u0026#39;100\u0026#39;) insert into Spending (user_id, spend_date, platform, amount) values (\u0026#39;3\u0026#39;, \u0026#39;2019-07-01\u0026#39;, \u0026#39;desktop\u0026#39;, \u0026#39;100\u0026#39;) insert into Spending (user_id, spend_date, platform, amount) values (\u0026#39;3\u0026#39;, \u0026#39;2019-07-02\u0026#39;, \u0026#39;desktop\u0026#39;, \u0026#39;100\u0026#39;) 解題思考 題目要求統計透過用戶 user在使用PC desktop、手機APP mobile 以及兩者皆有 both 的採購金額 amount 。\n分開計算PC desktop 和手機APPmobile 並不困難，因此需要著手處理的是兩者皆有 both 採購紀錄的用戶，並將這些用戶從PC desktop和手機APPmobile 的統計中分離出來。 透過 with clause 建立 p_user_platform 表格，從 spending 表格帶出 user_id、 spend_date，並透過 group by user_id, spend_date 將資料分組為每個用戶 user_id 在每個 spend_date 的採購資料，並分開加總 platform=\u0026quot;mobile\u0026quot; 、platform=\u0026quot;desktop\u0026quot; 的採購數量 amount以賦予 mobild_amount 和 desktop_amount 欄位。 透過 with clause 建立 p_user_summary 表格，判斷 mobild_amount 和 desktop_amount 的值，將每位用戶在每個 spend_date 的採購情形分類成 desktop、mobile和both 這個步驟也可以在建立 p_user_platform 的過程中進行分類，但我傾向明確每張表格的用途，以便理解每一個 query statement 中引用的資料表格與欄位。 透過 with clause 建立 p_spend_date 表格，並在 spending 表格擷取所有不重複的 spend_date ，並將每個不重複的 spend_date 擴展成帶有 desktop、mobile和both 的 record 。\n這個動作會將每個不重複的 spend_date ，從原先的 1 row 擴展成 3 rows。 透過查詢 p_spend_date 作為主要表格，使用 left join p_user_summary 帶出相對應 spend_date 、platform 的 record set，並利用 spend_date 、platform 統計出該 spend_date 在 desktop、mobile和both 的總採購量 total_amount 和總計人數 total_user 。 解決方案 with p_user_platform as ( select user_id, spend_date, sum(case when platform = \u0026#39;mobile\u0026#39; then amount else 0 end) as mobile_amount, sum(case when platform = \u0026#39;desktop\u0026#39; then amount else 0 end) as desktop_amount from spending group by user_id, spend_date ), p_user_summary as ( select user_id, spend_date, if(mobile_amount \u0026gt; 0, if(desktop_amount \u0026gt; 0, \u0026#39;both\u0026#39;, \u0026#39;mobile\u0026#39;), \u0026#39;desktop\u0026#39;) as platform, mobile_amount + desktop_amount as amount from p_user_platform ), p_spend_date as ( select distinct(spend_date), \u0026#39;desktop\u0026#39; as platform from spending union select distinct(spend_date), \u0026#39;mobile\u0026#39; as platform from spending union select distinct(spend_date), \u0026#39;both\u0026#39; as platform from spending ) select a.spend_date, a.platform, sum(ifnull(b.amount,0)) as total_amount, count(b.user_id) as total_users from p_spend_date a left join p_user_summary b using(spend_date, platform) group by a.spend_date, a.platform ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/user-purchase-platform/","summary":"題目 Table: Spending\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | spend_date | date | | platform | enum | | amount | int | +-------------+---------+ The table logs the history of the spending of users that make purchases from an online shopping website that has a desktop and a mobile application. (user_id, spend_date, platform) is the primary key of this table. The platform column is an ENUM type of (\u0026#39;desktop\u0026#39;, \u0026#39;mobile\u0026#39;).","title":"[leetcode][Database][Hard] 1127. User Purchase Platform"},{"content":"題目 Table: Users\n+----------------+---------+ | Column Name | Type | +----------------+---------+ | user_id | int | | join_date | date | | favorite_brand | varchar | +----------------+---------+ user_id is the primary key of this table. This table has the info of the users of an online shopping website where users can sell and buy items. Table: Orders\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | order_id | int | | order_date | date | | item_id | int | | buyer_id | int | | seller_id | int | +---------------+---------+ order_id is the primary key of this table. item_id is a foreign key to the Items table. buyer_id and seller_id are foreign keys to the Users table. Table: Items\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | item_id | int | | item_brand | varchar | +---------------+---------+ item_id is the primary key of this table. Write an SQL query to find for each user whether the brand of the second item (by date) they sold is their favorite brand. If a user sold less than two items, report the answer for that user as no. It is guaranteed that no seller sold more than one item on a day.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Users (user_id int, join_date date, favorite_brand varchar(10)) Create table If Not Exists Orders (order_id int, order_date date, item_id int, buyer_id int, seller_id int) Create table If Not Exists Items (item_id int, item_brand varchar(10)) Truncate table Users insert into Users (user_id, join_date, favorite_brand) values (\u0026#39;1\u0026#39;, \u0026#39;2019-01-01\u0026#39;, \u0026#39;Lenovo\u0026#39;) insert into Users (user_id, join_date, favorite_brand) values (\u0026#39;2\u0026#39;, \u0026#39;2019-02-09\u0026#39;, \u0026#39;Samsung\u0026#39;) insert into Users (user_id, join_date, favorite_brand) values (\u0026#39;3\u0026#39;, \u0026#39;2019-01-19\u0026#39;, \u0026#39;LG\u0026#39;) insert into Users (user_id, join_date, favorite_brand) values (\u0026#39;4\u0026#39;, \u0026#39;2019-05-21\u0026#39;, \u0026#39;HP\u0026#39;) Truncate table Orders insert into Orders (order_id, order_date, item_id, buyer_id, seller_id) values (\u0026#39;1\u0026#39;, \u0026#39;2019-08-01\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;) insert into Orders (order_id, order_date, item_id, buyer_id, seller_id) values (\u0026#39;2\u0026#39;, \u0026#39;2019-08-02\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;3\u0026#39;) insert into Orders (order_id, order_date, item_id, buyer_id, seller_id) values (\u0026#39;3\u0026#39;, \u0026#39;2019-08-03\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;) insert into Orders (order_id, order_date, item_id, buyer_id, seller_id) values (\u0026#39;4\u0026#39;, \u0026#39;2019-08-04\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;2\u0026#39;) insert into Orders (order_id, order_date, item_id, buyer_id, seller_id) values (\u0026#39;5\u0026#39;, \u0026#39;2019-08-04\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;) insert into Orders (order_id, order_date, item_id, buyer_id, seller_id) values (\u0026#39;6\u0026#39;, \u0026#39;2019-08-05\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;) Truncate table Items insert into Items (item_id, item_brand) values (\u0026#39;1\u0026#39;, \u0026#39;Samsung\u0026#39;) insert into Items (item_id, item_brand) values (\u0026#39;2\u0026#39;, \u0026#39;Lenovo\u0026#39;) insert into Items (item_id, item_brand) values (\u0026#39;3\u0026#39;, \u0026#39;LG\u0026#39;) insert into Items (item_id, item_brand) values (\u0026#39;4\u0026#39;, \u0026#39;HP\u0026#39;) 解題思考 題目要求判斷每位 seller 賣出的第二項 item 是否為該 seller 的喜愛品牌，並輸出 No 和 Yes 作為每位 seller 的分類結果。若某位 seller 只賣出一項 item，則結果應為 No。 透過 with clause 建立 order_info 表格，利用 rank()函式並依據賣出日期 orders.order_date 為每一筆銷售資料進行排序 rank_sell_item。\n同時，為 order_info 表格關聯 users表格以帶出每位 user的喜愛品牌 users.favorite_brand。 透過 with clause 建立 second_sell_brand 表格，並判斷 second_sell_brand.sell_brand = second_sell_brand.seller_fav_brand 以輸出 Yes 和 No 。 透過查詢 users 作為主要表格，使用 left join second_sell_brand 帶出每位 seller 賣出第二項 item 是否為自己的喜愛品牌結果。\n在建立 second_sell_brand 表格時，由於篩選條件 rank_sell_item=2 會過濾掉只賣出過一次的 user 資訊；而在 orders 表格中也存在某些 user 沒有賣出的紀錄。\n因此需要透過 left join second_sell_brand 保證每位 user 都包含於最終的輸出結果中。 解決方案 with order_info as ( select a.order_id, a.order_date, c.item_brand as sell_brand, a.seller_id, b.favorite_brand as seller_fav_brand, rank() over(partition by a.seller_id order by a.order_date) as rank_sell_item from orders a join users b on b.user_id = a.seller_id join items c on c.item_id = a.item_id order by a.order_id, a.order_date ), second_sell_brand as ( select seller_id, if(sell_brand=seller_fav_brand, \u0026#34;yes\u0026#34;, \u0026#34;no\u0026#34;) as 2nd_item_fav_brand from order_info where rank_sell_item = 2 ) select a.user_id as seller_id, ifnull(b.2nd_item_fav_brand,\u0026#34;no\u0026#34;) as 2nd_item_fav_brand from users a left join second_sell_brand b on b.seller_id = a.user_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/market-analysis-ii/","summary":"題目 Table: Users\n+----------------+---------+ | Column Name | Type | +----------------+---------+ | user_id | int | | join_date | date | | favorite_brand | varchar | +----------------+---------+ user_id is the primary key of this table. This table has the info of the users of an online shopping website where users can sell and buy items. Table: Orders\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | order_id | int | | order_date | date | | item_id | int | | buyer_id | int | | seller_id | int | +---------------+---------+ order_id is the primary key of this table.","title":"[leetcode][Database][Hard] 1159. Market Analysis II"},{"content":"題目 Table: Players\n+-------------+-------+ | Column Name | Type | +-------------+-------+ | player_id | int | | group_id | int | +-------------+-------+ player_id is the primary key of this table. Each row of this table indicates the group of each player. Table: Matches\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | match_id | int | | first_player | int | | second_player | int | | first_score | int | | second_score | int | +---------------+---------+ match_id is the primary key of this table. Each row is a record of a match, first_player and second_player contain the player_id of each match. first_score and second_score contain the number of points of the first_player and second_player respectively. You may assume that, in each match, players belong to the same group. The winner in each group is the player who scored the maximum total points within the group. In the case of a tie, the lowest player_id wins.\nWrite an SQL query to find the winner in each group.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Players (player_id int, group_id int) Create table If Not Exists Matches (match_id int, first_player int, second_player int, first_score int, second_score int) Truncate table Players insert into Players (player_id, group_id) values (\u0026#39;10\u0026#39;, \u0026#39;2\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;15\u0026#39;, \u0026#39;1\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;20\u0026#39;, \u0026#39;3\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;25\u0026#39;, \u0026#39;1\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;30\u0026#39;, \u0026#39;1\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;35\u0026#39;, \u0026#39;2\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;40\u0026#39;, \u0026#39;3\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;45\u0026#39;, \u0026#39;1\u0026#39;) insert into Players (player_id, group_id) values (\u0026#39;50\u0026#39;, \u0026#39;2\u0026#39;) Truncate table Matches insert into Matches (match_id, first_player, second_player, first_score, second_score) values (\u0026#39;1\u0026#39;, \u0026#39;15\u0026#39;, \u0026#39;45\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;0\u0026#39;) insert into Matches (match_id, first_player, second_player, first_score, second_score) values (\u0026#39;2\u0026#39;, \u0026#39;30\u0026#39;, \u0026#39;25\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;) insert into Matches (match_id, first_player, second_player, first_score, second_score) values (\u0026#39;3\u0026#39;, \u0026#39;30\u0026#39;, \u0026#39;15\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;0\u0026#39;) insert into Matches (match_id, first_player, second_player, first_score, second_score) values (\u0026#39;4\u0026#39;, \u0026#39;40\u0026#39;, \u0026#39;20\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;2\u0026#39;) insert into Matches (match_id, first_player, second_player, first_score, second_score) values (\u0026#39;5\u0026#39;, \u0026#39;35\u0026#39;, \u0026#39;50\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;) 解題思考 樞紐表要求輸出每個 group 內總計得分最高的玩家，若遇到平分情況則以 player_id 較小的一方獲勝。\n這題的思考方式和 [leetcode][Database][Hard]1972. First and Last Call On the Same Day 雷同。 透過 with clause 建立 max_score_of_player 表格，並分別加總每位 player 的 score 。\n將 score 表格中的 first_player 和 second_player 拆分成兩張子表，並擷取 first_score 和 second_score 以便進行每位 player 的 score 加總。 透過 with clause 建立 group_player_rank 表格，選擇 players 表格作為查詢主表並關聯 max_score_of_player ，並依據 max_score_of_player.score 降序和 players.player_id 升序的方式，利用 rank()函數對players.group 進行排名 rn。 最後，查詢 group_player_rank.rn 為 1 的資料，便能找出每個 group 總計得分最高的 player 。 解決方案 with max_score_of_player as ( select player_id, sum(score) as score from ( select first_player as player_id, first_score as score from matches union all select second_player as player_id, second_score as score from matches ) a group by player_id ), group_player_rank as ( select a.group_id, a.player_id, rank() over(partition by a.group_id order by b.score desc, a.player_id asc) as rn from players a join max_score_of_player b using(player_id) ) select group_id, player_id from group_player_rank where rn = 1 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/tournament-winners/","summary":"題目 Table: Players\n+-------------+-------+ | Column Name | Type | +-------------+-------+ | player_id | int | | group_id | int | +-------------+-------+ player_id is the primary key of this table. Each row of this table indicates the group of each player. Table: Matches\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | match_id | int | | first_player | int | | second_player | int | | first_score | int | | second_score | int | +---------------+---------+ match_id is the primary key of this table.","title":"[leetcode][Database][Hard] 1194. Tournament Winners"},{"content":"題目 Table: Product\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | product_id | int | | product_name | varchar | +---------------+---------+ product_id is the primary key for this table. product_name is the name of the product. Table: Sales\n+---------------------+---------+ | Column Name | Type | +---------------------+---------+ | product_id | int | | period_start | date | | period_end | date | | average_daily_sales | int | +---------------------+---------+ product_id is the primary key for this table. period_start and period_end indicate the start and end date for the sales period, and both dates are inclusive. The average_daily_sales column holds the average daily sales amount of the items for the period. The dates of the sales years are between 2018 to 2020. Write an SQL query to report the total sales amount of each item for each year, with corresponding product_name, product_id, product_name, and report_year.\nReturn the result table ordered by product_id and report_year.\nSQL Schema\nCreate table If Not Exists Product (product_id int, product_name varchar(30)) Create table If Not Exists Sales (product_id int, period_start date, period_end date, average_daily_sales int) Truncate table Product insert into Product (product_id, product_name) values (\u0026#39;1\u0026#39;, \u0026#39;LC Phone \u0026#39;) insert into Product (product_id, product_name) values (\u0026#39;2\u0026#39;, \u0026#39;LC T-Shirt\u0026#39;) insert into Product (product_id, product_name) values (\u0026#39;3\u0026#39;, \u0026#39;LC Keychain\u0026#39;) Truncate table Sales insert into Sales (product_id, period_start, period_end, average_daily_sales) values (\u0026#39;1\u0026#39;, \u0026#39;2019-01-25\u0026#39;, \u0026#39;2019-02-28\u0026#39;, \u0026#39;100\u0026#39;) insert into Sales (product_id, period_start, period_end, average_daily_sales) values (\u0026#39;2\u0026#39;, \u0026#39;2018-12-01\u0026#39;, \u0026#39;2020-01-01\u0026#39;, \u0026#39;10\u0026#39;) insert into Sales (product_id, period_start, period_end, average_daily_sales) values (\u0026#39;3\u0026#39;, \u0026#39;2019-12-01\u0026#39;, \u0026#39;2020-01-31\u0026#39;, \u0026#39;1\u0026#39;) 解題思考 透過 with clause 分別建立 report_year 2018、2019 以及 2020 的產品銷售表格 sell_2018、 sell_2019 、 sell_2020\n由於 sales 表格的 period_start 和 period_end 有跨越年度的可能性，而在其他的測試資料集中，也可能含有早於 2018 年的銷售資料，或者晚於 2020 年後的銷售資料，因此需要特別注意時間範圍的切割。 Union sell_2018 、 sell_2019 和 sell_2020 ，並從 union 表格中取出每日平均銷售金額 average_daily_sales 乘以當年度整體銷售天數 datediff(period_end, period_start)+1 ，便可得到當年度的平均銷售總額。 解決方案 with sell_2018 as ( select product_id, if( year(period_start) \u0026lt; \u0026#39;2018\u0026#39;, \u0026#39;2018-01-01\u0026#39;, period_start) as period_start, if( year(period_end) \u0026gt; \u0026#39;2018\u0026#39;, DATE_FORMAT(period_end,\u0026#39;2018-12-31\u0026#39;), period_end ) as period_end, average_daily_sales from sales where year(period_start) \u0026lt;= \u0026#39;2018\u0026#39; ), sell_2019 as ( select product_id, if( year(period_start) \u0026lt; \u0026#39;2019\u0026#39;, \u0026#39;2019-01-01\u0026#39;, period_start) as period_start, if( year(period_end) \u0026gt; \u0026#39;2019\u0026#39;, DATE_FORMAT(period_end,\u0026#39;2019-12-31\u0026#39;), period_end ) as period_end, average_daily_sales from sales where year(period_start) \u0026lt;= \u0026#39;2019\u0026#39; and year(period_end) \u0026gt;= \u0026#39;2019\u0026#39; ), sell_2020 as ( select product_id, if( year(period_start) \u0026lt; \u0026#39;2020\u0026#39;, \u0026#39;2020-01-01\u0026#39;, period_start) as period_start, if( year(period_end) \u0026gt; \u0026#39;2020\u0026#39;, DATE_FORMAT(period_end,\u0026#39;2020-12-31\u0026#39;), period_end ) as period_end, average_daily_sales from sales where year(period_start) \u0026lt;= \u0026#39;2020\u0026#39; and year(period_end) \u0026gt;= \u0026#39;2020\u0026#39; ), product_sell_info as ( select a.product_id, b.product_name, date_format(a.period_start, \u0026#34;%Y\u0026#34;) as report_year, average_daily_sales * (datediff(a.period_end, a.period_start)+1) as total_amount from ( select * from sell_2018 union select * from sell_2019 union select * from sell_2020 ) a left join product b using(product_id) ) select product_id, product_name, report_year, total_amount from product_sell_info order by product_id, report_year ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/total-sales-amount-by-year/","summary":"題目 Table: Product\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | product_id | int | | product_name | varchar | +---------------+---------+ product_id is the primary key for this table. product_name is the name of the product. Table: Sales\n+---------------------+---------+ | Column Name | Type | +---------------------+---------+ | product_id | int | | period_start | date | | period_end | date | | average_daily_sales | int | +---------------------+---------+ product_id is the primary key for this table.","title":"[leetcode][Database][Hard] 1384. Total Sales Amount by Year"},{"content":"題目 Table: Orders\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | order_id | int | | customer_id | int | | order_date | date | | item_id | varchar | | quantity | int | +---------------+---------+ (ordered_id, item_id) is the primary key for this table. This table contains information on the orders placed. order_date is the date item_id was ordered by the customer with id customer_id. Table: Items\n+---------------------+---------+ | Column Name | Type | +---------------------+---------+ | item_id | varchar | | item_name | varchar | | item_category | varchar | +---------------------+---------+ item_id is the primary key for this table. item_name is the name of the item. item_category is the category of the item. You are the business owner and would like to obtain a sales report for category items and the day of the week.\nWrite an SQL query to report how many units in each category have been ordered on each day of the week.\nReturn the result table ordered by category.\nSQL Schema\nCreate table If Not Exists Orders (order_id int, customer_id int, order_date date, item_id varchar(30), quantity int) Create table If Not Exists Items (item_id varchar(30), item_name varchar(30), item_category varchar(30)) Truncate table Orders insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2020-06-01\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;10\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2020-06-08\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;10\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;3\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;2020-06-02\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;5\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;4\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2020-06-03\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;5\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;5\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;2020-06-04\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;1\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;6\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;2020-06-05\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;5\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;7\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;2020-06-05\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;10\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;8\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;2020-06-14\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;) insert into Orders (order_id, customer_id, order_date, item_id, quantity) values (\u0026#39;9\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;2020-06-21\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;5\u0026#39;) Truncate table Items insert into Items (item_id, item_name, item_category) values (\u0026#39;1\u0026#39;, \u0026#39;LC Alg. Book\u0026#39;, \u0026#39;Book\u0026#39;) insert into Items (item_id, item_name, item_category) values (\u0026#39;2\u0026#39;, \u0026#39;LC DB. Book\u0026#39;, \u0026#39;Book\u0026#39;) insert into Items (item_id, item_name, item_category) values (\u0026#39;3\u0026#39;, \u0026#39;LC SmarthPhone\u0026#39;, \u0026#39;Phone\u0026#39;) insert into Items (item_id, item_name, item_category) values (\u0026#39;4\u0026#39;, \u0026#39;LC Phone 2020\u0026#39;, \u0026#39;Phone\u0026#39;) insert into Items (item_id, item_name, item_category) values (\u0026#39;5\u0026#39;, \u0026#39;LC SmartGlass\u0026#39;, \u0026#39;Glasses\u0026#39;) insert into Items (item_id, item_name, item_category) values (\u0026#39;6\u0026#39;, \u0026#39;LC T-Shirt XL\u0026#39;, \u0026#39;T-shirt\u0026#39;) 解題思考 樞紐表要求依據分類 category列出周一至周日的數量 quantity統計，因此可以使用 left join 逐一列出樞紐表周一至周日的欄位。 +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Category | Monday | Tuesday | Wednesday | Thursday | Friday | Saturday | Sunday | +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Book | 20 | 5 | 0 | 0 | 10 | 0 | 0 | | Glasses | 0 | 0 | 0 | 0 | 5 | 0 | 0 | | Phone | 0 | 0 | 5 | 1 | 0 | 0 | 10 | | T-Shirt | 0 | 0 | 0 | 0 | 0 | 0 | 0 | +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ 利用 dayOfWeek() 將 orders.order_date 轉換成周一至周日 day，並透過 with clause 建立新表格 orders_with_group，擷取 orders.order_id 、orders.quantity dayOfWeek() 轉換後的 day 以及和 items 表格關聯後取得的 items.item_category 欄位 依據 orders_with_group加總每個 day 的 quantity ，並透過 with clause 建立新表 sum_quantity_by_category 。\n最後，從 items 表格列出不重複的 item_category ，並利用 left join 分別關聯周一至周日 day 的 quantity 加總。 解決方案 with orders_with_group as ( select a.order_id, dayofweek(a.order_date) as day, b.item_category, a.quantity from orders a join items b using(item_id) ), sum_quantity_by_category as ( select item_category, day, sum(quantity) as sum_quantity from orders_with_group group by item_category, day ) select distinct(a.item_category) as Category, ifnull(Mon.sum_quantity, 0) as Monday, ifnull(Tue.sum_quantity, 0) as Tuesday, ifnull(Wen.sum_quantity, 0) as Wednesday, ifnull(Thu.sum_quantity, 0) as Thursday, ifnull(Fri.sum_quantity, 0) as Friday, ifnull(Sat.sum_quantity, 0) as Saturday, ifnull(Sun.sum_quantity, 0) as Sunday from items a left join ( select item_category, sum_quantity from sum_quantity_by_category where day=2) Mon using(item_category) left join ( select item_category, sum_quantity from sum_quantity_by_category where day=3) Tue using(item_category) left join ( select item_category, sum_quantity from sum_quantity_by_category where day=4) Wen using(item_category) left join ( select item_category, sum_quantity from sum_quantity_by_category where day=5) Thu using(item_category) left join ( select item_category, sum_quantity from sum_quantity_by_category where day=6) Fri using(item_category) left join ( select item_category, sum_quantity from sum_quantity_by_category where day=7) Sat using(item_category) left join ( select item_category, sum_quantity from sum_quantity_by_category where day=1) Sun using(item_category) order by a.item_category /* Map the return value of function dayOfWeek and text 1 = Sunday 2 = Monday 3 = Tuesday 4 = Wednesday 5 = Thursday 6 = Friday 7 = Saturday */ ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/sales-by-day-of-the-week/","summary":"題目 Table: Orders\n+---------------+---------+ | Column Name | Type | +---------------+---------+ | order_id | int | | customer_id | int | | order_date | date | | item_id | varchar | | quantity | int | +---------------+---------+ (ordered_id, item_id) is the primary key for this table. This table contains information on the orders placed. order_date is the date item_id was ordered by the customer with id customer_id. Table: Items\n+---------------------+---------+ | Column Name | Type | +---------------------+---------+ | item_id | varchar | | item_name | varchar | | item_category | varchar | +---------------------+---------+ item_id is the primary key for this table.","title":"[leetcode][Database][Hard] 1479. Sales by Day of the Week"},{"content":"題目 Table: Calls\n+--------------+----------+ | Column Name | Type | +--------------+----------+ | caller_id | int | | recipient_id | int | | call_time | datetime | +--------------+----------+ (caller_id, recipient_id, call_time) is the primary key for this table. Each row contains information about the time of a phone call between caller_id and recipient_id. Write an SQL query to report the IDs of the users whose first and last calls on any day were with the same person. Calls are counted regardless of being the caller or the recipient.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Calls (caller_id int, recipient_id int, call_time datetime) Truncate table Calls insert into Calls (caller_id, recipient_id, call_time) values (\u0026#39;8\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;2021-08-24 17:46:07\u0026#39;) insert into Calls (caller_id, recipient_id, call_time) values (\u0026#39;4\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;2021-08-24 19:57:13\u0026#39;) insert into Calls (caller_id, recipient_id, call_time) values (\u0026#39;5\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2021-08-11 05:28:44\u0026#39;) insert into Calls (caller_id, recipient_id, call_time) values (\u0026#39;8\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2021-08-17 04:04:15\u0026#39;) insert into Calls (caller_id, recipient_id, call_time) values (\u0026#39;11\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2021-08-17 13:07:00\u0026#39;) insert into Calls (caller_id, recipient_id, call_time) values (\u0026#39;8\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;2021-08-17 22:22:22\u0026#39;) 解題思考 建立 user_calls的 with clause，以提供最後輸出的表格使用。\nuser_calls將每筆 calls 中的通話紀錄拆分成兩筆 record ，即該筆通話紀錄的兩位 user 分別以自己的角度，紀錄該筆通話的時間以及通話的對象\nA ← communicate → B : A → B and B → A 建立 rank_calls 的 with clause ，對 user_calls 中的通話紀錄進行排序。\n對每個 user_id 的所有 call_time 通話時間做 升序 和 降序，以便找出第一筆通話 first call 和最後一筆通話 last call 。 對 rank_calls 每個 user_id 的 first call 和 last call 進行統計，若不重複的通話對象只有一位，則可以找出 first call 和 last call 都是同一人的 user_id 解決方案 with user_calls as ( select caller_id as user_id, call_time, recipient_id from calls union select recipient_id as user_id, call_time, caller_id as recipient_id from calls ), rank_calls as ( select user_id, recipient_id, date(call_time) as day, dense_rank() over(partition by user_id, date(call_time) order by call_time asc) as rn, dense_rank() over(partition by user_id, date(call_time) order by call_time desc) as rk from user_calls ) select distinct user_id from rank_calls where rn=1 or rk=1 group by user_id, day having count(distinct recipient_id) = 1 ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/first-and-last-call-on-the-same-day/","summary":"題目 Table: Calls\n+--------------+----------+ | Column Name | Type | +--------------+----------+ | caller_id | int | | recipient_id | int | | call_time | datetime | +--------------+----------+ (caller_id, recipient_id, call_time) is the primary key for this table. Each row contains information about the time of a phone call between caller_id and recipient_id. Write an SQL query to report the IDs of the users whose first and last calls on any day were with the same person.","title":"[leetcode][Database][Hard]1972. First and Last Call On the Same Day"},{"content":"題目 Table: Candidates\n+-------------+------+ | Column Name | Type | +-------------+------+ | employee_id | int | | experience | enum | | salary | int | +-------------+------+ employee_id is the primary key column for this table. experience is an enum with one of the values (\u0026#39;Senior\u0026#39;, \u0026#39;Junior\u0026#39;). Each row of this table indicates the id of a candidate, their monthly salary, and their experience. A company wants to hire new employees. The budget of the company for the salaries is $70000. The company\u0026rsquo;s criteria for hiring are:\nHiring the largest number of seniors. After hiring the maximum number of seniors, use the remaining budget to hire the largest number of juniors. Write an SQL query to find the number of seniors and juniors hired under the mentioned criteria.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Candidates (employee_id int, experience ENUM(\u0026#39;Senior\u0026#39;, \u0026#39;Junior\u0026#39;), salary int) Truncate table Candidates insert into Candidates (employee_id, experience, salary) values (\u0026#39;1\u0026#39;, \u0026#39;Junior\u0026#39;, \u0026#39;10000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;9\u0026#39;, \u0026#39;Junior\u0026#39;, \u0026#39;10000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;2\u0026#39;, \u0026#39;Senior\u0026#39;, \u0026#39;20000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;11\u0026#39;, \u0026#39;Senior\u0026#39;, \u0026#39;20000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;13\u0026#39;, \u0026#39;Senior\u0026#39;, \u0026#39;50000\u0026#39;) insert into Candidates (employee_id, experience, salary) values (\u0026#39;4\u0026#39;, \u0026#39;Junior\u0026#39;, \u0026#39;40000\u0026#39;) 解題思考 樞紐表要求輸出 Senior雇員和 Junior雇員的可招聘人數。\n利用子查詢功能，分別帶出 Senior和 Junior的招聘結果，以作為樞紐表的欄位值。 利用 with clause 建立 Senior雇員、 Junior雇員的薪水排序表。\n因題目要求必須先招聘盡可能多的 Senior 雇員，再利用剩餘預算朝聘盡可能多的 Junior 雇員 薪水排序表中，用 row_number() 標註排序結果，排序主要條件為薪資 salary 升序；同時排序結果可做為第幾位雇員，即可招聘的雇員數量 薪水排序表中，依據薪資 salary 和雇員編號 employee_id 累加雇員薪資；同時累加雇員薪資可做為已消耗的預算 budget 查詢招聘 senior 花費的預算 cumulate_budget 。\n取 cumulate_budget \u0026lt;= budget 的 record set，並從中找出 max(rn) 與對應的 senior.cumulate_budget 計算可分配給Junior招聘的剩餘預算 remaining = budget - senior.cumulate_budget ，取 junior.cumulate_budget \u0026lt;= emaining 的 record set，並從中找出 max(rn) 與對應的 junior.cumulate_budget 解決方案 with seniors_salary_rank as ( select employee_id, salary, row_number() over(order by salary, employee_id) as rn, sum(salary) over(order by salary, employee_id) as cumulate_budget from candidates where experience = \u0026#39;senior\u0026#39; order by salary ), juniors_salary_rank as ( select employee_id, salary, row_number() over(order by salary, employee_id asc) as rn, sum(salary) over(order by salary, employee_id) as cumulate_budget from candidates where experience = \u0026#39;junior\u0026#39; order by salary ), hire_seniors as ( select b.rn, 70000 - b.cumulate_budget as remain_budget from ( select max(cumulate_budget) as cumulate_budget from seniors_salary_rank where cumulate_budget \u0026lt;= 70000 ) a join seniors_salary_rank b using(cumulate_budget) ), hire_junior as ( select b.rn, ifnull((select remain_budget from hire_seniors limit 1),70000) - b.cumulate_budget as remain_budget from ( select max(cumulate_budget) as cumulate_budget from juniors_salary_rank where cumulate_budget \u0026lt;= ifnull((select remain_budget from hire_seniors limit 1),70000) ) a join juniors_salary_rank b using(cumulate_budget) ) select distinct(experience) as experience, case when experience = \u0026#39;Senior\u0026#39; then ifnull((select rn from hire_seniors limit 1 ),0) when experience = \u0026#39;Junior\u0026#39; then ifnull((select rn from hire_junior limit 1 ),0) end as accepted_candidates from candidates a ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/the-number-of-seniors-and-juniors-to-join-the-company/","summary":"題目 Table: Candidates\n+-------------+------+ | Column Name | Type | +-------------+------+ | employee_id | int | | experience | enum | | salary | int | +-------------+------+ employee_id is the primary key column for this table. experience is an enum with one of the values (\u0026#39;Senior\u0026#39;, \u0026#39;Junior\u0026#39;). Each row of this table indicates the id of a candidate, their monthly salary, and their experience. A company wants to hire new employees.","title":"[leetcode][Database][Hard] 2004. The Number of Seniors and Juniors to Join the Company"},{"content":"題目 Table: Buses\n+--------------+------+ | Column Name | Type | +--------------+------+ | bus_id | int | | arrival_time | int | | capacity | int | +--------------+------+ bus_id is the primary key column for this table. Each row of this table contains information about the arrival time of a bus at the LeetCode station and its capacity (the number of empty seats it has). No two buses will arrive at the same time and all bus capacities will be positive integers. Table: Passengers\n+--------------+------+ | Column Name | Type | +--------------+------+ | passenger_id | int | | arrival_time | int | +--------------+------+ passenger_id is the primary key column for this table. Each row of this table contains information about the arrival time of a passenger at the LeetCode station. Buses and passengers arrive at the LeetCode station. If a bus arrives at the station at a time tbus and a passenger arrived at a time tpassenger where tpassenger \u0026lt;= tbus and the passenger did not catch any bus, the passenger will use that bus. In addition, each bus has a capacity. If at the moment the bus arrives at the station there are more passengers waiting than its capacity capacity, only capacity passengers will use the bus.\nWrite an SQL query to report the number of users that used each bus.\nReturn the result table ordered by bus_id in ascending order.\nSQL Schema\nCreate table If Not Exists Buses (bus_id int, arrival_time int, capacity int) Create table If Not Exists Passengers (passenger_id int, arrival_time int) Truncate table Buses insert into Buses (bus_id, arrival_time, capacity) values (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;1\u0026#39;) insert into Buses (bus_id, arrival_time, capacity) values (\u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;10\u0026#39;) insert into Buses (bus_id, arrival_time, capacity) values (\u0026#39;3\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;2\u0026#39;) Truncate table Passengers insert into Passengers (passenger_id, arrival_time) values (\u0026#39;11\u0026#39;, \u0026#39;1\u0026#39;) insert into Passengers (passenger_id, arrival_time) values (\u0026#39;12\u0026#39;, \u0026#39;1\u0026#39;) insert into Passengers (passenger_id, arrival_time) values (\u0026#39;13\u0026#39;, \u0026#39;5\u0026#39;) insert into Passengers (passenger_id, arrival_time) values (\u0026#39;14\u0026#39;, \u0026#39;6\u0026#39;) insert into Passengers (passenger_id, arrival_time) values (\u0026#39;15\u0026#39;, \u0026#39;7\u0026#39;) 解題思考 乘客 passenger 的抵達時間需要在公車 bus 抵達之前，計算每班公車 bus_id 抵達時，可能潛在的乘客 passenger_id 總數量。\n由於公車 bus_id 的運載能力 capacity 可能無法滿足當前等待中的所有乘客 passenger_id ， 因此先行計算每班公車可能需要負荷的乘客運載量 承上，比較當前公車 bus_id運載能力 capacity 與等待搭乘的乘客數量，並從兩者中取最小值，表達搭乘該班次公車 bus_id 的實際乘客數量 可能存在的乘客數 - 累積的乘客數 = 實際搭乘的乘客數量 建立暫存表初始化 mysql variable ，用以 暫存實際搭乘的乘客數量 和 可能存在的乘客數 解決方案 with people_possiable_take_bus as ( /* Finding the possible passengers with each bus regardless of passenger catch the bus or not, that will cumulate all of passengers whoes previous arrival */ select a.bus_id, a.arrival_time, a.capacity, count(b.passenger_id) as possible_passenger_cnt from buses a left join passengers b on b.arrival_time \u0026lt;= a.arrival_time group by a.bus_id order by a.arrival_time ), alloc_people_take_bus as ( /* Calculating how many passengers can loaded by each bus, and accumulate passengers regardless of passenger catch bus or not, due to caulse `people_possiable_take_bus` pre-calculate the possible passengers with each bus, and the situation for the passengers whoes cannot catch currently or pervious bus includes of the statement `least(capacity, possible_passenger_cnt-@accum_passengers)`, that\u0026#39;s why we can directly to calculate `possible_passenger_cnt-@accum_passengers` and compare bus\u0026#39;s capacity and `possible_passenger_cnt-@accum_passengers` to take the least value */ select bus_id, passengers_cnt from ( select a.bus_id, @passengers_cnt := least(capacity, possible_passenger_cnt-@accum_passengers) as passengers_cnt, @accum_passengers := @accum_passengers + @passengers_cnt from people_possiable_take_bus a, (select @passengers_cnt := 0, @accum_passengers :=0) b ) output ) select bus_id, passengers_cnt from alloc_people_take_bus order by bus_id ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/the-number-of-passengers-in-each-bus-ii/","summary":"題目 Table: Buses\n+--------------+------+ | Column Name | Type | +--------------+------+ | bus_id | int | | arrival_time | int | | capacity | int | +--------------+------+ bus_id is the primary key column for this table. Each row of this table contains information about the arrival time of a bus at the LeetCode station and its capacity (the number of empty seats it has). No two buses will arrive at the same time and all bus capacities will be positive integers.","title":"[leetcode][Database][Hard] 2153. The Number of Passengers in Each Bus II"},{"content":"題目 Table: Products\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | product_id | int | | store | varchar | | price | int | +-------------+---------+ (product_id, store) is the primary key for this table. Each row of this table indicates the price of product_id in store. There will be at most 30 different stores in the table. price is the price of the product at this store. Important note: This problem targets those who have a good experience with SQL. If you are a beginner, we recommend that you skip it for now.\nImplement the procedure PivotProducts to reorganize the Products table so that each row has the id of one product and its price in each store. The price should be null if the product is not sold in a store. The columns of the table should contain each store and they should be sorted in lexicographical order.\nThe procedure should return the table after reorganizing it.\nReturn the result table in any order.\nSQL Schema\nCreate table If Not Exists Products (product_id int, store varchar(7), price int) Truncate table Products insert into Products (product_id, store, price) values (\u0026#39;1\u0026#39;, \u0026#39;Shop\u0026#39;, \u0026#39;110\u0026#39;) insert into Products (product_id, store, price) values (\u0026#39;1\u0026#39;, \u0026#39;LC_Store\u0026#39;, \u0026#39;100\u0026#39;) insert into Products (product_id, store, price) values (\u0026#39;2\u0026#39;, \u0026#39;Nozama\u0026#39;, \u0026#39;200\u0026#39;) insert into Products (product_id, store, price) values (\u0026#39;2\u0026#39;, \u0026#39;Souq\u0026#39;, \u0026#39;190\u0026#39;) insert into Products (product_id, store, price) values (\u0026#39;3\u0026#39;, \u0026#39;Shop\u0026#39;, \u0026#39;1000\u0026#39;) insert into Products (product_id, store, price) values (\u0026#39;3\u0026#39;, \u0026#39;Souq\u0026#39;, \u0026#39;1900\u0026#39;) 解題思考 題目要求輸出每個產品 product 在每間商店的售價 +------------+----------+--------+------+------+ | product_id | LC_Store | Nozama | Shop | Souq | +------------+----------+--------+------+------+ | 1 | 100 | null | 110 | null | | 2 | null | 200 | null | 190 | | 3 | null | null | 1000 | 1900 | +------------+----------+--------+------+------+ 使用 group_concat() 組合樞紐表欄位的 sql statement，因樞紐表需依據 store 列出欄位，而 store 的數目是不固定的。 使用 prepare statement 執行包含 group_concat() 預先組合好的 sql statement 使用 group product_id 對樞紐表進行總計，因樞紐表要求依據 product 統計在不同 store 中的售價 price 解決方案 CREATE PROCEDURE PivotProducts() BEGIN -- Override GROUP_CONCAT length which has a default limit of 1024 SET SESSION group_concat_max_len = 1000000; -- Store case statement for dynamically generated columns in a variable ie case_stmt SET @case_stmt = NULL; SELECT GROUP_CONCAT(DISTINCT CONCAT(\u0026#39;SUM(CASE WHEN store = \u0026#34;\u0026#39;, store, \u0026#39;\u0026#34; THEN price END) AS \u0026#39;, store)) INTO @case_stmt FROM products; -- Insert above statement (@case_stmt) in the following main query to frame final query SET @sql_query = CONCAT(\u0026#39;SELECT product_id, \u0026#39;, @case_stmt, \u0026#39; FROM products GROUP BY product_id\u0026#39;); -- Execute final query PREPARE final_sql_query FROM @sql_query; EXECUTE final_sql_query; DEALLOCATE PREPARE final_sql_query; END ","permalink":"https://blog.zhengweiliu.com/posts/leetcode/database/dynamic-pivoting-of-a-table/","summary":"題目 Table: Products\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | product_id | int | | store | varchar | | price | int | +-------------+---------+ (product_id, store) is the primary key for this table. Each row of this table indicates the price of product_id in store. There will be at most 30 different stores in the table. price is the price of the product at this store. Important note: This problem targets those who have a good experience with SQL.","title":"[leetcode][Database][Hard] 2252. Dynamic Pivoting of a Table"},{"content":"在 [Customer Data Platform 是如何煉成的 (二)]中提到了 User Behavior，但 User Behavior 的資料從哪裡來，又該如何定義呢 ? Customer Data Platform 是如何煉成的 (二) Date: 2022-03-28 \u0026nbsp; Categories: #Customer Data Platform\u0026nbsp; 顯而易見，討論出一個有共識、覺得可行的方式將問題進行轉化，這就屬於洞察(Insights)；而被提出的問題本身，則是被發現的異常(Anomaly)。原文最後提出「具有評分等級的使用者清單」則是貢獻(Contribute)。 有趣的是，當營銷人員依據清單進行預算投放後，便又能獲取新一輪的結果，這個結果除了可供驗證，同時也具備發現新異常的可能性。 ...... 以 GA ( Google Analytics ) 為例，在 Web 或 Mobile APP 中進行 GA 埋 code，這些 code 可以是 GA 預設的事件，如 : Page View 、 Session Engagement 、 Activity User，或者是自定義的 event 等等。 GCP BigQuery 提供的 Public datasets 中也提供已將 GA 資料轉化為 ecommerce 的 dataset ，來源為 Qwiklabs: Predict Visitor Purchases with a Classification Model in BQML\n本篇文章也利用這份公開資料集進行說明 :\nWhat Goal We Need Feature and Label Improve and Tune User Behavior 比較直觀的是對 behavior 的理解，可以想像當消費者在不同的 E-Commerce website 進行瀏覽商品、獲得推薦或是購物車結帳等操作時，由於 website 的設計不同，消費者可能需要跨越不同的頁面、點擊不同的連結，或是輸入不同的資料等等。\n因此，消費者在 website A 與 website B 的 「behavior」也會不同；從另外一個角度來看 : 同一個 website 中，消費者要達成相同目的的操作，必定會在有限個數的操作途徑中完成。而這些途徑也就構成了 User Behavior 基本單位，而有針對性、目的性的對途徑資料進行挑選，也就構成了一個 specific behavior 的定義。\nWhat Goal We Need ? 在一開始會想知道 website 訪問人數、購買次數以及轉化率各是多少；如下圖所示，分別是\n訪問人數 : 約 74 萬 購買次數 : 約 2 萬 轉化率 : 約 2.7% 然而，僅從結果層面獲得的資料，無法描述每項商品的銷售情況，因此對每項商品進行排比\n這可能就是常見的報表內容，即各項商品的銷售情況與 website 的成效指標；但是若更進一步的思考，這份報表所表達的是對資料進行統計後的資訊，或是常見的用詞 Data Informed ；那麼，分析的目標只是產生報表數據嗎 ?\n或者，是希望能從資料中協助識別「哪些訪問者更可能會促成購買商品的事件」呢 ?\nFeature and Label 首先考慮有多少訪問人數進行了購買，包含了第一次瀏覽就購買以及再次訪問後進行購買\n共有 (11,873 / 741,721) = 1.6% 的人是在第二次訪問商品頁面時，才進行購買；雖然沒有一個正確答案，但普遍的原因可能是消費者在購買前會進行商品的比價。\n以此為例，可以從原始資料中列舉一些因素，作為判斷訪問者是否會進行購買的依據 ( feature )，並將再次回訪是否產生購買行為作為答案 ( label ) 對其訓練一個模型 ( model )；期望在下次收集到相關資料時，模型可以識別並告知訪問者是否會產生購買行為。\n第一次挑選 feature 時，對以下兩個因素進行分析\nbounces : 訪問者是否立即離開 website time_on_site : 訪問者在 website 停留的時間 通常在訓練和評估模型之前，直接判斷 feature 的選擇是好或不好都為時過早，但在 time_on_site 排比前 10 的結果中，只有 1 個客戶返回購買；而模型測試的準確率也確實不好。\nImprove and Tune 在原始數據中，可能有更多的 feature 可以幫助 model 進行識別購買行為起到作用；而找出 feature 的方法除了對所有排列組合逐一進行嘗試外，也可以透過與相關人員，如 : 營銷人員、UX 設計師、統計專家或資料科學家等等，進行討論並達成共識後得出。\n在這個案例中，除了 bounces 與 time_on_site 之外，還可以加入以下的 feature\n訪問者第一次訪問時，在結帳過程中經歷了多少次的操作 (距離) 流量的來源 : 透過搜索或是 referring site 等等 設備的類別 : 手機 、 平板 或是 PC 地理資訊 : 來自哪個國家 重新訓練模型後測試的準確率也有所提高，同時模型也能提供一個預測結果，告知該訪問者是否會進行購買行為\nSummary Qwiklabs: Predict Visitor Purchases with a Classification Model in BQML 的案例在最後給出的結論如下\n在前 6% 的首次訪問者中，超過 6% 的人會在後續訪問時產生購買行為 整體而言，只有 0.7% 的首次訪問者，會在後續訪問時產生購買行為 瞄準前 6% 的第一次訪問者名單，會使營銷投資回報率提高 9 倍 因此，若是能夠在得到模型預測的結果後，依據營銷策略進行即時的投放處理，包含但不限於: EDM廣告、Coupon折價券或是限定綑綁折扣等等，建構起一個自動化營銷的方法；這也是一種 Data Driven 的方法。\n整篇文章寫到這裡，從 「What Goal We Need」 中辨別更具價值的目標、「Feature and Label」中定義需產出的貢獻與評斷方式，到最後「Improve and Tune」透過討論達成共識，並進行相對應的調整，讓整體結果能夠產出的更好貢獻，我認為這是一種不斷優化與建立 CDP 的好方法。\n","permalink":"https://blog.zhengweiliu.com/posts/normal/customer-data-platform-3/","summary":"\u003cp\u003e在最後給出的結論如下\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在前 6% 的首次訪問者中，超過 6% 的人會在後續訪問時產生購買行為\u003c/li\u003e\n\u003cli\u003e整體而言，只有 0.7% 的首次訪問者，會在後續訪問時產生購買行為\u003c/li\u003e\n\u003cli\u003e瞄準前 6% 的第一次訪問者名單，會使營銷投資回報率提高 9 倍\n因此，若是能夠在得到模型預測的結果後，依據營銷策略進行即時的投放處理，包含但不限於: EDM廣告、Coupon折價券或是限定綑綁折扣等等，建構起一個自動化營銷的方法；這也是一種 \u003cstrong\u003eData Driven\u003c/strong\u003e 的方法。\u003c/li\u003e\n\u003c/ul\u003e\n","title":"Customer Data Platform 是如何煉成的 (三)"},{"content":"在 [Customer Data Platform 是如何煉成的]中提到，Data Platform 透過洞察與發現 ( Insights Discovery )、貢獻與進化 ( Contribute Evolution) 以及異常偵測 ( Anomaly Detection) 組成一個周而復始的正向循環，讓資料提供具有貢獻的結果。 Customer Data Platform 是如何煉成的 Date: 2022-03-22 \u0026nbsp; Categories: #Customer Data Platform\u0026nbsp; 提到 CDP ( Customer Data Platform ) ，可能就會想到利用顧客相關資料，為顧客分群分類貼標籤，透過網站、經營社群或 APP 進行精準投放廣告，達到再行銷的成果；甚至是透過 Machine Learning，或結合 CRM 、 Google Analytics 等資料，達成預估市場規模、優化推薦商品等目的。 ...... 最近讀到 How to Optimize KPIs by Distilling Data With Machine Learning 這篇文章，原文中提及的例子: 利用 User Behavior 進行機器學習，最後產出一個具備評分的可交付名單，提供給營銷人員進行後續操作；詳細的部分請再點閱原文，以下我想分享閱讀後的心得。\n費米估算 依據維基百科的描述\n一個經典的費米問題的例子是費米提出的「在芝加哥有多少鋼琴調琴師」\n比如說，我們會採用以下的假設\n1. 大約有9,000,000 人生活在芝加哥。\n2. 在芝加哥平均每個家庭有2個人。\n3. 大約在20個家庭中有1個家庭需要定期鋼琴調音。\n4. 定期調琴的鋼琴每年需要調整一次。\n5. 每個調琴師大約需要2小時調琴，包括路上時間。\n6. 每個調琴師每天工作8小時，一周5天，一年50周。\n最後經過計算，大略的估算出 225 個調琴師在芝加哥；在這個問題的時空背景下，事實上， 一共有大約 290 名調琴師在芝加哥。\n量化問題 在調琴師的問題中，在沒有任一組織或權威可以提供完整調琴師清單的情況下，從圍繞調琴師的因素進行分析與展開，對相關的條件訂下一個基準數據，再一步步逼近命題，最終估算出大略的人數；\n而原文中「如何有效利用有限的營銷預算」，換個方式來描述問題則變成「將營銷預算投入給哪些消費者，可以取得最大成效」 : 這使得問題本身可以圍繞著「消費者」這個因素進行分析與展開，或許是分析使用者消費行為、或許是透過 RFM 來進行初步的分析，更可以導入消費者會員等級權益等額外資料進行輔助。\n顯而易見，討論出一個有共識、覺得可行的方式將問題進行轉化，這就屬於洞察(Insights)；而被提出的問題本身，則是被發現的異常(Anomaly)。原文最後提出「具有評分等級的使用者清單」則是貢獻(Contribute)。\n有趣的是，當營銷人員依據清單進行預算投放後，便又能獲取新一輪的結果，這個結果除了可供驗證，同時也具備發現新異常的可能性。\n背後的核心 同樣以調琴師的問題為例\n是否曾考慮過，為什麼「在芝加哥有多少鋼琴調琴師」這個問題會被提出 ?\n這就具備無限多種可能性，如 : 「目前有一個專供調琴師使用的產品，因此想知道芝加哥的市場規模」「芝加哥是否適合擴大鋼琴的販售規模」「如果想投身調琴師的職業，在芝加哥是否適合」 等等\n與原文提出的「如何有效利用有限的營銷預算」一致，「怎麼做能夠提升營銷數字」「某類型的商品是否適合投放營銷預算」等等\n我們希望借鑑成功案例時，不妨先試著共同討論\n我們正面對著什麼樣的情況 ?\n我們希望能改變什麼情況 ?\n以更好的辨認目前的問題，屬於可以分析的「真議題」，或者只是展現一個虛榮數字的「假議題」\n","permalink":"https://blog.zhengweiliu.com/posts/normal/customer-data-platform-2/","summary":"顯而易見，討論出一個有共識、覺得可行的方式將問題進行轉化，這就屬於洞察(Insights)；而被提出的問題本身，則是被發現的異常(Anomaly)。原文最後提出「具有評分等級的使用者清單」則是貢獻(Contribute)。\n有趣的是，當營銷人員依據清單進行預算投放後，便又能獲取新一輪的結果，這個結果除了可供驗證，同時也具備發現新異常的可能性。","title":"Customer Data Platform 是如何煉成的 (二)"},{"content":"提到 CDP ( Customer Data Platform ) ，可能就會想到利用顧客相關資料，為顧客分群分類貼標籤，透過網站、經營社群或 APP 進行精準投放廣告，達到再行銷的成果；甚至是透過 Machine Learning，或結合 CRM 、 Google Analytics 等資料，達成預估市場規模、優化推薦商品等目的。\n似乎只要有充分的資料，就能開始享受 CDP 為行銷帶來諸多好處。然而，具體上 CDP 是怎麼運作的 ? 又該如何善用 CDP 的功能呢 ? 或許可以從瞭解 CDP 是如何構成的開始。\nCustomer and Data Platform 不如簡單粗暴的，試著從名稱上將 Customer 與 Data Platform 分開 ：\n對於 Customer ，或者說是與顧客相關的資料，從會員帳號的建立到商品購買紀錄、網頁瀏覽操作紀錄，甚至是會員權益分級等等；因顧客的主動行為產生，無論是否存在誘因，並且蒐集起來寶貴資料。\n如同每道料理都是由食材原料組成，僅有食材原料卻沒有辦法變成美味的料理；Data Platform 擔任烹飪者的角色，將這些寶貴資料進一步處理，端出一道又一道的營養又可口的美食。\nBring Benefits with Data-Platform 回想一下料理的烹飪過程，從準備材料、對食材進行清洗、將食材切割成適當大小或醃製入味，到觀察火候並依序加入對應的食材，最後出鍋上菜，若上菜後發現味道不好，還可以再依據這次的經驗進行調整與修正。\nData Platform 也對資料進行清理、前處理，對處理後的資料進行重組，最終產生出有貢獻的數據成果，並透過轉譯的方式進行交付；整個流程就像下圖所描述：\n洞察與發現 ( Insights Discovery ) \u0026gt; 貢獻與進化 ( Contribute Evolution) \u0026gt; 異常偵測 ( Anomaly Detection) \u0026gt; 洞察與發現 ( Insights Discovery ) \u0026gt; …\n而在這一循環的流程中，都緊密的圍繞著一個核心 ( Kernel ) 在進行：如同CDP 是的 Kernel 是 Customer 、 麻婆豆腐的 Kernel 是麻婆一樣 。\nData Platform 中的所有處理、步驟以及流程，都是為了核心在服務。\n洞察與發現 ( Insights Discovery ) 如果我們產生一個疑問，大多數的情況下在第一時間，我們都會問 :「發生什麼事 ?」 或是 「某個事件是不是造成什麼影響」，而不會是 「某個具體的量(或者數字)是多少」\n這些問題本身就是 Insight 的催化劑，促使我們想進一步去分析、去理解\n貢獻與進化 ( Contribute Evolution) 提出問題並在分析後，若能找到一些可能的答案，便能利用 IFTTT ( If This Then That ) 進行問題簡化，即 :\n如果發生 A 狀況，那麼會造成什麼結果 / 需要如何應對\n這是一種將數據結果進行翻譯的過程，讓數據變成一個應對清單，並能輕鬆的交付給其他利益相關者，以便他們對結果進行下一步的操作。\n異常偵測 ( Anomaly Detection) 從上一步驟產生的應對清單，在利益相關者對其進行操作之後，如 : 廣告投放 ， 便能對清單進行驗證、討論；或是對特定名單進行持續的觀測，看看是否仍有與數據結果不符，或者相對異常的行為出現。\n當上述的行為出現後，在與相關利益者們進一步討論原因 ( 回到了 Insights Discovery ) ，獲取新的或者增強應對清單 ( 又到了 Contribute Evolution ) ，再次投入進行操作 ( 再持續進行 Anomaly Detection )，以此往復直到這個問題不再需要處理，或者不再具備價值時，就能停止。\n","permalink":"https://blog.zhengweiliu.com/posts/normal/customer-data-platform-1/","summary":"提到 CDP ( Customer Data Platform ) ，可能就會想到利用顧客相關資料，為顧客分群分類貼標籤，透過網站、經營社群或 APP 進行精準投放廣告，達到再行銷的成果；甚至是透過 Machine Learning，或結合 CRM 、 Google Analytics 等資料，達成預估市場規模、優化推薦商品等目的。","title":"Customer Data Platform 是如何煉成的"},{"content":"在 GCP Billing Analytics 中提到過關於 Cloud Functions 的計費超乎預期，進一步分析開發的使用習慣後，也找出部分功能應該將其從 Cloud Functions 搬遷至基於 GCE instances 的服務上，以達到節費的期望。 GCP Billing Analysis Date: 2021-12-27 \u0026nbsp; Categories: #Google Cloud Platform\u0026nbsp; #Analysis\u0026nbsp; 在產品的開發中，團隊消耗成本最高的前幾項排名既在意料之中，Google Compute Engine (GCE)、 Cloud Functions 、 BigQuery 以及 Google Cloud Storage，但細項的部分也在意料之外。 ...... 在原先的設計中，我們將 Cloud Functions 作為 ETL data flow 的其中一個環節，透過 Pub/Sub trigger Cloud Functions 的方式使其運作；考慮到 Pub/Sub subscriber push/pull 的 Ack 等待時間有著最長 600 秒的限制，我將這部分需要搬遷的 Cloud Functions 大致分為兩種需求\n靜態資料源: 在提取資料時，可預期資料是存在且可被存取的 動態資料源: 可能發生資料不存在，或者是無法存取的情況 本篇文章是記錄\n用 Kubernetes Pod 替代 Cloud Function 環節以處理動態資料源的方法 Google Kubernetes Engine: Ingress \u0026amp; Service ASGI 與FastAPI Dockerize \u0026amp; Deployment 靜態資料源的處理方案 \u0026gt; Migrate Google Cloud Functions to Airflow Migrate Google Cloud Functions to Airflow Date: 2022-01-22 \u0026nbsp; Categories: #Google Cloud Platform\u0026nbsp; #Data Engineering\u0026nbsp; 本篇文章是記錄 用 Airflow DAG (Directed Acyclic Graph) 替代 Cloud Function 環節以處理靜態資料源的方法 Airflow GCP Operators 使用 在 DAG 中平行處理(parallel processing)的方式 ...... Design Change Figure 1 是一個常見的使用案例，我將 Cloud Function 的執行邏輯簡略為 4 個部份來進行描述，即: 等待 Request (Accept Request) 、 處理邏輯 (Process)、產出結果 (Result) ，以及回復 Ack (Response HTTP Status Code)\nProcess 的區塊中，若需要向外部資料源提出存取請求，如: 3rd-party API 、爬蟲、網路磁碟機等，獲取相關的資訊後才能繼續進行處理的工作，在本篇文章中則以動態資料源來稱呼這些外部資料源\n對於 Runtime 時可能遭遇錯誤的資料源，可能遇到請求被拒絕(Reject)，如: 403、404或者5系列的錯誤代碼，或是遇到請求的資源本身不存在。\nGoogle Kubernetes Engine: Ingress \u0026amp; Service Figure 2 使用 Kubernetes Pod 替代 Cloud Function ， 因團隊先前已採用 Google Kubernetes Engine (GKE) 進行容器化的部署，這邊也就延續團隊成果。\n我也將 Pub/Sub 的模式從 trigger 更改為 Push Message : 當 Pub/Sub Subscriber Queue 存在訊息時， Subscriber 會推送 Message 到設定好的 Webhook URL，並且遵循 Ack 等待時間有著最長 600 秒的限制。\n關於 Deployment 的部分會在稍後提到，這邊先討論 Ingress 和 Service 的設置\nService Type: NodePort\napiVersion: v1kind: Servicemetadata: name: my-servicespec: type: NodePort selector: app: MyApp ports: # By default and for convenience, the `targetPort` is set to the same value as the `port` field. - port: 80 targetPort: 80 nodePort: 30080 Ingress\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-service-backend annotations: ingress.gcp.kubernetes.io/pre-shared-cert: \u0026#34;k8s-example-com\u0026#34; kubernetes.io/ingress.allow-http: \u0026#34;false\u0026#34; kubernetes.io/ingress.global-static-ip-name: k8s-example-com spec: defaultBackend: service: name: my-services port: number: 80 rules: - host: k8s.example.com http: paths: - path: /my-service pathType: Prefix backend: service: name: my-service port: number: 80 這樣便能將 Ingress 和 Service 設置完成，Ingress 和 Service 需要在同一個 namespace 。\nASGI \u0026amp; FastAPI 考量到團隊開發大部份依賴 Python framework，因此在替代 Cloud Function HTTP Server 的選擇上，最後我採用了基於 ASGI (Asynchronous Server Gateway Interface) 的 FastAPI ，以應付團隊中除了 Pub/Sub 之外的需求。\n對於 WSGI 和 ASGI 的比較，我覺得這篇博客 WSGI与ASGI的区别与联系 說的很清楚，推薦大家可以看一下。\nFastAPI 的文件中也詳細提供了製作 Container Image 的方法，同時也提到了關於部署在 Kubernetes 上的注意事項，有一份詳細、容易使用的官方文件，也是我選擇 FastAPI 的原因之一，並且 FastAPI 也內建了 Swagger UI 和 ReDoc 兩種文件模式，這也是一個加分大項。\nDockerize \u0026amp; Deployment Dockerfile\n依據 FastAPI 文件提供 Dockerfile 撰寫即可，需注意在 uvicorn 的 command加上 --proxy-headers 。\nFROM python:3.8 WORKDIR / COPY ./requirements.txt /requirements.txt RUN pip install --no-cache-dir --upgrade -r /requirements.txt COPY ./ / CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;main:app\u0026#34;, \u0026#34;--proxy-headers\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;80\u0026#34;] 依需求更改 Dockerfile 時需要注意 Docker Build Cache，由於 Docker Build Image 時會一層一層的往上迭代(每一行指令就是一層)， 而每一次 Build Image 都會檢查與上一次的差異，並從影響差異的 最低層 重新迭代，如: 當 requirements.txt 內容有所變更時，即便 source code 沒有改變，該次的 Docker Build 也會從 COPY ./requirements.txt /requirements.txt` 開始從新迭代。\nMain.py\n在 main.py 提供 domain host 之後的完整 URL path ，讓 app 的 route 可以找到對應的端口，並提供 /my-service/health 給 Load Balancer 進行 health check。\nfrom typing import Optional, Dict from fastapi import (FastAPI, status) from fastapi.encoders import jsonable_encoder from pydantic import BaseModel class Message(BaseModel): attrs: Optional[Dict] = None data: str message_id: str publish_time: str class PubSubMessage(BaseModel): message: Message subscription: str app = FastAPI() [@app](http://twitter.com/app \u0026#34;Twitter profile for @app\u0026#34;).get(\u0026#39;/\u0026#39;, status_code=status.HTTP_200_OK) def home(): pass [@app](http://twitter.com/app \u0026#34;Twitter profile for @app\u0026#34;).get(\u0026#39;/my-service/health\u0026#39;, status_code=status.HTTP_200_OK) def health(): pass @app.post(\u0026#39;/my-service/subscriber-webhook\u0026#39;, status_code=status.HTTP_200_OK) def subscriber_webhook(message: PubSubMessage): message_data: Dict = jsonable_encoder(message) return message_data Deployment\n依據 Kubertenes 官方提供的模板撰寫，再依需求進行更改即可。\napiVersion: apps/v1 kind: Deployment metadata: name: subscriber-webhook-deployment labels: app: subscriber-webhook spec: replicas: 3 selector: matchLabels: app: subscriber-webhook template: metadata: labels: app: subscriber-webhook spec: containers: - name: subscriber-webhook image: {REPLACE_YOUR_REGISTRY}/subscriber-webhook:1.0 ports: - containerPort: 80 可視需要加入 readinessProbe 或 livenessProbe\n如果有 Autoscaling 的需求，參考 Horizontal Pod Autoscaling (HPA) 與 範例 修改即可。\n","permalink":"https://blog.zhengweiliu.com/posts/normal/migrate-google-cloud-functions-to-kubernetes/","summary":"\u003cp\u003e本篇文章是記錄\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#1234\"\u003e用 \u003ccode\u003eKubernetes Pod\u003c/code\u003e 替代 \u003ccode\u003eCloud Function\u003c/code\u003e 環節以處理\u003ccode\u003e動態資料源\u003c/code\u003e的方法\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dee1\"\u003e\u003ccode\u003eGoogle Kubernetes Engine: Ingress \u0026amp; Service\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#fa5e\"\u003e\u003ccode\u003eASGI 與FastAPI\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#b458\"\u003e\u003ccode\u003eDockerize \u0026amp; Deployment\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n","title":"Migrate Google Cloud Functions to Kubernetes"},{"content":"在 [GCP Billing Analytics] 中提到過關於 Cloud Functions 的計費超乎預期，進一步分析開發的使用習慣後，也找出部分功能應該將其從 Cloud Functions 搬遷至基於 GCE instances 的服務上，以達到節費的期望。 GCP Billing Analysis Date: 2021-12-27 \u0026nbsp; Categories: #Google Cloud Platform\u0026nbsp; #Analysis\u0026nbsp; 在產品的開發中，團隊消耗成本最高的前幾項排名既在意料之中，Google Compute Engine (GCE)、 Cloud Functions 、 BigQuery 以及 Google Cloud Storage，但細項的部分也在意料之外。 ...... 在原先的設計中，我們將 Cloud Functions 作為 ETL data flow 的其中一個環節，透過 Pub/Sub trigger Cloud Functions 的方式使其運作；考慮到 Pub/Sub subscriber push/pull 的 Ack 等待時間有著最長 600 秒的限制，我將這部分需要搬遷的 Cloud Functions 大致分為兩種需求\n靜態資料源: 在提取資料時，可預期資料是存在且可被存取的 動態資料源: 可能發生資料不存在，或者是無法存取的情況 本篇文章是記錄\n用 Airflow DAG (Directed Acyclic Graph) 替代 Cloud Function 環節以處理靜態資料源的方法 Airflow GCP Operators 使用 在 DAG 中平行處理(parallel processing)的方式 動態資料源的處理方案 \u0026gt; Migrate Google Cloud Functions to Kubernetes\nDesign Change Figure 1 是一個經典的使用案例，透過 GCS notification 的機制，當 bucket 中有檔案 (Object) 異動時，將異動的資訊 publish 到指定的 Pub/Sub Topic。 部署 Cloud Function 可以指定--trigger-topic 接受 Topic 的觸發，使得 Cloud Function 可以接收異動檔案的資訊，如: bucket name、object path ， 進行轉置 (Transform) 處理後將結果存放到 Big Query 。\n這也是我稱呼為靜態資料源的原因\n由於訊息傳遞的時間相對迅速，當 Cloud Function 需要擷取對應的檔案時，該檔案存在於 GCS 上的對應位置\nAirflow DAG \u0026amp; Operators Figure 2 則用 Airflow 2.0 DAG 替代 Cloud Function ， Airflow 是 Python based 的工作流管理系統，可以幫助開發者將工作流程標準化以及執行重複性的工作，我認為滿適合應用於靜態資料源的場景上。\n並且 Airflow 官方也提供對應 GCP 服務 Operators 的文件 與 安裝方式。可以直接使用，也可以參考 Operators 的 Source Code 來重新編寫自定義的 Operator；如 Figure 2 的 PubSubPullOperator 便能直接使用官方提供的 packages ，而 TransformOperator與 BatchInsertOperator 也可以尋找到對應 operator source code 以進行參考與改寫，以後有機會的話在另外撰寫文章記錄。\nParallel Processing in DAG 在 Figure 2 中改為使用 pull message 的方式，因此可以透過 PubSubPullOperator 來設置每次拉取訊息數量的上限；\n然而，考慮到 Airflow schedule 的最小間隔單位為 1 分鐘，一旦 publish message 的數量與日遽增、或是出現 burst 的情形時，僅憑一組 PubSubPullOperator \u0026gt; TransformOperator \u0026gt; BatchInsertOperator 的工作流程設置可能無法消化；因此就需要考量在 DAG 中建立多組的工作流程，以進行平行處理。\nFigure 3 是我理解 Pub/Sub 拉取訊息的工作原理(若有錯誤也煩請指正，感謝)，訊息的傳遞步驟略可簡述為:\nStep 1. Publish message to Topic\nStep 2. Message push into subscriber group queue by fanout mode\nStep 3. Single/Multi puller to pull message from a subscriber group queue\n這樣一想就比較簡單了，只要在 DAG 中建立多組的 PubSubPullOperator \u0026gt; TransformOperator \u0026gt; BatchInsertOperator 工作流程，每個 PubSubPullOperator 都扮演著 Puller 的角色。\n如 Figure 4 展示的工作流程設置，這樣就能達成平行處理的構想啦! 同時，我也採用了 DummyOperator 作為整個 DAG 的起始與完成，主要是希望在使用 Airflow UI 時能夠比較好的表達 DAG 的工作狀態。\nPython example code for parallel processing in DAG PARALLEL: int = 5 start = DummyOperator(task_id=\u0026#39;start\u0026#39;) complete = DummyOperator(task_id=\u0026#39;complete\u0026#39;) for i in range(PARALLEL): pull_message = PubSubPullOperator(task_id=f\u0026#39;pull_message_{i}\u0026#39;) transform = TramsformOperator(task_id=f\u0026#39;transform_{i}\u0026#39;) insert_to_bq = BatchInsertOperator(task_id=f\u0026#39;insert_to_bq_{i}\u0026#39;) start \u0026gt;\u0026gt; pull_message \u0026gt;\u0026gt; transform \u0026gt;\u0026gt; insert_to_bq \u0026gt;\u0026gt; complete 另外，在能估算出單位時間 publish 的 message 數量，便能簡單地將 schedule 間隔時間、單次拉取訊息的數量上限，以及工作流程組數視為調整參數，以調整工作流程的處理效率。\n","permalink":"https://blog.zhengweiliu.com/posts/normal/migrate-google-cloud-functions-to-airflow/","summary":"\u003cp\u003e本篇文章是記錄\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#a63a\"\u003e用 \u003ccode\u003eAirflow DAG\u003c/code\u003e (Directed Acyclic Graph) 替代 \u003ccode\u003eCloud Function\u003c/code\u003e 環節以處理\u003ccode\u003e靜態資料源\u003c/code\u003e的方法\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#7077\"\u003e\u003ccode\u003eAirflow GCP Operators\u003c/code\u003e 使用\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#0481\"\u003e在 \u003ccode\u003eDAG 中平行處理(parallel processing)\u003c/code\u003e的方式\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n","title":"Migrate Google Cloud Functions to Airflow"},{"content":"最近利用 GA4 、 UA ，以及團隊的開發產品所蒐集到的資料，協助團隊進一步了解產品的成效與成本的利用情況。團隊的開發與產品環境皆建立在 Google Cloud Platform (GCP) 上，在分析 GCP billing report 的原始資料時，也引發了我 \u0026ldquo;對於同仁們對於如何利用開發環境\u0026rdquo; 感到好奇，寫下這篇文章作為紀錄。\n在產品的開發中，團隊消耗成本最高的前幾項排名既在意料之中，Google Compute Engine (GCE)、 Cloud Functions 、 BigQuery 以及 Google Cloud Storage，但細項的部分也在意料之外。\nGoogle Compute Engine (GCE) 在 GCP 上，無論我們開啟的是一般的 VM 機器，又或者是 Google Kubernetes Engine (GKE) 的 Node ， 本身所使用的資源單位都可以稱為 Instance ； 換句話說，可以簡單的將 Instance 理解為能夠提供絕大部分 VM 相關功能的資源，如 : vCPU、Memory、Disk、Netwroking 以及機器學習最需要的 GPU (TPU)等等，因此這一部份的資源用量也都會被歸因到 GCE 上。\n將 billing report data 依據 SKU 進行加總並命名為 「Cost」欄位，再對 「Cost」欄位做 kernel density estimation (kde) 後可以得到 「Cost」的群聚密度，同時也能獲取一組較為合理的上下邊界以利取得離群值，「Cost」的離群值對於 billing report 的意義則在於找出異常的費用；以 下將固定使用 kde 取離群值的作法，因此不再一一贅述。\n從離群值得知，Instance Core 、 Instance Ram 以及 GPU 的費用都是比較可觀的\nInstance: 依據 Figure 1. GCE charged detail 給出的資訊，Instance 分為 Custom 與 N1-Predefined 兩種類別，這兩種類別在團隊中分別作為 GKE Node 與 GCE instance 來使用。依據 Google 在 GCE 定價的文件中可以得知， 1. Instance 的 CPU 與 Memory 是分別以 \u0026ldquo;running time\u0026rdquo; 進行收費， 2. custom machine type 會比 predefined machine type 收取更多費用。\n觀察 Custom Instance Core 、 N1 Predefined Instance Core 以及 N1 Predefined Instance Ram 的堆疊圖也可以發現，三者在 8 月至 11 月的費用並沒有出現 burst peak ， 反而在變化上呈現相對平滑的狀況；對於開發團隊來說，這其實不是一個正常的表現: 有限的人力伴隨著開發迭代週期，會出現大量使用 CPU 計算以驗證 feature 的開發情況，也會進行伴隨著壓力測試出現大量載入資料迫使 Memory 使用量增加的情況。\n因此，最可能的情況其實是: 團隊使用了超過需求量的資源。因為供過於求，導致收費並沒有發生變化，尤其是 GKE Node 應該要有卻沒有呈現的 auto-scaling 效益，最終的結論便是資源溢出造成的浪費，我們也在發現後的第一時間即時做出調整與改善。\nGPU: 團隊所開發的產品 ADsvantage | AI 智慧寫手 是一款 AI 智能廣告工具，24 小時智能監控，讓你不必隨時在線，AI 幫你顧廣告， 因此需要 GPU 來訓練 model 以及應用也是很合理的事情 (防不勝防，自己的業配自己寫XD)\nCloud Functions 和 GCE Instance 收費相同， Cloud Functions 也是以 CPU 和 Memory 的 \u0026ldquo;running time\u0026rdquo; 分別進行收費；差別在於 GCE 收費是以 Hour 作單位，而 Cloud Functions 則是以 100 毫秒(ms) 作為計費單位；即使 Cloud Function 調用 (Invocations) 次數達到千萬次，對於調用的收費也遠遠小於 CPU 和 Memory。\n這邊也多提一句，千萬不要把 Cloud Functions 當作 API 來使用， Cloud Functions 有它適合的場景，但顯然不是\u0026quot;永保在線\u0026quot;的服務。\nBigQuery Long-Term Storage 與 Active Storage 的識別條件: 超過90天沒有 modify / 90 天內仍有 modify 的資料表； Analysis 則是相對直覺的 Query 費用。\nFigure 3. BigQuery charged detail 告訴我們，目前開發環境中的 BigQuery 有太多 Long-Term 的資料被儲存著，這部分有屬於 machine learning 的 train data set ，當然也有太久沒有使用過的資料；同時資料的多寡也影響了 Analysis 每次的收費，因此我們對資料集進行了一次評估與審核，剔除掉已不再需要的資料，以期節省費用。\nGoogle Cloud Storage (GCS) GCS 提供了可對 儲存桶(bucket) 內的檔案 (objects) 實施 生命週期(life-cycle)管理的功能: 透過規則的設定，可以將符合規則天數的 objects 從 standard (nearline / coldline) 等級變更為 nearline、coldline 以及 archive 等級，各等級有不同的收費標準。如: archive 儲存的費用相對最低，但 access 的費用最高， standard 的儲存收費相對最高，但不收取 access 的費用 (如果有產生 traffic 則有可能會進行 bandwidth 的收費)。\n在過往的經歷中，即使資料已經沒有被使用到了仍然會\u0026quot;習慣性\u0026quot;的將其進行封存，以待某天會再度使用。然而多的是，我不知道的 archive 一直在被 access 的事\u0026hellip;\n想當然是馬上對 objects 進行盤點，並加入到資料審核與剃除的流程中啦!!\nSummary 經過這次的分析，也確實找出很多以往在開發中總不經意忽略的小事，然而正是這些最重要的小事，在收費上卻往往變成了大事。\n與一起踩過雷的同行共勉之，也希望這次的分析能夠對於日後使用雲端平台資源更加警慎。\n","permalink":"https://blog.zhengweiliu.com/posts/normal/gcp-billing-analytics/","summary":"在產品的開發中，團隊消耗成本最高的前幾項排名既在意料之中，Google Compute Engine (GCE)、 Cloud Functions 、 BigQuery 以及 Google Cloud Storage，但細項的部分也在意料之外。","title":"GCP Billing Analysis"},{"content":"既上次發布 [Google Certified 與 Cloud]後，和 Ryan 討論人流偵測系統中的資料流，以及感測設備是否存活的議題； Ryan 的工作背景是 Compute Vision 相關，相對於 ETL 資料處理流程中屬於提供 E ( extract ) 端服務的角色，也特別重視 extract 的功能是否都能如期發揮作用。 Google Certified 與 Cloud Date: 2021-09-22 \u0026nbsp; Categories: #Google Cloud Platform\u0026nbsp; #Certified\u0026nbsp; 透過幾個問題的交流過程，記錄我對使用雲端平台以及上雲這件事情的想法 拿認證對工作實戰的幫助以及對職涯的幫助，還是說有使用經驗其實不一定要拿認證，以實用性來說是不是熟悉其中幾項服務就足夠了 ? 考取認證僅證明你確實理解官方在這張認證領域上所提出的 Best Practice，並且具備將其轉換應用到實務上的基礎能力, 推薦的學習路徑和學習資源 ：會建議先去拿助理認證，還是可以直衝專家認證 ? ...... ETL | ELT 是流程還是系統 ?\nETL ( Extract-Transform-Load ) 與 ELT ( Extract-Load-Transform ) 是資料處理中常見的處理流程代名詞；個人認為 ETL ≠ ELT ， L | T 的先後順序除了影響處理流程的腳本之外，其實也需要搭配 scenario 來一起討論，同時也可能需要依賴應用系統的受眾群體特徵，搭建出對應的處理框架，以期在合理的效能下達成提供資料的目的。\n在上述過程中可以看出，ETL | ELT 會依據實際狀況而對於框架設計有所改變， ETL | ELT 應屬於流程，在實作完成後才會變成具體的系統；而流程則可以被獨立提出進行討論。\nExtract 是否有在好好運作 ? 資料遺失是否可以避免 ?\n在 Ryan 提出的議題中，extract 的服務由具體的感應偵測設備產生 log 資料，並不斷的往後段進行傳送，以便進行分析或儲存；當 extract device 離線或者是發生故障，若沒有在第一時間進行確認與通知相關人員，往往要等到進行資料統計時才會發現資料遺失。\n為此，主動進行 Check Sensor Is Alive 的機制看起來不可避免，或是有其他的途徑可以達成相同的目的呢 ?\nExtract Device 的資訊屬於已知或者未知 ?\n回到人流偵測的場景中，我本身對於這個應用場景較為陌生，因此參考了《基於影像處理及深度學習的兩階段人流偵測系統》[1]\n傳統計算人流的方法可能會在入口處利用計數 器手動計算，或者透過閘門式機械設備逐一計算，常 見的方法為紅外線感應或旋轉門計數，但是對於公車 候車亭等開放式、半開放式的場域而言，缺少固定入 口來協助逐一計算，因此本研究利用影像處理方法達 到智慧監控的目的。\n透過影像處理方法，對 Camera 蒐集到的畫面特徵擷取並將 ROI ( Region of interest ) 作為機器學習的 input data 之一；從這段描述中可以分析出兩個先決條件\n1\\. Camera 作為蒐集影像的設備，對應了 extract device 的角色\n2\\. Camera 的部署地點是已知條件，換句話說 : extract device 已紀錄在案\n已知條件的 extract device 在處理上會相對便利：由於已經確認該 device 有週期性或者需要不間斷的蒐集資料並回傳，透過設計 Slots 的方式來定期收取資料，並給予一些容許值；對於超出容許值的 device 或許就能夠先進行 Is Alive 的判斷機制，並盡早的通知相關人員或進行對應的處理。\n為什麼需要容許值 ?\n為了方便設計 ETL | ELT ，在設計的最初通常會假設 extract device 蒐集並提供的資料量是 100% ，即沒有任何資料遺失。然而 100% 的資料傳遞表示實作好的系統中，各個環節都不會有任何意外狀況發生，如：extract device 永遠不故障或者不需要汰舊換新、偵測區域的地點永遠不會有施工與環境物變更、永遠都有人流經過 ROI或是 ROI 永遠不變更\u0026hellip;等，往往在實際操作的經驗上，都會面臨因各種調整事件而造成的 incident 。\n因此，透過討論達成共識並給定一個容許值，可以讓整個系統在達成原先設立目的的同時具備一些彈性，並可以在後續的處理 ( Process ) 上考慮加入容許值條件以進行調整。\nCloud IoT Core \u0026amp; Monitoring\nGCP ( Google Cloud Platform ) 上提供了 Cloud IoT Core 的服務，讓 Device 可以直接或者間接將資料回傳至 GCP resources ，並且也提供了 monitoring 的功能來檢視 resource ，包含已記錄在案的 device ， 並提供視覺化的圖形報表以方便檢視功能運作的狀況，如：uptime check。\nAWS 、Azure 或是其他雲端平台可能也有提供相對應的功能組合或者服務，若沒有頭緒的話可以參考 GCP 給出的範例，並嘗試在不同的雲平台中討論合適的解決方案與架構。\nReference [1] 基於影像處理及深度學習的兩階段人流偵測系統, 林泓邦 1 *、林仁信 2 、廖伯翔 3, 中華民國自動機工程學會第二十五屆車輛工程學術研討會論文集, 中華民國一百零九年十月三十日\n","permalink":"https://blog.zhengweiliu.com/posts/normal/etl-elt-iot-device-alive-check/","summary":"在產品的開發中，團隊消耗成本最高的前幾項排名既在意料之中，Google Compute Engine (GCE)、 Cloud Functions 、 BigQuery 以及 Google Cloud Storage，但細項的部分也在意料之外。","title":"ETL | ELT 與 IoT Device Alive Check"},{"content":"轉換到雲端領域工作也過了大半年，這段不算長且還在進行中旅程中也獲取了三張 Google Cloud Platform ( GCP ) 的 Certified : Associate Cloud Engineer | Professional Cloud Architect | Professional Cloud Network Engineer\n每每在考取認證的當下，也試著將這份喜悅分享給社群好友，也因此成為了開啟與好友交流雲端使用經驗的契機。\n最近，和 Enzo 聊到在工作領域深耕的話題。Enzo 對資料科學的領域具有高度熱忱，也希望朝著 Senior Data Engineer 的角色發展；目前對於 Senior Data Engineer 的專業需求中，經常看到需要具備雲端平台的服務或工具等使用經驗；Enzo 除了使用中的 Google Compute Engine ( GCE ) Virtual Machine 服務之外，也希望進一步了解自學 GCP 的必要性與可能性，同時透過考取認證的方式確認自己學習的成果，以及希望將其作為對外證明的一舉兩得好方式。\n和 Enzo 交流討論的過程中，我也從中發現一些值得紀錄的觀點。無論未來的我對這個觀點是抱持著贊同的態度，也或者大相徑庭，都是一種值得回味的思考。\n以下透過幾個問題的交流過程，記錄我對使用雲端平台以及上雲這件事情的想法\n拿認證對工作實戰的幫助以及對職涯的幫助，還是說有使用經驗其實不一定要拿認證，以實用性來說是不是熟悉其中幾項服務就足夠了 ?\n當初考慮轉換工作領域時，我也曾思考過這個問題；再陸續考取認證的過程中，也找到了一個自己認為合適的答案。\n考取認證僅證明你確實理解官方在這張認證領域上所提出的 Best Practice，並且具備將其轉換應用到實務上的基礎能力\n換句話說 : 認證是一個敲門磚。\n對外來說確實也是一個不錯的證明，面對非相同專業領域的人而言，這也代表了官方的背書。\n推薦的學習路徑和學習資源 ：會建議先去拿助理認證，還是可以直衝專家認證 ?\n因為長期使用 GCE 的經驗，促使 Enzo 希望從 GCP 的認證作為起步。\n對於希望學習 GCP 的人而言， Associate Cloud Engineer ( ACE ) 確實是幫助入門 Cloud 的好途徑：由於 Cloud 的資源眾多，若希望執行搬遷上雲的計畫， ACE 會以較為具體的執行方案帶你領略各個 GCP 資源的使用藍圖以及限制。\n舉 Data 相關的工作為例，個人淺見認為不管從事的工作內容是 資料科學家 | 資料分析師 | 資料工程師 ，都無可避免的需要明確的知道資料從哪裡來、資料格式長相如何，以及處理過的資料最終需要往哪邊去。\n因此，可以得到一個較為直覺的論點\n資料流 ( Dataflow ) 與 資料處理 ( Process ) 可以單獨存在與單獨討論\n在過往的開發工作進行時，通常所使用的設備資源都是完整可見的硬體設施，而我們在這些設施上完成相關的開發作業；後來，當發現各家雲平台都有提供 Virtual Machine 的資源時，最直覺的上雲就是將工作搬往雲平台的 Virtual Machine ，藉此省下的硬體設施的費用。\n至此，如果你問我使用雲平台的 Virtual Machine 算不算上雲了，我的回答是看狀況\n雲端與地端的區別在哪裡 ?\n若我們以最廣為人知的定義來討論雲端與地端，則地端屬於公司內部設備，使用內部網路來隔離與外部網路的接觸，並透過層層保護設施來保護內部服務與資料不會輕易的被未授權的使用者存取；\n而雲端則是使用雲平台提供同等硬體設施的虛擬資源，如： GCE、GKE，或者是雲平台提供的虛擬網路定義，如：VPC ( Virtual Private Cloud ) ，同時雲平台也提供底層的封包加解密、密鑰管理以及 Security Command Center 等等服務。\n在使用雲平台的經驗中，可以省下大量需要人力進行底層設施 ( Infrastructure ) 更換以及佈署的時間。\n然而，如果我們用雲平台的資源，做著與地端時期相同的工作，真的就是上雲了嗎 ?\n硬體上雲與服務上雲\n在智慧手機不如現在功能強大的十幾年前，我們大多在個人電腦上使用社群媒體來與朋友進行交流；而現在，我們利用智慧手機來刷社群媒體的時間可能比坐在電腦前的時間還多。\n誠然，我們將地端時期的工作搬遷到雲平台上，可以節省硬體設施等相關費用，但工作上的思考仍然停留在地端時期，我們只是將工作搬遷到另外一個環境上而已。\n同樣是以 Dataflow 與 Process 的角度來看\n地端時期，資料流中的每個端點，如： NAS、SFTP 或是其他伺服器服務，都是明確的執行在具體一台伺服器設備，或是特定的叢集伺服器機群；這樣明確的設備端到設備端的資料流向也是地端時期的明顯特徵之一。\n雲平台提供了 Virtual Machine 的資源，提供了從地端上雲的可能性；然而，我們會發現在這個模式的運作下， Dataflow 與 Process 仍然依賴著設備端到設備端的模式。\n還記得前面提到的 資料流 ( Dataflow ) 與 資料處理 ( Process ) 可以單獨存在與單獨討論 嗎 ? 這表示它們其實並不需要依賴特定的設備端，明確地說\n並不僅僅只能在 Virtual Machine 上實現\n眾多服務為何只取 Virtual Machine ?\n雲平台提供了眾多的資源，幾乎可以滿足絕大部分的資料流設計與脫離 Virtual Machine 執行 Process 的方式，即 serverless，以下列舉幾個雲平台中與 Data 相關的資源\nData Storage Google Cloud Storage 提供 Bucket 資源可以儲存大量文件資料 Google Cloud Pub/Sub 提供 MQTT 服務，作為資料流渠道角色，同時也具備儲存批次資料的能力 Firebase 提供了最適合 Web 與 Mobile App 需要的儲存空間 Serverless Cloud Functions 提供了 serverless 的服務，適用於執行微服務 function、 stateless function 或是單純的 API endpoints Data Process Dataprep 提供了視覺化與簡易編輯的 Transform 功能，也就是 ETL | ELT 中經常被提到的 “ T ” Dataflow 提供了整個資料流的編排功能，可以透過視覺化工具來編排資料流的來源、處理以及目的地；同時也支援將雲平台中與資料相關的資源作為來源端或目的地端，也支援從其他雲平台引入資料 雲端一角\n至此便能總結出幾項要點，用以描述我對雲端的觀點\n將 workload 抽離對於資源的依賴性；workload 應可被單獨設計與討論 雲端資源的提供者並非僅有一家。雲端與地端都能作為資料流來源，同時也能作為資料流去處 節省硬體與底層資源的佈建佈署等成本 資料的尾巴\n為什麼通篇都以資料流作為舉例，並不斷地提起呢 ?\nEvery company is a data company 中給出了很好的解釋，有空的話可以看看\n","permalink":"https://blog.zhengweiliu.com/posts/normal/google-certified-cloud/","summary":"透過幾個問題的交流過程，記錄我對使用雲端平台以及上雲這件事情的想法\n拿認證對工作實戰的幫助以及對職涯的幫助，還是說有使用經驗其實不一定要拿認證，以實用性來說是不是熟悉其中幾項服務就足夠了 ?\n考取認證僅證明你確實理解官方在這張認證領域上所提出的 Best Practice，並且具備將其轉換應用到實務上的基礎能力,\n推薦的學習路徑和學習資源 ：會建議先去拿助理認證，還是可以直衝專家認證 ?","title":"Google Certified 與 Cloud"}]